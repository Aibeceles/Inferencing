{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d015e88e-e923-4c26-8dad-15118bc740b1",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e70f2d-33de-412b-b7c2-3aa358dd8c22",
   "metadata": {},
   "source": [
    "# 3. Generating Topics and Subtopics with NeMo Curator\n",
    "\n",
    "In this notebook, we will demonstrate how to generate synthetic data using **[NVIDIA NeMo Curator](https://github.com/NVIDIA/NeMo-Curator/)**, a powerful tool designed to create high-quality datasets for training large language models (LLMs).\n",
    "\n",
    "Synthetic data generation is particularly valuable when real-world data is limited, noisy, or difficult to obtain. NeMo Curator simplifies this process by providing prebuilt pipelines and customizable modules that allow developers to generate, filter, and refine synthetic datasets efficiently. \n",
    "\n",
    "By leveraging its synthetic data generation capabilities, we will produce a diverse list of **topics and subtopics in English and Spanish**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42566d5-0e43-4836-80ef-27c34db645e5",
   "metadata": {},
   "source": [
    "**[3.1 NeMo Curator OpenAI Client](#3.1-NeMo-Curator-OpenAI-Client)<br>**\n",
    "**[3.2 Topics-Subtopics Generation in English and Spanish](#3.2-Topics-Subtopics-Generation-in-English-and-Spanish)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267cb1d0-56d0-429b-ab50-c9f6d165847c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b2130-d07c-4ab0-b4e9-dfe6ff7df61f",
   "metadata": {},
   "source": [
    "## Connecting to the NVIDIA API Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5032ed9a-5a4d-474f-ab73-6ddb510f69ef",
   "metadata": {},
   "source": [
    "NeMo Curator supports connecting to [OpenAI API](https://github.com/openai/openai-python?tab=readme-ov-file#openai-python-api-library) compatible services and [NeMo Deploy](https://docs.nvidia.com/nemo-framework/user-guide/latest/deployingthenemoframeworkmodel.html#use-nemo-export-and-deploy-module-apis-to-run-inference) services.\n",
    "\n",
    "In this notebook, we rely on the `build.nvidia.com` API endpoints. You can use this same flow with a model deployed as an NVIDIA NIM for LLMs which can be found [here](https://github.com/NVIDIA/NeMo-Curator/blob/main/docs/user-guide/syntheticdata.rst#connecting-to-an-llm-service)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5f9cd2-309d-4837-be42-6dec0f2404a4",
   "metadata": {},
   "source": [
    "Your environment already has an NVIDIA API key installed for you. For work outside of this workshop environment, please see the instructions below for how to obtain your own free NVIDIA API key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c166a59-6faa-4485-b96d-3e8e9fdfa365",
   "metadata": {},
   "source": [
    "### Obtaining Your Own NVIDIA API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2770e34f-30f9-45da-aaad-0d324a85e81e",
   "metadata": {},
   "source": [
    "If you would like an NVIDIA API key for your own work outside this workshop environment, you can generate one for free using the following steps:\n",
    "\n",
    "1. Login (or sign up) through [build.nvidia.com](https://build.nvidia.com/explore/discover).\n",
    "2. Click the `Get API Key` button available on the the `nvidia/nemotron-4-340b-instruct` page, found [here](https://build.nvidia.com/nvidia/nemotron-4-340b-instruct)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f92012-996f-478f-b0e0-dceac50bb4b9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a734d7-6638-4fbf-9ac7-7bc8ea672963",
   "metadata": {},
   "source": [
    "## 3.1 NeMo Curator OpenAI Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e16386",
   "metadata": {},
   "source": [
    "### Loading NVIDIA API Credentials\n",
    "\n",
    "Before connecting to NVIDIA's API, we need to load the required credentials. This cell automatically checks multiple locations:\n",
    "\n",
    "1. **Project directory** (priority 1): `./secrets.env` (in the same folder as this notebook) ✅ **Found!**\n",
    "2. **Home directory** (priority 2): `~/.nvidia/secrets.env`\n",
    "3. **Environment variables** (priority 3): Pre-set in some workshop environments\n",
    "\n",
    "**Required credentials:**\n",
    "- `NVIDIA_API_KEY`: Your NVIDIA API key from build.nvidia.com\n",
    "- `NVIDIA_BASE_URL`: The NVIDIA API endpoint (https://integrate.api.nvidia.com/v1)\n",
    "\n",
    "**Get your free API key:**\n",
    "Visit https://build.nvidia.com/nvidia/nemotron-4-340b-instruct and click \"Get API Key\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf7a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ NVIDIA API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load NVIDIA API credentials from secrets file\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to secrets file - check multiple locations\n",
    "# Priority: 1) Local project directory, 2) Home directory, 3) Environment variables\n",
    "try:\n",
    "    project_secrets = Path(\"secrets.env\")\n",
    "    home_secrets = Path.home() / \".nvidia\" / \"secrets.env\"\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Path setup issue: {e}\")\n",
    "    project_secrets = None\n",
    "    home_secrets = None\n",
    "\n",
    "def load_secrets_from_file(filepath):\n",
    "    \"\"\"Load environment variables from a secrets file\"\"\"\n",
    "    try:\n",
    "        if not filepath or not filepath.exists():\n",
    "            return False\n",
    "        \n",
    "        print(f\"Loading secrets from {filepath}\")\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line and not line.startswith('#') and '=' in line:\n",
    "                    key, value = line.split('=', 1)\n",
    "                    os.environ[key.strip()] = value.strip().strip('\"').strip(\"'\")\n",
    "        print(\"✓ NVIDIA API credentials loaded\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading secrets: {e}\")\n",
    "        return False\n",
    "\n",
    "# Try loading from different locations\n",
    "loaded = False\n",
    "if project_secrets and project_secrets.exists():\n",
    "    loaded = load_secrets_from_file(project_secrets)\n",
    "elif home_secrets and home_secrets.exists():\n",
    "    loaded = load_secrets_from_file(home_secrets)\n",
    "elif \"NVIDIA_API_KEY\" in os.environ:\n",
    "    print(\"✓ Using NVIDIA_API_KEY from environment variables\")\n",
    "    loaded = True\n",
    "else:\n",
    "    print(\"⚠️  NVIDIA_API_KEY not found!\")\n",
    "    print(\"\\nSearched locations:\")\n",
    "    print(f\"  1. ./secrets.env\")\n",
    "    print(f\"  2. ~/.nvidia/secrets.env\")\n",
    "    print(f\"  3. Environment variables\")\n",
    "    print(\"\\nPlease create a secrets.env file with:\")\n",
    "    print(\"   NVIDIA_API_KEY=nvapi-xxxxxxxxxxxxx\")\n",
    "    print(\"   NVIDIA_BASE_URL=https://integrate.api.nvidia.com/v1\")\n",
    "\n",
    "# Verify credentials are available\n",
    "if \"NVIDIA_API_KEY\" in os.environ and \"NVIDIA_BASE_URL\" in os.environ:\n",
    "    print(f\"\\n✓ API Key: {os.environ['NVIDIA_API_KEY'][:10]}...\")\n",
    "    print(f\"✓ Base URL: {os.environ['NVIDIA_BASE_URL']}\")\n",
    "else:\n",
    "    print(\"\\n❌ Missing required environment variables!\")\n",
    "    print(\"   Required: NVIDIA_API_KEY, NVIDIA_BASE_URL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e07843-0158-4721-ba93-93497f146993",
   "metadata": {},
   "source": [
    "We are going to:\n",
    "\n",
    "1. Initialize OpenAI's client.\n",
    "2. Initialize NeMo Curator's `OpenAIClient`\n",
    "3. Perform a request, and print the LLM response.\n",
    "\n",
    "Notice that the structure of the request is very close to the original OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40377443-1318-4ddd-9e37-2563a055c323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OpenAI client initialized successfully\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI's client with the NVIDIA API endpoint and the API key loaded from secrets.env\n",
    "openai_client = OpenAI(\n",
    "    base_url=os.getenv(\"NVIDIA_BASE_URL\"),\n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\"),\n",
    ")\n",
    "\n",
    "print(\"✓ OpenAI client initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "128d3bef-4cd3-42dc-8aed-f216c00a53e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import OpenAIClient\n",
    "\n",
    "# Initialize NeMo Curator's OpenAIClient by passing the OpenAI client instance.\n",
    "# This wraps the OpenAI client to provide additional functionality specific to NeMo Curator.\n",
    "curator_openai_client = OpenAIClient(openai_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a54ec151-5166-4474-b38f-a9eb90036a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There once was a scientist, so clever and bright,\n",
      "Who used GPU computing, day and night.\n",
      "With CUDA and OpenCL,\n",
      "He solved complex problems,\n",
      "In a flash, like a lightning bolt.\n"
     ]
    }
   ],
   "source": [
    "# Perform a request to a hosted model\n",
    "llm_response = curator_openai_client.query_model(\n",
    "    model=\"nvidia/nemotron-mini-4b-instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a limerick about the wonders of GPU computing.\",\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.2,\n",
    "    top_p=0.7,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "# Print response\n",
    "print(llm_response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3fb50d-110e-4c3f-9a7f-b0eb58fdee14",
   "metadata": {},
   "source": [
    "## 3.2 Topics-Subtopics Generation in English and Spanish\n",
    "\n",
    "The NeMo Curator Synthetic Data Generation (SDG) features are primarily accessed through the `NemotronGenerator` class.\n",
    "\n",
    "This useful wrapped helps expose both: \n",
    "\n",
    "1. Pre-built SDG pipelines \n",
    "2. A number of specific generation utilities.\n",
    "\n",
    "Before heading into the pre-built pipelines, we're going to \"break-apart\" an existing pipeline, in this case: the Topics-Subtopics Generation Pipeline - and see the granular customization that Nemo Curator provides for each step. \n",
    "\n",
    "We're going to work through the following process (extracted from [Nemotron-4 340B Technical Report](https://arxiv.org/pdf/2406.11704)):\n",
    "\n",
    "1. Generate `n` Macro Topics - Have our LLM generate `n` broad topics relating to daily life, the world, etc.\n",
    "2. Generate `n` Sub Topics - Have our LLM take each Macro Topic and generate `n` topics relating to the Macro Topics.\n",
    "\n",
    "### 3.2.1 Generating `n` Macro Topics in English and Spanish\n",
    "\n",
    "Our first step is to generate our Macro Topics. \n",
    "\n",
    "Let's look at the prompt that drives this process as well, to get a better understanding of what's happening \"under the hood\":\n",
    "\n",
    "```python\n",
    "\"Can you generate {n_macro_topics} comprehensive topics that encompass various aspects of our daily life, the world, and science? Your answer should be a list of topics. Make the topics as diverse as possible. For example, 1. Food and drinks. \\n2. Technology.\\n\"\n",
    "```\n",
    "\n",
    "To do this, we'll use the [`generate_macro_topics`](https://github.com/NVIDIA/NeMo-Curator/blob/cd4c4907bd4d87cd11d0f37be4ae0fe167a79696/nemo_curator/synthetic/nemotron.py#L115) method of our `NemotronGenerator`.\n",
    "\n",
    "> NOTE: All prompt templates are fully customizable, and we'll take a look at how we can do that in the upcoming cells!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5716b7ea-22b3-4996-858b-11e45bc83218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Sustainable Agriculture and Food Systems: This topic covers the production, distribution, and consumption of food in a way that ensures environmental, economic, and social sustainability. It includes organic farming, permaculture, food waste reduction, and fair trade.\n",
      "\n",
      "2. Climate Change and Renewable Energy: This topic discusses the impact of human activities on climate change, the science behind it, and the solutions to mitigate its effects. It includes renewable energy sources, carbon capture technologies, and climate policies.\n",
      "\n",
      "3. Mental Health and Wellness: This topic focuses on understanding mental health, its importance, and strategies for maintaining it. It includes stress management, mindfulness, therapy, and the role of technology in mental health.\n",
      "\n",
      "4. Global Politics and Diplomacy: This topic explores the political landscape of the world, international relations, diplomacy, and global governance. It includes peacekeeping, human rights, international law, and the role of multilateral organizations.\n",
      "\n",
      "5. Artificial Intelligence and Ethics: This topic delves into the development and application of artificial intelligence, its impact on society, and the ethical considerations surrounding its use. It includes AI in healthcare, education, business, and the potential risks and benefits.\n",
      "\n",
      "Each of these topics offers a broad perspective on various aspects of our daily life, the world, and science, encouraging a comprehensive understanding of the world we live in.\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.synthetic import NemotronGenerator\n",
    "\n",
    "# An example class used to generate syntethic data.\n",
    "generator = NemotronGenerator(curator_openai_client)\n",
    "\n",
    "# Model used to generate syntethic data.\n",
    "model = \"mistralai/mistral-7b-instruct-v0.3\"\n",
    "model_kwargs = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_tokens\": 1024,\n",
    "}\n",
    "\n",
    "# Number of macro topics to generate\n",
    "n_macro_topics = 5\n",
    "\n",
    "# Generate macro topics\n",
    "llm_response = generator.generate_macro_topics(\n",
    "    model=model, model_kwargs=model_kwargs, n_macro_topics=n_macro_topics\n",
    ")\n",
    "\n",
    "print(llm_response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c185bb8-0da1-4f13-8ad1-1b67a9e39693",
   "metadata": {},
   "source": [
    "While this is a great start - we'd love to have this response in a Python list. \n",
    "\n",
    "We'll use the NeMo Curator `convert_response_to_yaml_list` method to accomplish this goal. \n",
    "\n",
    "Please notice that currently, this method is quite strict. Custom parsing might be required depending on model choice, and use-case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25d40d7f-7fab-4890-af41-f81b6ebea325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sustainable Agriculture and Food Systems', 'Climate Change and Renewable Energy', 'Mental Health and Wellness', 'Global Politics and Diplomacy', 'Artificial Intelligence and Ethics']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from nemo_curator.synthetic.error import YamlConversionError\n",
    "from nemo_curator.synthetic.prompts import DEFAULT_MACRO_TOPICS_PROMPT_TEMPLATE\n",
    "\n",
    "\n",
    "def generate_macro_topics(\n",
    "    generator: NemotronGenerator,\n",
    "    model: str,\n",
    "    model_kwargs: dict,\n",
    "    n_macro_topics: int,\n",
    "    prompt_template: str = DEFAULT_MACRO_TOPICS_PROMPT_TEMPLATE,\n",
    "    n_retries: int = 5,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generates a list of macro topics using a language model and retries on YAML conversion errors.\n",
    "\n",
    "    This method leverages a `NemotronGenerator` instance to generate macro topics by invoking a\n",
    "    language model (LLM). It processes the LLM's response, converts it into a YAML-formatted list,\n",
    "    and retries the process up to `n_retries` times if a `YamlConversionError` occurs.\n",
    "\n",
    "    Args:\n",
    "        generator (NemotronGenerator): An instance of the `NemotronGenerator` class responsible for\n",
    "            generating and processing LLM responses.\n",
    "        model (str): The name or identifier of the language model to be used for generating macro topics.\n",
    "        model_kwargs (dict): A dictionary of additional keyword arguments to configure the language model.\n",
    "        n_macro_topics (int): The number of macro topics to generate.\n",
    "        prompt_template (str, optional): A string template used to construct the prompt for the LLM.\n",
    "            Defaults to `DEFAULT_MACRO_TOPICS_PROMPT_TEMPLATE`.\n",
    "        n_retries (int, optional): The maximum number of retry attempts in case of a `YamlConversionError`.\n",
    "            Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of generated macro topics as strings.\n",
    "\n",
    "    Raises:\n",
    "        YamlConversionError: If all retry attempts fail due to YAML conversion issues.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the generated macro topics\n",
    "    macro_topics = []\n",
    "\n",
    "    # Attempt to generate and convert macro topics up to `n_retries` times\n",
    "    for _ in range(n_retries):\n",
    "        try:\n",
    "            # Generate a response from the language model using the specified parameters\n",
    "            llm_response = generator.generate_macro_topics(\n",
    "                n_macro_topics=n_macro_topics,\n",
    "                model=model,\n",
    "                model_kwargs=model_kwargs,\n",
    "                prompt_template=prompt_template,\n",
    "            )\n",
    "\n",
    "            # Convert the first response from the LLM into a YAML-formatted list of topics\n",
    "            macro_topics = generator.convert_response_to_yaml_list(\n",
    "                llm_response=llm_response[0], model=model, model_kwargs=model_kwargs\n",
    "            )\n",
    "\n",
    "            # Break out of the retry loop if conversion is successful\n",
    "            break\n",
    "        except YamlConversionError as e:\n",
    "            # Print an error message and retry if YAML conversion fails\n",
    "            print(f\"Hit: {e}, Retrying...\")\n",
    "\n",
    "    # Return the generated list of macro topics (empty if all retries failed)\n",
    "    return macro_topics\n",
    "\n",
    "\n",
    "# Generate a list of macro topics in English\n",
    "macro_topics_english = generate_macro_topics(\n",
    "    generator=generator,\n",
    "    model=model,\n",
    "    model_kwargs=model_kwargs,\n",
    "    n_macro_topics=n_macro_topics,\n",
    "    prompt_template=DEFAULT_MACRO_TOPICS_PROMPT_TEMPLATE,\n",
    ")\n",
    "\n",
    "# Print the generated list of macro topics in English\n",
    "print(macro_topics_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c703fec3-1cf8-46f5-a334-0d752547c024",
   "metadata": {},
   "source": [
    "Let’s replicate the process in Spanish. To do this, we simply need to modify the prompt accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe04e95f-0bd3-41ce-859e-81865e342f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Salud y bienestar', 'Medio ambiente y sustentabilidad', 'Educación y aprendizaje', 'Economía y negocios', 'Ciencia y tecnología avanzada']\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt template in Spanish for generating macro topics\n",
    "macro_topics_prompt_template_spanish = (\n",
    "    \"Genera {n_macro_topics} temas amplios que abarquen diversos aspectos de nuestra vida diaria, el mundo y la ciencia. \"\n",
    "    \"Tu respuesta debe ser únicamente una lista de temas, sin incluir explicaciones ni texto adicional. Por ejemplo: \"\n",
    "    \"1. Comida y bebidas. \\n2. Tecnología.\\n\"\n",
    ")\n",
    "\n",
    "# Generate macro topics list in Spanish\n",
    "macro_topics_spanish = generate_macro_topics(\n",
    "    generator=generator,\n",
    "    model=model,\n",
    "    model_kwargs=model_kwargs,\n",
    "    n_macro_topics=n_macro_topics,\n",
    "    prompt_template=macro_topics_prompt_template_spanish,\n",
    ")\n",
    "\n",
    "# Print the generated list of macro topics in Spanish\n",
    "print(macro_topics_spanish)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d227d-a349-4638-b859-e4129f41a4d1",
   "metadata": {},
   "source": [
    "**Exercise:** Generate `n_macro_topics` in a language other than Spanish or English.\n",
    "\n",
    "If you are not fluent in another language, use an LLM or an online translation tool (e.g., [Google Translate](https://translate.google.com), [DeepL](https://www.deepl.com/)) to translate one of the previous prompts into your chosen language.\n",
    "\n",
    "Once translated, run the pipeline to generate macro topics in that language.\n",
    "\n",
    "Afterward, reflect on the results:\n",
    "\n",
    "- Did the topics align with the intended meaning of the original prompt?\n",
    "- Were there any noticeable differences in diversity or quality compared to the topics generated in Spanish or English?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666c2dfe-c786-434b-b623-97aeee816a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308a186c-0d97-454b-8d35-244016abc15e",
   "metadata": {},
   "source": [
    "### 3.2.2 Generating `n` Subtopics in English and Spanish\n",
    "\n",
    "We will follow the same procedure as before; however, this time, the prompt will be tailored to specify the generation of subtopics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f26918d-67f8-4969-bf1a-fc9c70ccd03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Agroecology and Biodiversity Conservation\n",
      "2. Climate-Smart Agriculture and Carbon Sequestration\n",
      "3. Food Waste Management and Zero Hunger Initiatives\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt template for generating subtopics in English.\n",
    "subtopics_prompt_template_english = (\n",
    "    \"Generate {n_subtopics} topics that cover various aspects of {macro_topic}. \"\n",
    "    \"Your response should only be a list of topics, as diverse as possible. Do not include explanations or additional text. For example: \"\n",
    "    \"1. Food and drinks. \\n2. Technology.\\n\"\n",
    ")\n",
    "\n",
    "# Specify the number of subtopics to generate for each macro topic.\n",
    "n_subtopics = 3\n",
    "\n",
    "# Use the generator to produce `n_subtopics` in English for the first macro topic in the list.\n",
    "llm_response = generator.generate_subtopics(\n",
    "    model=model,\n",
    "    model_kwargs=model_kwargs,\n",
    "    macro_topic=macro_topics_english[0],\n",
    "    n_subtopics=n_subtopics,\n",
    "    prompt_template=subtopics_prompt_template_english,\n",
    ")\n",
    "\n",
    "# Print the generated subtopics.\n",
    "print(llm_response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94086068-1f90-4ed6-a3a8-d41d21682714",
   "metadata": {},
   "source": [
    "As we did earlier, we will utilize the `convert_response_to_yaml_list` method to transform the LLM response into a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fed5d426-b7d0-40bb-9111-feca86d0086f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Agroecology and Biodiversity Conservation', 'Climate-Smart Agriculture and Carbon Sequestration', 'Food Waste Management and Zero Hunger Initiatives']\n"
     ]
    }
   ],
   "source": [
    "def generate_subtopics(\n",
    "    model: str,\n",
    "    model_kwargs: dict,\n",
    "    macro_topic: str,\n",
    "    n_subtopics: int,\n",
    "    prompt_template: str,\n",
    "    n_retries: int = 5,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generates a list of subtopics for a given macro topic using a language model (LLM).\n",
    "\n",
    "    This function interacts with the `generator` object to produce subtopics based on the provided\n",
    "    macro topic and prompt template. If the YAML conversion fails, it retries the process up to\n",
    "    `n_retries` times.\n",
    "\n",
    "    Args:\n",
    "        model (str): The name or identifier of the language model to use.\n",
    "        model_kwargs (dict): A dictionary of additional configuration parameters for the language model.\n",
    "        macro_topic (str): The macro topic for which subtopics will be generated.\n",
    "        n_subtopics (int): The number of subtopics to generate.\n",
    "        prompt_template (str): The prompt template used to guide the LLM in generating subtopics.\n",
    "        n_retries (int, optional): The maximum number of retry attempts if YAML conversion fails. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of generated subtopics as strings. If all retries fail, returns an empty list.\n",
    "\n",
    "    Raises:\n",
    "        YamlConversionError: If YAML conversion fails after all retry attempts.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store generated subtopics\n",
    "    subtopics = []\n",
    "\n",
    "    # Retry loop to handle potential YAML conversion errors\n",
    "    for _ in range(n_retries):\n",
    "        try:\n",
    "            # Generate a response from the language model using the specified parameters\n",
    "            llm_response = generator.generate_subtopics(\n",
    "                model=model,\n",
    "                model_kwargs=model_kwargs,\n",
    "                macro_topic=macro_topic,\n",
    "                n_subtopics=n_subtopics,\n",
    "                prompt_template=prompt_template,\n",
    "            )\n",
    "\n",
    "            # Convert the first response from the LLM into a YAML-formatted list of subtopics\n",
    "            subtopics = generator.convert_response_to_yaml_list(\n",
    "                llm_response=llm_response[0], model=model\n",
    "            )\n",
    "\n",
    "            # Exit the retry loop if conversion is successful\n",
    "            break\n",
    "        except YamlConversionError as e:\n",
    "            # Print an error message and retry if YAML conversion fails\n",
    "            print(f\"Hit: {e}, Retrying...\")\n",
    "\n",
    "    # Return the generated list of subtopics (or an empty list if all retries failed)\n",
    "    return subtopics\n",
    "\n",
    "\n",
    "# Generate a list of subtopics in English for the first macro topic in `macro_topics_english`\n",
    "subtopics_english = generate_subtopics(\n",
    "    model=model,\n",
    "    model_kwargs=model_kwargs,\n",
    "    macro_topic=macro_topics_english[0],\n",
    "    n_subtopics=n_subtopics,\n",
    "    prompt_template=subtopics_prompt_template_english,\n",
    ")\n",
    "\n",
    "# Print the generated list of subtopics in English\n",
    "print(subtopics_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf242cf-1b9e-4170-a354-5ff1665297bc",
   "metadata": {},
   "source": [
    "Let’s replicate the process in Spanish. To do this, we simply need to modify the prompt accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80e32f9e-51d2-402a-b39c-9f874eba9119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Salud Mental y Emocional', 'Nutrición y Alimentación', 'Ejercicio y Actividad Física']\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt template for generating subtopics in Spanish.\n",
    "subtopics_prompt_template_spanish = (\n",
    "    \"Genera {n_subtopics} temas amplios que abarquen diversos aspectos de {macro_topic}. \"\n",
    "    \"Tu respuesta debe ser únicamente una lista de temas. \"\n",
    "    \"Cada tema es un elemento de la lista. No incluyas subtemas ni explicaciones ni texto adicional. \"\n",
    "    \"Por ejemplo: 1. Comida y bebidas. \\n2. Tecnología.\\n\"\n",
    ")\n",
    "\n",
    "# Use the generator to produce `n_subtopics` in Spanish for the first macro topic in the list.\n",
    "subtopics_spanish = generate_subtopics(\n",
    "    model=model,\n",
    "    model_kwargs=model_kwargs,\n",
    "    macro_topic=macro_topics_spanish[0],\n",
    "    n_subtopics=n_subtopics,\n",
    "    prompt_template=subtopics_prompt_template_spanish,\n",
    ")\n",
    "\n",
    "# Print the generated subtopics.\n",
    "print(subtopics_spanish)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc02193-bc79-4e33-8fdf-3c8fd21f22dd",
   "metadata": {},
   "source": [
    "**Exercise:** Generate `n_subtopics` for a macro topic in a language other than Spanish or English.\n",
    "\n",
    "Afterward, reflect on the results:\n",
    "- Did the subtopics align with the intended meaning of the original prompt?\n",
    "- Were there any noticeable differences in diversity, specificity, or quality compared to subtopics generated in Spanish or English?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b955cb63-72c0-43bc-95a6-0831b9e172cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c899aafa-293e-4f88-9265-4214a84c6e18",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "In this notebook, you used Nemo Curator to generate lists of topics and subtopics in both English and Spanish.\n",
    "\n",
    "In the next notebook, we will select a subtopic to create a **Question-and-Answer Dataset for Supervised Fine-Tuning (SFT)**, available in both languages.\n",
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px; float: right;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
