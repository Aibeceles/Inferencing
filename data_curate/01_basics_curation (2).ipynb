{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb478833-19ac-41a1-923f-6cc9f64181d5",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0936ea03-3f38-49e9-91c3-2d370769c8a8",
   "metadata": {},
   "source": [
    "# 1. Basics of Data Curation\n",
    "\n",
    "******\n",
    "\n",
    "Generative AI developemet requires a havy data curation process. The quality the model largely depends on the quality of the data used for training. NVIDIA NeMo Curator is an open-source framework designed to streamline this process by preparing large-scale, high-quality datasets for pretraining and continuous training.\n",
    "\n",
    "NeMo Curator offers built-in workflows for curating data from various public sources such as Common Crawl, Wikipedia, and arXiv. At the same time, it provides the flexibility to customize pipelines to suit the specific needs of your project.\n",
    "\n",
    "This notebook guides the process of basic data preparation involved in most Language Models developements: \n",
    "\n",
    "**[1.1 Text Cleaning and Unification](#1.1-Text-Cleaning-and-Unification)<br>**\n",
    "**[1.2 Document Size Filtering](#1.2-Document-Size-Filtering)<br>**\n",
    "**[1.3 Filter Personally Identifiable Information (PII)](#1.3-Filter-Personally-Identifiable-Information-(PII))<br>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cfc4b2-8fd0-4cbf-86ae-094e6c8fead9",
   "metadata": {},
   "source": [
    "***************\n",
    "### Environment Setup\n",
    "\n",
    "For large-scale data processing, NeMo Curator provides both GPU and CPU based modules. Understanding how these modules interact and how to configure your environment is key to optimizing performance.\n",
    "\n",
    "CPU-based modules rely on [Dask](https://www.dask.org/) to distribute computations across multi-node clusters while GPU-accelerated modules uses [RAPIDS](https://rapids.ai/) to handle large-scale datasets efficiently.\n",
    "\n",
    "Let's check first your current environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "652486cf-06ef-43af-b136-96cda3b7714c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:             x86_64\n",
      "  CPU op-mode(s):         32-bit, 64-bit\n",
      "  Address sizes:          39 bits physical, 48 bits virtual\n",
      "  Byte Order:             Little Endian\n",
      "CPU(s):                   32\n",
      "  On-line CPU(s) list:    0-31\n",
      "Vendor ID:                GenuineIntel\n",
      "  Model name:             Intel(R) Core(TM) i9-14900HX\n",
      "    CPU family:           6\n",
      "    Model:                183\n",
      "    Thread(s) per core:   2\n",
      "    Core(s) per socket:   16\n",
      "    Socket(s):            1\n",
      "    Stepping:             1\n",
      "    BogoMIPS:             4838.40\n",
      "    Flags:                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge m\n",
      "                          ca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht sysc\n",
      "                          all nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xt\n",
      "                          opology tsc_reliable nonstop_tsc cpuid tsc_known_freq \n",
      "                          pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2\n",
      "                          apic movbe popcnt tsc_deadline_timer aes xsave avx f16\n",
      "                          c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibr\n",
      "                          s ibpb stibp ibrs_enhanced tpr_shadow ept vpid ept_ad \n",
      "                          fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid r\n",
      "                          dseed adx smap clflushopt clwb sha_ni xsaveopt xsavec \n",
      "                          xgetbv1 xsaves avx_vnni vnmi umip waitpkg gfni vaes vp\n",
      "                          clmulqdq rdpid movdiri movdir64b fsrm md_clear seriali\n",
      "                          ze flush_l1d arch_capabilities\n",
      "Virtualization features:  \n",
      "  Virtualization:         VT-x\n",
      "  Hypervisor vendor:      Microsoft\n",
      "  Virtualization type:    full\n",
      "Caches (sum of all):      \n",
      "  L1d:                    768 KiB (16 instances)\n",
      "  L1i:                    512 KiB (16 instances)\n",
      "  L2:                     32 MiB (16 instances)\n",
      "  L3:                     36 MiB (1 instance)\n",
      "NUMA:                     \n",
      "  NUMA node(s):           1\n",
      "  NUMA node0 CPU(s):      0-31\n",
      "Vulnerabilities:          \n",
      "  Gather data sampling:   Not affected\n",
      "  Itlb multihit:          Not affected\n",
      "  L1tf:                   Not affected\n",
      "  Mds:                    Not affected\n",
      "  Meltdown:               Not affected\n",
      "  Mmio stale data:        Not affected\n",
      "  Reg file data sampling: Mitigation; Clear Register File\n",
      "  Retbleed:               Mitigation; Enhanced IBRS\n",
      "  Spec rstack overflow:   Not affected\n",
      "  Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prct\n",
      "                          l\n",
      "  Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointe\n",
      "                          r sanitization\n",
      "  Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditiona\n",
      "                          l; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\n",
      "  Srbds:                  Not affected\n",
      "  Tsx async abort:        Not affected\n"
     ]
    }
   ],
   "source": [
    "# check CPU details\n",
    "! lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82bb7e1f-e3c7-42f8-9f88-3585f63f08a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor: x86_64\n",
      "Physical cores: 16\n",
      "Total cores: 32\n",
      "Max Frequency: 0.00 MHz\n",
      "Sun Jan  4 18:36:02 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.75                 Driver Version: 566.24         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...    On  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   49C    P5              4W /   80W |     685MiB /   8188MiB |      5%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import psutil  # you may need to install: pip install psutil\n",
    "\n",
    "print(f\"Processor: {platform.processor()}\")\n",
    "print(f\"Physical cores: {psutil.cpu_count(logical=False)}\")\n",
    "print(f\"Total cores: {psutil.cpu_count(logical=True)}\")\n",
    "print(f\"Max Frequency: {psutil.cpu_freq().max:.2f} MHz\")\n",
    "\n",
    "# check GPU details\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba34833-3574-421b-81f7-bd2fd38190cd",
   "metadata": {},
   "source": [
    "## Optimized Dask Cluster Configuration\n",
    "\n",
    "The next cell initializes a Dask `LocalCluster` with **optimized settings for your system**:\n",
    "\n",
    "- **3 workers** √ó **3GB** = 9GB total (optimal for 15.47 GB RAM)\n",
    "- **2 threads per worker** for balanced parallelism\n",
    "- **Memory management thresholds** to prevent worker crashes:\n",
    "  - Starts managing memory at 85%\n",
    "  - Spills to disk at 90%\n",
    "  - Pauses worker at 95%\n",
    "\n",
    "This configuration was determined by `system_testing.ipynb` and eliminates memory warnings. The cluster can be reused for all modules except deduplication (which requires a GPU cluster).\n",
    "\n",
    "üí° **Monitor your cluster**: The dashboard link will be shown after initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15f7f130-88b8-4e2c-b2f0-1a26bcfebcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dask cluster ready!\n",
      "Workers: 3\n",
      "Total memory: 9 GB\n",
      "üìä Dashboard: http://127.0.0.1:8787/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/distributed/worker_memory.py:508: FutureWarning: Parameter memory_target_fraction has been deprecated and will be removed in a future version; please use dask config key distributed.worker.memory.target instead\n",
      "  warnings.warn(\n",
      "/home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/distributed/worker_memory.py:508: FutureWarning: Parameter memory_spill_fraction has been deprecated and will be removed in a future version; please use dask config key distributed.worker.memory.spill instead\n",
      "  warnings.warn(\n",
      "/home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/distributed/worker_memory.py:508: FutureWarning: Parameter memory_pause_fraction has been deprecated and will be removed in a future version; please use dask config key distributed.worker.memory.pause instead\n",
      "  warnings.warn(\n",
      "/home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/distributed/worker_memory.py:508: FutureWarning: Parameter memory_target_fraction has been deprecated and will be removed in a future version; please use dask config key distributed.worker.memory.target instead\n",
      "  warnings.warn(\n",
      "/home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/distributed/worker_memory.py:508: FutureWarning: Parameter memory_spill_fraction has been deprecated and will be removed in a future version; please use dask config key distributed.worker.memory.spill instead\n",
      "  warnings.warn(\n",
      "/home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/distributed/worker_memory.py:508: FutureWarning: Parameter memory_pause_fraction has been deprecated and will be removed in a future version; please use dask config key distributed.worker.memory.pause instead\n",
      "  warnings.warn(\n",
      "/home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/distributed/worker_memory.py:508: FutureWarning: Parameter memory_target_fraction has been deprecated and will be removed in a future version; please use dask config key distributed.worker.memory.target instead\n",
      "  warnings.warn(\n",
      "/home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/distributed/worker_memory.py:508: FutureWarning: Parameter memory_spill_fraction has been deprecated and will be removed in a future version; please use dask config key distributed.worker.memory.spill instead\n",
      "  warnings.warn(\n",
      "/home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/distributed/worker_memory.py:508: FutureWarning: Parameter memory_pause_fraction has been deprecated and will be removed in a future version; please use dask config key distributed.worker.memory.pause instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Optimal Dask Configuration for Your System\n",
    "# Based on system_testing.ipynb recommendations\n",
    "# System: 15.47 GB RAM, 32 cores\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Create optimized cluster\n",
    "cluster = LocalCluster(\n",
    "    n_workers=3,                    # Optimized for your system\n",
    "    threads_per_worker=2,           # Balance between parallelism and memory\n",
    "    memory_limit='3GB',             # Per worker memory limit\n",
    "    memory_target_fraction=0.85,    # Start memory management at 85%\n",
    "    memory_spill_fraction=0.90,     # Spill to disk at 90%\n",
    "    memory_pause_fraction=0.95,     # Pause worker at 95%\n",
    "    processes=True                  # Use separate processes for better memory isolation\n",
    ")\n",
    "\n",
    "client = Client(cluster)\n",
    "print(\"‚úì Dask cluster ready!\")\n",
    "print(f\"Workers: {len(cluster.workers)}\")\n",
    "print(f\"Total memory: {len(cluster.workers) * 3} GB\")\n",
    "print(f\"üìä Dashboard: {client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b1c0923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install nemo-curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad38150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import IPython\n",
    "#IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea8f3322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeMo Curator version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import nemo_curator\n",
    "print(f\"NeMo Curator version: {nemo_curator.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794d3c8c-df05-4291-a17e-beda92da813f",
   "metadata": {},
   "source": [
    "Lear more about Nemo Curator's CPU and GPU Modules with Dask in the dedicated [documentation](https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/cpuvsgpu.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebab4e1-a211-4845-82b3-f779defda8b1",
   "metadata": {},
   "source": [
    "## 1.1 Multilingual Datasets\n",
    "\n",
    "In this notebook, we will use a subset of the [MC4](https://huggingface.co/datasets/allenai/c4), the C4 Multilingual Dataset.\n",
    "\n",
    "For the sake of this exercice, to create a more diverse dataset:\n",
    "- We merged Spanish and French samples (100 per language)\n",
    "- We duplicated all samples (making 200 samples per language)\n",
    "- We shuffled the samples\n",
    "\n",
    "So, we have 400 samples, 200 from each language. The structure is a JSON format with 3 filed: `text`, `timestamp` and `url`. \n",
    "\n",
    "Let's have a look at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a8427d4-8e4d-4731-8e5b-b417f9165f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dataset file path\n",
    "multilingual_data = \"./original_data/file.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "271915b3-498f-4445-a415-05d8b599e566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 ./original_data/file.json\n"
     ]
    }
   ],
   "source": [
    "# check number of samples\n",
    "! wc -l {multilingual_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72dee403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 ./original_data/file.json\n"
     ]
    }
   ],
   "source": [
    "# Count lines in a file\n",
    "with open(multilingual_data, 'r', encoding='utf-8') as f:\n",
    "    line_count = sum(1 for line in f)\n",
    "print(f\"{line_count} {multilingual_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b224be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"Dragon Ball: Le 20e film de la sage sortira le 14 d\\u00e9cembre, premi\\u00e8re image teaser sur Buzz, insolite et culture\\nDragon Ball: Le 20e film de la sage sortira le 14 d\\u00e9cembre, premi\\u00e8re image teaser\\nLe 20e film Dragon Ball sortira le vendredi 14 d\\u00e9cembre 2018. La premi\\u00e8re affiche teaser montre un Gok\\u00fb jeune adulte, environ celui de la fin de Dragon Ball et le d\\u00e9but de Dragon Ball Z. \\u00c0 lire aussi >>> Le gouvernement mexicain pr\\u00e9voit la diffusion sur place publique des \\u00e9pisodes 130 et 131 de Dragon [\\u2026]...\\nLire la suite du buzz sur bleachmx Source : bleachmx - 12/03/2018 22:31 - trending_up142\\nfilm comm\\u00e9moration Akira Toriyama dbz dragon ball Dragon Ball Super dragon ball z affiche Dragon Ball Super Anime V Jump D\\u00e9cembre 2018 Dragon Ball Z Battle of Gods Dragon Ball Z Fukkatsu No [F] Dragon Ball Z La R\\u00e9surrection de [F] Potins Films\\nLe site Deadline indique ce Jeudi que le film d\\u2019animation Dragon Ball Super \\u2013 Broly a rapport\\u00e9, selon les estimations, plus de 7 millions de dollars pour sa premi\\u00e8re journ\\u00e9e d\\u2019exploitation aux \\u00c9tats-Unis. Il pr\\u00e9voit que le film rapporte plus de 15 millions de dollars pour sa premi\\u00e8re semaine. \\u00c0 voir aussi >>> Dragon Ball [\\u2026] ...\\nSource : bleachmx - 18/01/2019 00:16 - trending_up 15\\nDragon Ball Super Broly: Le film d\\u2019animation pr\\u00e9vu dans 90 pays - bleachmx\\nDragon Ball Super \\u2013 Broly: Le film adapt\\u00e9 en manga et en light novel - bleachmx\\nLors du DRAGON BALL Games SUPER Showcase il a \\u00e9t\\u00e9 annonc\\u00e9 que le je vid\\u00e9o Super Dragon Ball Heroes: World Mission sortira sur Nintendo Switch et PC (via Steam) le 5 avril 2019 en occident. Le trailer sous-titr\\u00e9 a \\u00e9t\\u00e9 pr\\u00e9sent\\u00e9 ainsi qu\\u2019une vid\\u00e9o de gameplays. La vid\\u00e9o pr\\u00e9sente Cirrus (Shiirus \\u2013 Shiirasu), le nouveau [\\u2026] ...\\nSource : bleachmx - 15/01/2019 01:45 - trending_up 22\\nSuper Dragon Ball Heroes: World Mission : Deuxi\\u00e8me trailer du jeu vid\\u00e9o - bleachmx\\nDes avants-premi\\u00e8res pour le film Dragon Ball Super Broly dans les cin\\u00e9mas CGR\\nLe film Dragon Ball Super Broly est attendu comme le messie par les fans de l'univers d'Akira Toriyama. Apr\\u00e8s les annonces d'avants-premi\\u00e8res au ...\\nSource : manga-news - 08/01/2019 11:00 - trending_up 15\\nLes avants-premi\\u00e8res fran\\u00e7aises du film Dragon Ball Super Broly d\\u00e9voil\\u00e9es\\nParticuli\\u00e8rement attendu, le film Dragon Ball Super Broly sortira dans les cin\\u00e9mas de France le 13 mars. Mais avant \\u00e7a, plusieurs avant-premi\\u00e8res ...\\nSource : manga-news - 04/01/2019 15:16 - trending_up 16\\nAvant-premi?res de Dragon Ball Super Broly dans toute la France (Les 23 et 24 janvier 2019)\\nLes 23 et 24 janvier 2019 Le film Dragon Ball Super Broly est sorti au Japon le 13 d\\u00e9cembre 2018 et para\\u00eetra \\u00e9galement au cin\\u00e9ma en France le 13 mars 2019. Avant cette date, plusieurs avant-premi\\u00e8res son programm\\u00e9s en janvier. Mercredi 23 et jeudi 24 janvier Le Grand Rex (Paris) Jeudi 24 janvier Path\\u00e9 : St Herblain, Path\\u00e9 Nantes-Atlantis (44, Loire-Atlantique) Lab\\u00e8ge, Gaumont (31, Haute-Garonne) Toulouse, Gaumont Wilson ( 31, Haute-Garonne) Belle-Epine, Path\\u00e9 (94, Thiais) ... ...\\nSource : animint - 04/01/2019 14:46 - trending_up 26\\nOfficiel : le film Dragon Ball Super : Broly en France le 13 mars (VOSTFR et VF) ! - animeland\\nDragon Ball Super \\u2013 Broly : Le film dans les cin\\u00e9mas fran\\u00e7ais en XXXX 2019 - bleachmx\\nDragon Ball Super \\u2013 Broly : Le film dans les cin\\u00e9mas fran\\u00e7ais en XXXX 2019 ? - bleachmx\\nSuper Dragon Ball Heroes : \\u00c9pisode 6, preview et date de sortie de l\\u2019\\u00e9pisode 7\\nLe site officiel de l\\u2019anime promotionnel Super Dragon Ball Heroes: Universal Mission a mis en ligne une affiche, un synopsis ainsi qu\\u2019un teaser vid\\u00e9o pour l\\u2019\\u00e9pisode 7. L\\u2019\\u00e9pisode 7 sortira le 10 janvier 2019. \\u00c0 voir aussi >>> Dragon Ball Super \\u2013 Broly : 2 milliards de yens de recettes, le film bat des records [\\u2026] ...\\nSource : bleachmx - 30/12/2018 00:02 - trending_up 39\\nSuper Dragon Ball Heroes : \\u00c9pisode 5, preview date de sortie de l\\u2019\\u00e9pisode 6 - bleachmx\\nLe jeu Super Dragon Ball Heroes : World Mission dat\\u00e9 en Occident - manga-news\\nDragon Ball Super \\u2013 Broly : 2 milliards de yens de recettes, le film bat des records au box office japonais\\nLe journal Mainichi Shimbun a annonc\\u00e9 dans ses pages que le film d\\u2019animation Dragon Ball Super: Broly a rapport\\u00e9 2 milliards de yens (18,1 millions de dollars $) en 11 jours au box office japonais. Il est le film de la franchise a avoir atteint le plus rapidement la barre des 2 milliards de yens. [\\u2026] ...\\nSource : bleachmx - 26/12/2018 17:46 - trending_up 15\\nSuper Dragon Ball Heroes : \\u00c9pisode 6, preview et date de sortie de l\\u2019\\u00e9pisode 7 - bleachmx\\nDragon Ball Super \\u2013 Broly: Un Vegeta enfin respect\\u00e9, les premi\\u00e8res minutes \\u00e9mouvantes et l\\u2019avant-premi\\u00e8re mondiale - bleachmx\\nLe film animation Dragon Ball Super Broly rapporte 15 millions \\u20ac en 11 jours au Japon\\nLe webjournal japonais Mantan Web a r\\u00e9v\\u00e9l\\u00e9 que le film animation Dragon Ball Super Broly a g\\u00e9n\\u00e9r\\u00e9 2 milliards de yen de recettes (15,9 millions \\u20ac) en 11 jours ! ...\\nSource : adala-news - 25/12/2018 11:01 - trending_up 43\\nLe film animation Dragon Ball Super Broly, en Trailer 2 - adala-news\\nLe film animation Dragon Ball Super Broly, en Trailer 3 - adala-news\\nTop 5 des films animation Dragon Ball les plus populaires au Japon\\nLa Toei a d\\u00e9voil\\u00e9 le classement des films animation Dragon Ball pr\\u00e9f\\u00e9r\\u00e9s des japonais ! ...\\nSource : adala-news - 24/12/2018 00:03 - trending_up 28\\nPremiers chiffres du film animation Dragon Ball Super Broly au Japon - adala-news\\nLe film animation Dragon Ball Super Broly rapporte 15 millions \\u20ac en 11 jours au Japon - adala-news\\nDragon Ball Super Chapitre Scan 043 VF\\nLe chapitre 43 de Dragon Ball Super pour ce mois de d\\u00e9cembre 2018 pour cl\\u00f4turer l\\u2019ann\\u00e9e. Le film Dragon Ball Super \\u2013 Broly est enfin sorti au Japon et le manga entame l\\u2019Arc du Prisonnier de la Patrouille Galactique. C\\u2019est donc une nouvelle fois le film qui fait la couverture du magazine. Toutes les pages [\\u2026] ...\\nSource : bleachmx - 21/12/2018 04:00 - trending_up 38\",\n",
      "  \"timestamp\": 1548042730000,\n",
      "  \"url\": \"https://cultinfos.com/buzz/332814-dragon-ball-20e-film-de-sage-sortira-14-decembre-premiere-image-teaser\"\n",
      "}\n",
      "{\n",
      "  \"text\": \"Cours D'histoire Des \\u00c9tats Europ\\u00e9ens: Depuis Le Bouleversement De L'empire Romain D'occident Jusqu'en 1789, Volume 1 (French Edition) \\u27a0 Maximilien Samson Frederic Schoell | \\u0411\\u0443\\u043a\\u0432\\u043e\\u0435\\u0434 ISBN 978-5-8792-8565-9\\nCours D'histoire Des \\u00c9tats Europ\\u00e9ens: Depuis Le Bouleversement De L'empire Romain D'occident Jusqu'en 1789, Volume 1 (French Edition)\\nISBN: 978-5-8792-8565-9\\n\\u041a\\u043e\\u0434: pod 1702886\\n\\u0410\\u0432\\u0442\\u043e\\u0440\\u044b: Samson, Schoell\\n\\u0421 \\u0442\\u043e\\u0432\\u0430\\u0440\\u043e\\u043c \\u00abCours D'histoire Des \\u00c9tats Europ\\u00e9ens: Depuis Le Bouleversement De L'empire Romain D'occident Jusqu'en 1789, Volume 1 (French Edition)\\u00bb \\u0447\\u0430\\u0441\\u0442\\u043e \\u043f\\u043e\\u043a\\u0443\\u043f\\u0430\\u044e\\u0442\\n\\u0415\\u0441\\u043b\\u0438 \\u0412\\u044b \\u043e\\u0431\\u043d\\u0430\\u0440\\u0443\\u0436\\u0438\\u043b\\u0438 \\u043e\\u0448\\u0438\\u0431\\u043a\\u0443 \\u0432 \\u043e\\u043f\\u0438\\u0441\\u0430\\u043d\\u0438\\u0438 \\u0442\\u043e\\u0432\\u0430\\u0440\\u0430 \\u00abCours D'histoire Des \\u00c9tats Europ\\u00e9ens: Depuis Le Bouleversement De L'empire Romain D'occident Jusqu'en 1789, Volume 1 (French Edition)\\u00bb Maximilien Samson Frederic Schoell, \\u0432\\u044b\\u0434\\u0435\\u043b\\u0438\\u0442\\u0435 \\u0435\\u0451 \\u043c\\u044b\\u0448\\u043a\\u043e\\u0439 \\u0438 \\u043d\\u0430\\u0436\\u043c\\u0438\\u0442\\u0435: Ctrl+Enter. \\u0421\\u043f\\u0430\\u0441\\u0438\\u0431\\u043e!\",\n",
      "  \"timestamp\": 1547767539000,\n",
      "  \"url\": \"https://www.bookvoed.ru/book?id=1433688\"\n",
      "}\n",
      "{\n",
      "  \"text\": \"Se realiz\\u00f3 una jornada de promoci\\u00f3n del buentrato hacia los adultos mayores en R\\u00edo Gallegos - Ministerio de Desarrollo Social\\nSe realiz\\u00f3 una jornada de promoci\\u00f3n del buentrato hacia los adultos mayores en R\\u00edo Gallegos\\nFue el fin de semana pasado en la capital santacruce\\u00f1a. El s\\u00e1bado tuvo lugar una capacitaci\\u00f3n sobre deporte social. El domingo m\\u00e1s de 350 personas participaron de actividades recreativas, charlas y talleres.\\nSe realiz\\u00f3 la jornada de promoci\\u00f3n del buentrato hacia los adultos mayores en R\\u00edo Gallegos.\\nFue el fin de semana pasado en el SUM de Vialidad Nacional de la capital santacruce\\u00f1a.\\nEl s\\u00e1bado tuvo lugar una capacitaci\\u00f3n sobre deporte social.\\nEl domingo m\\u00e1s de 350 personas participaron de actividades recreativas, charlas y talleres.\\nEl Ministerio de Desarrollo Social a trav\\u00e9s del Centro de Referencia de Santa Cruz (CDR) llev\\u00f3 a cabo una jornada de promoci\\u00f3n del buentrato hacia los adultos mayores en la ciudad de R\\u00edo Gallegos. Las actividades se desarrollaron durante el fin de semana pasado en el marco del \\u201cD\\u00eda mundial contra el abuso y el maltrato a los mayores\\u201d.\\nEl s\\u00e1bado, desde el proyecto \\u201cMadurar en Positivo\\u201d perteneciente al Plan Nacional de Deporte Social, se realiz\\u00f3 una capacitaci\\u00f3n en el sal\\u00f3n de usos m\\u00faltiples de Vialidad Nacional dirigida a m\\u00e1s de 50 ciudadanos. Asimismo el domingo, m\\u00e1s de 350 adultos mayores participaron de actividades recreativas y disfrutaron de juegos de kerm\\u00e9s, tejo, sapo, bingo, ping pong, v\\u00f3leibol adaptado, bowling y talleres.\\nLa Subsecretar\\u00eda de Responsabilidad Social brind\\u00f3 una charla para concientizar acerca de la importancia de reducir los factores de riesgo asociados a la vida sedentaria, promoviendo h\\u00e1bitos saludables para mejorar la calidad de vida. Por su parte, la Direcci\\u00f3n de Deporte Social llev\\u00f3 a cabo el \\u201ctaller de la risa, donde a trav\\u00e9s de t\\u00e9cnicas de juegos la directora Nacional, Patricia Borrillo habl\\u00f3 de las herramientas para que las personas mayores aprendan a distanciarse de las preocupaciones y cuenten con distintas alternativas ante la resoluci\\u00f3n de un problema.\\nA su vez, los presentes pudieron acceder de manera gratuita, a una unidad m\\u00f3vil sanitaria del programa \\u201cArgentina Sonr\\u00ede\\u201d del Ministerio de Salud de Naci\\u00f3n. Tambi\\u00e9n se hicieron controles de glucemia, presi\\u00f3n y vacunaci\\u00f3n. Por la tarde, se presentaron diversos n\\u00fameros art\\u00edsticos para compartir en familia: el grupo Papelnonos, el coro del Centro de Jubilados \\u201cEl Despertar\\u201d y el grupo de danzas del Centro de Jubilados \\u201cLa Amistad\\u201d y del centro \\u201cEncuentro de Amigos\\u201d.\\nAl finalizar la jornada, el referente del CDR local, Ariel Fern\\u00e1ndez, destac\\u00f3 las acciones conjuntas entre todos los organismos presentes, afirmando que la actividad no tiene que ver con el trabajo de un ministerio sino con \\u201cun proyecto pol\\u00edtico con una mirada integral\\u201d.\\nLa cartera social a trav\\u00e9s de los CDR y en articulaci\\u00f3n con todos los organismos del Gobierno nacional, concibe a las personas mayores como protagonistas del cambio social y promotores de la cultura del buentrato. De esta manera, se trabajan acciones diarias en pos de mejorar la calidad de vida de los adultos mayores.\\n\\\"Sean transgresores, no le digan que s\\u00ed a todo. Acu\\u00e9rdense la experiencia no se jubila. El desaf\\u00edo es que todos los adultos mayores sean sujetos activos de derecho\\\".\\nLos mayores ense\\u00f1an a los chicos los juegos de su infancia\\nEl Hogar Balestra cumpli\\u00f3 90 a\\u00f1os\\nConoc\\u00e9 m\\u00e1s sobre Adultos mayores\\nEncontr\\u00e1 d\\u00f3nde consultar sobre Adultos mayores\",\n",
      "  \"timestamp\": 1524296308000,\n",
      "  \"url\": \"http://www.desarrollosocial.gob.ar/noticias/se-realizo-una-jornada-de-promocion-del-buentrato-hacia-los-adultos-mayores-en-rio-gallegos/\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Read first 3 lines\n",
    "with open(multilingual_data, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 3:\n",
    "            break\n",
    "        print(json.dumps(json.loads(line), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d22236-bc9e-4668-ae3c-d35bdbf6f5de",
   "metadata": {},
   "source": [
    "Notice, **languages are not annotated in the dataset**, allowing us to leverage AI-based language separation later in the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f276db-4372-4775-b0a0-08b94fe5c3f9",
   "metadata": {},
   "source": [
    "Let's now create a document dataset from a pandas data frame. For more information on the arguments see Dask‚Äôs from_pandas documentation\n",
    "\n",
    "NeMo Curator's `DocumentDataset` employs Dask's distributed dataframes to mangage large datasets across multiple nodes and allow for easy restarting of interrupted curation. `DocumentDataset` supports reading and writing to sharded *jsonl* and *parquet* files both on local disk and from remote sources such as S3 bukets.\n",
    "\n",
    "Let's load our multilingual dataset with Nemo Curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3d2d25e-5a7d-4639-9e17-bf09759726c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e230e1fb-898a-47e1-84d0-a2578cd01a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 files with blocksize='1gb' / files_per_partition=None\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.datasets import DocumentDataset\n",
    "\n",
    "multilingual_data_path = \"./original_data\"\n",
    "multilingual_dataset = DocumentDataset.read_json(\n",
    "    multilingual_data_path, add_filename=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2d0481e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nemo_curator                       0.9.0\n"
     ]
    }
   ],
   "source": [
    "! pip list | grep nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a183c62b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453208e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b89f475e-0284-4681-aec1-a536e0bd751c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Dragon Ball: Le 20e film de la sage sortira le...</td>\n",
       "      <td>2019-01-21 03:52:10</td>\n",
       "      <td>https://cultinfos.com/buzz/332814-dragon-ball-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Cours D'histoire Des √âtats Europ√©ens: Depuis L...</td>\n",
       "      <td>2019-01-17 23:25:39</td>\n",
       "      <td>https://www.bookvoed.ru/book?id=1433688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Se realiz√≥ una jornada de promoci√≥n del buentr...</td>\n",
       "      <td>2018-04-21 07:38:28</td>\n",
       "      <td>http://www.desarrollosocial.gob.ar/noticias/se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name                                               text  \\\n",
       "0  file.json  Dragon Ball: Le 20e film de la sage sortira le...   \n",
       "1  file.json  Cours D'histoire Des √âtats Europ√©ens: Depuis L...   \n",
       "2  file.json  Se realiz√≥ una jornada de promoci√≥n del buentr...   \n",
       "\n",
       "            timestamp                                                url  \n",
       "0 2019-01-21 03:52:10  https://cultinfos.com/buzz/332814-dragon-ball-...  \n",
       "1 2019-01-17 23:25:39            https://www.bookvoed.ru/book?id=1433688  \n",
       "2 2018-04-21 07:38:28  http://www.desarrollosocial.gob.ar/noticias/se...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilingual_dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe1ef92-efd2-434b-96d3-2722bcfcc955",
   "metadata": {},
   "source": [
    "## 1.2 Basic Text cleaning and Unification\n",
    "\n",
    "NeMo Curator provides various `DocumentModifier` implementations such as the `UnicodeReformatter` which uses [ftfy](https://pypi.org/project/ftfy/) (fixes text for you) to resolve all unicode issues in the dataset. \n",
    "\n",
    "It is also possible to implement your custom text cleaner using `DocumentModifier`. For instance, we can standardize inconsistent quotation marks that appear very often in curated large dataset, remove HTML, URLs, and email tags, etc.\n",
    "\n",
    "\n",
    "Let's first create the output folders to save the cleaned step outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf4df995-300b-470d-847b-78839ca859b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set dataset file path\n",
    "curated_data_path = \"./curated\"\n",
    "clean_and_unify_data_path = os.path.join(curated_data_path, \"01_clean_and_unify\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(curated_data_path, exist_ok=True)\n",
    "os.makedirs(clean_and_unify_data_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb4a71-2347-4cd3-a5e9-0711434a467d",
   "metadata": {},
   "source": [
    "Let's now implement a custom text cleaner `QuotationTagUnifier`.\n",
    "\n",
    "It is designed to modify text documents by normalizing quotation marks and removing unwanted elements. \n",
    "\n",
    "The result is a cleaned and standardized text output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643dc7a4-d9e7-464d-864b-baa653cab173",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bd0a641-4063-4781-8d0d-e5e697ad50f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import dask\n",
    "import pandas as pd\n",
    "from nemo_curator.modifiers import DocumentModifier, UnicodeReformatter\n",
    "from nemo_curator.modules.modify import Modify\n",
    "\n",
    "\n",
    "class QuotationTagUnifier(DocumentModifier):\n",
    "    def modify_document(self, text: str) -> str:\n",
    "        text = text.replace(\"‚Äò\", \"'\").replace(\"‚Äô\", \"'\")\n",
    "        text = text.replace(\"‚Äú\", '\"').replace(\"‚Äù\", '\"')\n",
    "        text = text.replace(\"\\t\", \" \")\n",
    "        text = re.sub(\n",
    "            r\"(<[^>]+>)|(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)\",\n",
    "            \"\",\n",
    "            text,\n",
    "        )\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9838a49-15d8-4bfd-b5ea-c6c7d09652ba",
   "metadata": {},
   "source": [
    "Next, we can chain modifiers together using the `Sequential` class, which takes a list of operations to be run sequentially and applies them to a given `DocumentDataset`.ipynb_checkpoints/\n",
    " \n",
    "Let's call this sequence the `cleaners`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35cd6bbb-0be0-4089-88d7-e46ca1d21c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import Sequential\n",
    "\n",
    "cleaners = Sequential(\n",
    "    [\n",
    "        # Apply: Unify all the quotation marks and remove tags\n",
    "        Modify(QuotationTagUnifier()),\n",
    "        # Apply: Unify all unicode\n",
    "        Modify(UnicodeReformatter()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d43d8e7-a892-46e8-9719-1ea2363d6577",
   "metadata": {},
   "source": [
    "Let's run that on a toy example with few sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cf51cce-2ff4-4ded-a54d-a12773a8074f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ryan went out to play ‚Äòfootbal‚Äô</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He is very  \\t  happy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Visit &lt;a href='www.example.com'&gt;example.com&lt;/a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                    Ryan went out to play ‚Äòfootbal‚Äô\n",
       "1                             He is very  \\t  happy.\n",
       "2  Visit <a href='www.example.com'>example.com</a..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the toy samples\n",
    "dataframe_toy = pd.DataFrame(\n",
    "    {\n",
    "        \"text\": [\n",
    "            \"Ryan went out to play ‚Äòfootbal‚Äô\",\n",
    "            \"He is very  \\t  happy.\",\n",
    "            \"Visit <a href='www.example.com'>example.com</a> for more information or contact us at info@example.com\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset_toy = DocumentDataset(dask.dataframe.from_pandas(dataframe_toy, npartitions=1))\n",
    "\n",
    "# check the samples\n",
    "dataset_toy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c044dd9",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Reconfigure Cluster for PII Processing\n",
    "\n",
    "PII processing with spaCy models is **memory-intensive**. We need to reconfigure the cluster with:\n",
    "- **Fewer workers** (2 instead of 3)\n",
    "- **More memory per worker** (6GB instead of 3GB)\n",
    "\n",
    "This prevents workers from being killed due to memory exhaustion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a62e8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconfiguring cluster for PII processing...\n",
      "Closing current cluster...\n",
      "Creating PII-optimized cluster...\n",
      "\n",
      "‚úì PII-optimized cluster ready!\n",
      "  Workers: 2 √ó 6GB = 12GB total\n",
      "  üìä Dashboard: http://127.0.0.1:8787/status\n",
      "\n",
      "This configuration prevents memory issues during PII processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/distributed/worker_memory.py:508: FutureWarning: Parameter memory_target_fraction has been deprecated and will be removed in a future version; please use dask config key distributed.worker.memory.target instead\n",
      "  warnings.warn(\n",
      "/home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/distributed/worker_memory.py:508: FutureWarning: Parameter memory_spill_fraction has been deprecated and will be removed in a future version; please use dask config key distributed.worker.memory.spill instead\n",
      "  warnings.warn(\n",
      "/home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/distributed/worker_memory.py:508: FutureWarning: Parameter memory_pause_fraction has been deprecated and will be removed in a future version; please use dask config key distributed.worker.memory.pause instead\n",
      "  warnings.warn(\n",
      "/home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/distributed/worker_memory.py:508: FutureWarning: Parameter memory_target_fraction has been deprecated and will be removed in a future version; please use dask config key distributed.worker.memory.target instead\n",
      "  warnings.warn(\n",
      "/home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/distributed/worker_memory.py:508: FutureWarning: Parameter memory_spill_fraction has been deprecated and will be removed in a future version; please use dask config key distributed.worker.memory.spill instead\n",
      "  warnings.warn(\n",
      "/home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/distributed/worker_memory.py:508: FutureWarning: Parameter memory_pause_fraction has been deprecated and will be removed in a future version; please use dask config key distributed.worker.memory.pause instead\n",
      "  warnings.warn(\n",
      "2026-01-04 18:36:51,534 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:44371' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('getitem-fused-assign-352b11840df799f4ba7768c41126ade2', 0)} (stimulus_id='handle-worker-cleanup-1767569811.5338256')\n"
     ]
    }
   ],
   "source": [
    "# Reconfigure cluster for memory-intensive PII processing\n",
    "print(\"Reconfiguring cluster for PII processing...\")\n",
    "print(\"Closing current cluster...\")\n",
    "\n",
    "client.close()\n",
    "cluster.close()\n",
    "#time.sleep(3)\n",
    "\n",
    "# Create larger-memory cluster optimized for PII\n",
    "print(\"Creating PII-optimized cluster...\")\n",
    "cluster = LocalCluster(\n",
    "    n_workers=2,                    # Reduced from 3 to 2 workers\n",
    "    threads_per_worker=2,\n",
    "    memory_limit='6GB',             # Increased from 3GB to 6GB per worker\n",
    "    memory_target_fraction=0.85,\n",
    "    memory_spill_fraction=0.90,\n",
    "    memory_pause_fraction=0.95,\n",
    "    processes=True\n",
    ")\n",
    "\n",
    "client = Client(cluster)\n",
    "print(\"\\n‚úì PII-optimized cluster ready!\")\n",
    "print(f\"  Workers: {len(cluster.workers)} √ó 6GB = {len(cluster.workers) * 6}GB total\")\n",
    "print(f\"  üìä Dashboard: {client.dashboard_link}\")\n",
    "print(\"\\nThis configuration prevents memory issues during PII processing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705224b3-31d4-4702-97a6-8e2f572b7a6f",
   "metadata": {},
   "source": [
    "Now, let's apply our sequence of cleaners to the toy samples. To execute this sequence on the Dask cluster, we use `.persist()`, which keeps the transformed data in memory for optimized processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc070516-28b6-425d-a9b6-d3792f3ed8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_clean_and_unify = cleaners(dataset_toy).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855bc55b-3a28-4825-baa3-cea46491c084",
   "metadata": {},
   "source": [
    "Let's check the output.\n",
    "\n",
    "Expected output are samples with normalized quotations, removed tabs and HTML, URL and Email tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d78c5a8-4548-4106-8f4d-cab9ffc18eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ryan went out to play 'footbal'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He is very     happy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Visit example.com for more information or cont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                    Ryan went out to play 'footbal'\n",
       "1                              He is very     happy.\n",
       "2  Visit example.com for more information or cont..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check cleaned toy samples\n",
    "dataset_test_clean_and_unify.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52122f-9a73-421a-a3b7-0b4dd234cb2b",
   "metadata": {},
   "source": [
    "Now, let's apply this cleaning step to our multilingual dataset. We can achieve this by creating a sequence of curation steps, starting with the cleaning sequence as the first function in our data curation pipeline.\n",
    "\n",
    "Run the next cell to create the cleaning step as a function that would be the first curation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e999870-cbe9-4d28-8b02-8d6e6a8f80c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the sequence of cleaning operations as a function\n",
    "def clean_and_unify(dataset: DocumentDataset) -> DocumentDataset:\n",
    "    cleaners = Sequential(\n",
    "        [\n",
    "            # Apply: Unify all the quotation marks and remove tags\n",
    "            Modify(QuotationTagUnifier()),\n",
    "            # Apply: Unify all unicode\n",
    "            Modify(UnicodeReformatter()),\n",
    "        ]\n",
    "    ) \n",
    "    return cleaners(dataset)\n",
    "\n",
    "\n",
    "# sequence of data curation setps. so far, only cclean_and_unify is defined\n",
    "curation_steps = Sequential([clean_and_unify])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af0dae-640c-48ad-9023-25d19a4caa80",
   "metadata": {},
   "source": [
    "Let's now execute this step on out multilingual dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e5a3590-7749-4009-89a1-0f2d30870c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the curation steps to the multilingual dataset\n",
    "dataset_clean_and_unify = curation_steps(multilingual_dataset).persist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bee639b-99d7-4026-a87a-90c8dd960335",
   "metadata": {},
   "source": [
    "Let's check some outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc175867-7d83-4452-98df-f79bce5d1297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Dragon Ball: Le 20e film de la sage sortira le...</td>\n",
       "      <td>2019-01-21 03:52:10</td>\n",
       "      <td>https://cultinfos.com/buzz/332814-dragon-ball-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Cours D'histoire Des √âtats Europ√©ens: Depuis L...</td>\n",
       "      <td>2019-01-17 23:25:39</td>\n",
       "      <td>https://www.bookvoed.ru/book?id=1433688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Se realiz√≥ una jornada de promoci√≥n del buentr...</td>\n",
       "      <td>2018-04-21 07:38:28</td>\n",
       "      <td>http://www.desarrollosocial.gob.ar/noticias/se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Restaurantes con Web Y Telefono Y Dias Y Horar...</td>\n",
       "      <td>2020-08-11 16:33:05</td>\n",
       "      <td>http://mendoza.guia.clarin.com/restaurantes-co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Responsable qualit√© - Int√©rim : Emploi et recr...</td>\n",
       "      <td>2020-08-07 01:17:37</td>\n",
       "      <td>https://images3.meteojob.com/Emploi-Interim-Re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name                                               text  \\\n",
       "0  file.json  Dragon Ball: Le 20e film de la sage sortira le...   \n",
       "1  file.json  Cours D'histoire Des √âtats Europ√©ens: Depuis L...   \n",
       "2  file.json  Se realiz√≥ una jornada de promoci√≥n del buentr...   \n",
       "3  file.json  Restaurantes con Web Y Telefono Y Dias Y Horar...   \n",
       "4  file.json  Responsable qualit√© - Int√©rim : Emploi et recr...   \n",
       "\n",
       "            timestamp                                                url  \n",
       "0 2019-01-21 03:52:10  https://cultinfos.com/buzz/332814-dragon-ball-...  \n",
       "1 2019-01-17 23:25:39            https://www.bookvoed.ru/book?id=1433688  \n",
       "2 2018-04-21 07:38:28  http://www.desarrollosocial.gob.ar/noticias/se...  \n",
       "3 2020-08-11 16:33:05  http://mendoza.guia.clarin.com/restaurantes-co...  \n",
       "4 2020-08-07 01:17:37  https://images3.meteojob.com/Emploi-Interim-Re...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_clean_and_unify.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2abe75-18db-44eb-b881-be16d046fb47",
   "metadata": {},
   "source": [
    "We can save the created Document into a json file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d14fede-e581-4235-81c3-7ace79a593a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to disk complete for 1 partition(s)\n"
     ]
    }
   ],
   "source": [
    "# save output to json\n",
    "dataset_clean_and_unify.to_json(clean_and_unify_data_path, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89228aa9-641f-4a1d-96b5-61fccb703832",
   "metadata": {},
   "source": [
    "## Dataset document size Filtering\n",
    "\n",
    "Extremely short documents may lack sufficient context or information for the model to learn meaningful concepts. By filtering out such documents, we can ensure that the data used for training is sufficiently informative and balanced.\n",
    "\n",
    "Let's explore how to apply word counts and filtering using NeMo Curator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f59e1ac-4a33-4305-abe6-25d3e3ab596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "from nemo_curator import ScoreFilter\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.filters import (\n",
    "    DocumentFilter,\n",
    "    RepeatingTopNGramsFilter,\n",
    "    WordCountFilter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89f424f0-2db3-47b4-a919-666aea19d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncompleteDocumentFilter(DocumentFilter):\n",
    "    \"\"\"\n",
    "    If the document doesn't end with a terminating punctuation mark, then discard.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Accepted document terminators.\n",
    "        self._story_terminators = {\".\", \"!\", \"?\", '\"', \"‚Äù\"}\n",
    "\n",
    "    def score_document(self, text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Determines if a document's score is valid based on the last character of the text.\n",
    "        Args:\n",
    "            text (str): The document text.\n",
    "        Returns:\n",
    "            bool: True if the document's score is valid, False otherwise.\n",
    "        \"\"\"\n",
    "        return text.strip()[-1] in self._story_terminators\n",
    "\n",
    "    def keep_document(self, score) -> bool:\n",
    "        return score\n",
    "\n",
    "    # Dask serialization methods for distributed processing\n",
    "    def __hash__(self):\n",
    "        \"\"\"Make the filter hashable for Dask\"\"\"\n",
    "        return hash(frozenset(self._story_terminators))\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Make the filter comparable for Dask\"\"\"\n",
    "        if not isinstance(other, IncompleteDocumentFilter):\n",
    "            return False\n",
    "        return self._story_terminators == other._story_terminators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d402e2b-9b38-4a0d-877b-a62a935f1c89",
   "metadata": {},
   "source": [
    "The following code defines a function, `filter_dataset`, that cleans a `DocumentDataset` by applying several filters:\n",
    "\n",
    "- **Word Count Filter**: Removes documents with fewer than 80 words by default.\n",
    "- **Incomplete Document Filter**: Removes incomplete documents.\n",
    "- **Repeating N-Grams Filters**: Removes documents with excessive repetition of word sequences (2-grams, 3-grams, 4-grams) above certain thresholds (20%, 18%, 16% respectively).\n",
    "\n",
    "These filters are applied sequentially to refine the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c610d1cb-f44f-42fc-bb37-8759a50b1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(dataset: DocumentDataset) -> DocumentDataset:\n",
    "    filters = Sequential(\n",
    "        [\n",
    "            ScoreFilter(\n",
    "                WordCountFilter(min_words=80),\n",
    "                text_field=\"text\",\n",
    "                score_field=\"word_count\",\n",
    "            ),\n",
    "            ScoreFilter(IncompleteDocumentFilter(), text_field=\"text\"),\n",
    "            ScoreFilter(\n",
    "                RepeatingTopNGramsFilter(n=2, max_repeating_ngram_ratio=0.2),\n",
    "                text_field=\"text\",\n",
    "            ),\n",
    "            ScoreFilter(\n",
    "                RepeatingTopNGramsFilter(n=3, max_repeating_ngram_ratio=0.18),\n",
    "                text_field=\"text\",\n",
    "            ),\n",
    "            ScoreFilter(\n",
    "                RepeatingTopNGramsFilter(n=4, max_repeating_ngram_ratio=0.16),\n",
    "                text_field=\"text\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return filters(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ee9ddd-57b3-4cac-8351-fb8b7fb5acef",
   "metadata": {},
   "source": [
    "Let's now apply that on our multilingual dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ef91e32-c9ea-4bf4-b1ab-016ee9413e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the pipeline...\n",
      "CPU times: user 83.4 ms, sys: 16.1 ms, total: 99.5 ms\n",
      "Wall time: 95.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "curation_steps = Sequential([clean_and_unify, filter_dataset])\n",
    "\n",
    "print(\"Executing the pipeline...\")\n",
    "filtered_dataset = curation_steps(multilingual_dataset).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e0c2dd-0e63-4105-aeee-a41b12f0fb06",
   "metadata": {},
   "source": [
    "We can check the outputs. Notice that a new field named `word_count` has been added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72e03f52-5757-41b9-9e09-05c3cb5e67ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Cours D'histoire Des √âtats Europ√©ens: Depuis L...</td>\n",
       "      <td>2019-01-17 23:25:39</td>\n",
       "      <td>https://www.bookvoed.ru/book?id=1433688</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Copy-Paste ejecutado en el Windows Phone 7.[V√≠...</td>\n",
       "      <td>2019-07-20 07:52:44</td>\n",
       "      <td>https://geeksroom.com/2010/12/copy-paste-ejecu...</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Agenda de eventos y actividades en Barcelona p...</td>\n",
       "      <td>2018-07-22 22:13:01</td>\n",
       "      <td>http://barcelona.carpediem.cd/events/?dt=06.04...</td>\n",
       "      <td>746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>file.json</td>\n",
       "      <td>EE.UU | EE.UU\\nenero 22, 2014 Juan Pedro S√°nch...</td>\n",
       "      <td>2019-09-20 03:50:18</td>\n",
       "      <td>https://makeexperience.wordpress.com/tag/ee-uu/</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>file.json</td>\n",
       "      <td>jandrobell - Pesca Mediterraneo 2\\njandrobell\\...</td>\n",
       "      <td>2018-08-18 11:01:22</td>\n",
       "      <td>http://www.pescamediterraneo2.com/foros/profil...</td>\n",
       "      <td>1865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    file_name                                               text  \\\n",
       "1   file.json  Cours D'histoire Des √âtats Europ√©ens: Depuis L...   \n",
       "6   file.json  Copy-Paste ejecutado en el Windows Phone 7.[V√≠...   \n",
       "8   file.json  Agenda de eventos y actividades en Barcelona p...   \n",
       "11  file.json  EE.UU | EE.UU\\nenero 22, 2014 Juan Pedro S√°nch...   \n",
       "13  file.json  jandrobell - Pesca Mediterraneo 2\\njandrobell\\...   \n",
       "\n",
       "             timestamp                                                url  \\\n",
       "1  2019-01-17 23:25:39            https://www.bookvoed.ru/book?id=1433688   \n",
       "6  2019-07-20 07:52:44  https://geeksroom.com/2010/12/copy-paste-ejecu...   \n",
       "8  2018-07-22 22:13:01  http://barcelona.carpediem.cd/events/?dt=06.04...   \n",
       "11 2019-09-20 03:50:18    https://makeexperience.wordpress.com/tag/ee-uu/   \n",
       "13 2018-08-18 11:01:22  http://www.pescamediterraneo2.com/foros/profil...   \n",
       "\n",
       "    word_count  \n",
       "1          111  \n",
       "6          137  \n",
       "8          746  \n",
       "11         323  \n",
       "13        1865  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737b41e2-79a7-4a5e-a823-1aa07d29926b",
   "metadata": {},
   "source": [
    "Let's save the output, we need to create the folder first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7840e33-7414-4819-99bc-60e3df51c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_path = os.path.join(curated_data_path, \"02_filter_dataset\")\n",
    "\n",
    "! mkdir -p {filtered_data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee80a5b3-8542-42a7-b818-3390653702f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to disk complete for 1 partition(s)\n"
     ]
    }
   ],
   "source": [
    "# save output to json\n",
    "filtered_dataset.to_json(filtered_data_path, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf4874-8107-41f0-8358-971011b17fe1",
   "metadata": {},
   "source": [
    "Check the saved file by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c80b85c-27b1-4d48-a8cb-dfe6c6273319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fr_core_news_sm in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (3.7.0)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from fr_core_news_sm) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr_core_news_sm) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr_core_news_sm) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr_core_news_sm) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr_core_news_sm) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr_core_news_sm) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr_core_news_sm) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr_core_news_sm) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr_core_news_sm) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr_core_news_sm) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr_core_news_sm) (0.4.3)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr_core_news_sm) (0.21.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr_core_news_sm) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr_core_news_sm) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr_core_news_sm) (2.12.5)\n",
      "Requirement already satisfied: jinja2 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr_core_news_sm) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr_core_news_sm) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr_core_news_sm) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr_core_news_sm) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr_core_news_sm) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr_core_news_sm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr_core_news_sm) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr_core_news_sm) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr_core_news_sm) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr_core_news_sm) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr_core_news_sm) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr_core_news_sm) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr_core_news_sm) (2026.1.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr_core_news_sm) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr_core_news_sm) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr_core_news_sm) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr_core_news_sm) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr_core_news_sm) (14.2.0)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr_core_news_sm) (0.21.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr_core_news_sm) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr_core_news_sm) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr_core_news_sm) (2.0.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr_core_news_sm) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr_core_news_sm) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr_core_news_sm) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr_core_news_sm) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install fr_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8dd4da-9f59-454c-a4c5-725d8cc2a1db",
   "metadata": {},
   "source": [
    "### 1.3 PII Identification and Removal\n",
    "\n",
    "The Personal Identifiable Information (PII) identification tool is designed to remove sensitive data from datasets.\n",
    "\n",
    "The identification leverages [presidio_analyzer](https://pypi.org/project/presidio-analyzer/) a Python based service for detecting PII entities in text.\n",
    "\n",
    "Let's try to analyze a toy sample: *My name is Dana and my number is 212-555-5555*\n",
    "\n",
    "Expected output is the type `PERSON` and `PHONE_NUMBER` and the char start and end position.\n",
    "\n",
    "```\n",
    " type: PERSON, start: 11, end: 15, score: 0.85,\n",
    " type: PHONE_NUMBER, start: 33, end: 45, score: 0.75\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2aaef24e-c838-408c-9242-f40cfc11146d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[type: PERSON, start: 11, end: 15, score: 0.85, type: PHONE_NUMBER, start: 33, end: 45, score: 0.75]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "\n",
    "# Hide deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Set up the engine, loads the NLP module (spaCy model by default) and other PII recognizers\n",
    "LANGUAGES_CONFIG_FILE = \"./languages-config.yml\"\n",
    "# Create NLP engine based on configuration file\n",
    "provider = NlpEngineProvider(conf_file=LANGUAGES_CONFIG_FILE)\n",
    "nlp_engine_with_spanish = provider.create_engine()\n",
    "\n",
    "analyzer = AnalyzerEngine(\n",
    "    supported_languages=[\"en\", \"es\", \"fr\"], nlp_engine=nlp_engine_with_spanish\n",
    ")\n",
    "\n",
    "results = analyzer.analyze(\n",
    "    text=\"My name is Dana and my number is 212-555-5555\",\n",
    "    entities=[\"PHONE_NUMBER\", \"PERSON\"],\n",
    "    language=\"en\",\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a163574-b7e2-4100-a054-ec53c0a2ff89",
   "metadata": {},
   "source": [
    "Run the analyzer for French sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "300f2672-9671-4fde-8ac7-270b4c81cbaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[type: EMAIL_ADDRESS, start: 14, end: 29, score: 1.0,\n",
       " type: URL, start: 18, end: 29, score: 0.5]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.analyze(text=\"Mon email est mon@example.com\", language=\"fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfef904-5323-411f-9b04-7fc8cbf95f81",
   "metadata": {},
   "source": [
    "Try your own examples in these three languages for accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "000f9d69-1563-48a7-b2ea-4299489c62ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[type: EMAIL_ADDRESS, start: 23, end: 43, score: 1.0,\n",
       " type: URL, start: 34, end: 43, score: 0.5]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = \"my email address is at tomaspries@gmail.com\"\n",
    "analyzer.analyze(text=input, language=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa84a63-5062-44bc-b948-f85f246d7e42",
   "metadata": {},
   "source": [
    "Nemo Curator integrates PII Identification and Removal efficiently leveraging Dask for parallelization. The tool currently supports the identification and removal of the following sensitive data types:\n",
    "\n",
    "`ADDRESS`,`CREDIT_CARD`,`EMAIL_ADDRESS`,`DATE_TIME`,`IP_ADDRESS`,`LOCATION`,`PERSON`,`URL`,`US_SSN`,`US_PASSPORT`,`US_DRIVER_LICENSE`,`PHONE_NUMBER`,\n",
    "\n",
    "Let;s run the Nemo Curator PII Identification `PiiModifier` on a toy sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eff2cc51-7177-4224-a637-5c167c6f7e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ryan went out to play football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>His email is ryan@example.com and phone is 212...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                     Ryan went out to play football\n",
       "1  His email is ryan@example.com and phone is 212..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create toy samples with PII data\n",
    "dataframe_toy = pd.DataFrame(\n",
    "    {\n",
    "        \"text\": [\n",
    "            \"Ryan went out to play football\",\n",
    "            \"His email is ryan@example.com and phone is 212-555-5555\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "dataset_toy = DocumentDataset(dask.dataframe.from_pandas(dataframe_toy, npartitions=1))\n",
    "\n",
    "dataset_toy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de519ec7-ad64-44a1-b0ea-72ea09c5dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 18:36:33,178 - distributed.nanny - WARNING - Restarting worker\n",
      "/home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/distributed/worker_memory.py:508: FutureWarning: Parameter memory_target_fraction has been deprecated and will be removed in a future version; please use dask config key distributed.worker.memory.target instead\n",
      "  warnings.warn(\n",
      "/home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/distributed/worker_memory.py:508: FutureWarning: Parameter memory_spill_fraction has been deprecated and will be removed in a future version; please use dask config key distributed.worker.memory.spill instead\n",
      "  warnings.warn(\n",
      "/home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/distributed/worker_memory.py:508: FutureWarning: Parameter memory_pause_fraction has been deprecated and will be removed in a future version; please use dask config key distributed.worker.memory.pause instead\n",
      "  warnings.warn(\n",
      "2026-01-04 18:36:34,622 - distributed.nanny - WARNING - Restarting worker\n",
      "/home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/distributed/worker_memory.py:508: FutureWarning: Parameter memory_target_fraction has been deprecated and will be removed in a future version; please use dask config key distributed.worker.memory.target instead\n",
      "  warnings.warn(\n",
      "/home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/distributed/worker_memory.py:508: FutureWarning: Parameter memory_spill_fraction has been deprecated and will be removed in a future version; please use dask config key distributed.worker.memory.spill instead\n",
      "  warnings.warn(\n",
      "/home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/distributed/worker_memory.py:508: FutureWarning: Parameter memory_pause_fraction has been deprecated and will be removed in a future version; please use dask config key distributed.worker.memory.pause instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d3f12d-ef3f-48a5-bd1a-1902de2d8c3d",
   "metadata": {},
   "source": [
    "Let's build and apply the `PiiModifier` on the toy sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1b52e11-bfd9-4aa5-b721-cff7e67ff6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.modifiers.pii_modifier import PiiModifier\n",
    "\n",
    "# Use smaller batch size for memory efficiency\n",
    "modifier = PiiModifier(\n",
    "    batch_size=500,                      # Reduced from 2000 for memory efficiency\n",
    "    language=\"en\",\n",
    "    supported_entities=[\"PERSON\", \"EMAIL_ADDRESS\", \"PHONE_NUMBER\"],\n",
    "    anonymize_action=\"replace\",\n",
    "    device=\"cpu\"                         # Force CPU usage\n",
    ")\n",
    "\n",
    "modify = Modify(modifier)\n",
    "modified_dataset = modify(dataset_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "071a86f9-1e93-47e9-a1f8-e88f182bc386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 18:36:40 INFO:Loaded recognizer: EmailRecognizer\n",
      "2026-01-04 18:36:40 INFO:Loaded recognizer: PhoneRecognizer\n",
      "2026-01-04 18:36:40 INFO:Loaded recognizer: SpacyRecognizer\n",
      "2026-01-04 18:36:40 INFO:Loaded recognizer: UsSsnRecognizer\n",
      "2026-01-04 18:36:40 INFO:Loaded recognizer: CreditCardRecognizer\n",
      "2026-01-04 18:36:40 INFO:Loaded recognizer: IpRecognizer\n",
      "2026-01-04 18:36:40 WARNING:model_to_presidio_entity_mapping is missing from configuration, using default\n",
      "2026-01-04 18:36:40 WARNING:low_score_entity_names is missing from configuration, using default\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;PERSON&gt; went out to play football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>His email is &lt;EMAIL_ADDRESS&gt; and phone is &lt;PHO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                 <PERSON> went out to play football\n",
       "1  His email is <EMAIL_ADDRESS> and phone is <PHO..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check modified data\n",
    "modified_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1654a77-4330-4902-99bf-056be4920dcb",
   "metadata": {},
   "source": [
    "Now, let's integrate this PII identification step into our curation sequence and apply it to the multilingual dataset. This will ensure that sensitive data is properly detected and removed while maintaining data quality. \n",
    "\n",
    "Let's create first the `redact_pii` function for PII identification and removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e7d0f90-6efa-4ea9-a9e4-c1849e0a637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.modifiers.pii_modifier import PiiModifier\n",
    "\n",
    "\n",
    "def redact_pii(dataset: DocumentDataset) -> DocumentDataset:\n",
    "    \"\"\"\n",
    "    Redact PII (Personally Identifiable Information) from documents.\n",
    "    Uses optimized settings for memory efficiency.\n",
    "    \"\"\"\n",
    "    redactor = Modify(\n",
    "        PiiModifier(\n",
    "            batch_size=500,                      # Reduced batch size for memory efficiency\n",
    "            supported_entities=[\"PERSON\"],       # Only redact person names\n",
    "            anonymize_action=\"replace\",          # Replace PII with entity tags\n",
    "            device=\"cpu\",                        # Force CPU usage (not GPU)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return redactor(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6622c883-8fb5-4951-84e7-9f0a80a7ce29",
   "metadata": {},
   "source": [
    "Let's now run the sequence of curation steps including the PII removal function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c0c780b-2632-4ff9-9ddd-7a889bf1f58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the pipeline...\n",
      "CPU times: user 190 ms, sys: 16.9 ms, total: 207 ms\n",
      "Wall time: 201 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "curation_steps = Sequential([clean_and_unify, filter_dataset, redact_pii])\n",
    "\n",
    "print(\"Executing the pipeline...\")\n",
    "redact_pii_dataset = curation_steps(multilingual_dataset).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4ce773a-9520-44a9-bad4-9915d8a808d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Cours D'histoire Des √âtats Europ√©ens: &lt;PERSON&gt;...</td>\n",
       "      <td>2019-01-17 23:25:39</td>\n",
       "      <td>https://www.bookvoed.ru/book?id=1433688</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Copy-Paste ejecutado en el Windows Phone 7.[V√≠...</td>\n",
       "      <td>2019-07-20 07:52:44</td>\n",
       "      <td>https://geeksroom.com/2010/12/copy-paste-ejecu...</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Agenda de eventos y actividades en Barcelona p...</td>\n",
       "      <td>2018-07-22 22:13:01</td>\n",
       "      <td>http://barcelona.carpediem.cd/events/?dt=06.04...</td>\n",
       "      <td>746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>file.json</td>\n",
       "      <td>EE.UU | EE.UU\\nenero 22, 2014 &lt;PERSON&gt;, Met, M...</td>\n",
       "      <td>2019-09-20 03:50:18</td>\n",
       "      <td>https://makeexperience.wordpress.com/tag/ee-uu/</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>file.json</td>\n",
       "      <td>jandrobell - Pesca Mediterraneo 2\\njandrobell\\...</td>\n",
       "      <td>2018-08-18 11:01:22</td>\n",
       "      <td>http://www.pescamediterraneo2.com/foros/profil...</td>\n",
       "      <td>1865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    file_name                                               text  \\\n",
       "1   file.json  Cours D'histoire Des √âtats Europ√©ens: <PERSON>...   \n",
       "6   file.json  Copy-Paste ejecutado en el Windows Phone 7.[V√≠...   \n",
       "8   file.json  Agenda de eventos y actividades en Barcelona p...   \n",
       "11  file.json  EE.UU | EE.UU\\nenero 22, 2014 <PERSON>, Met, M...   \n",
       "13  file.json  jandrobell - Pesca Mediterraneo 2\\njandrobell\\...   \n",
       "\n",
       "             timestamp                                                url  \\\n",
       "1  2019-01-17 23:25:39            https://www.bookvoed.ru/book?id=1433688   \n",
       "6  2019-07-20 07:52:44  https://geeksroom.com/2010/12/copy-paste-ejecu...   \n",
       "8  2018-07-22 22:13:01  http://barcelona.carpediem.cd/events/?dt=06.04...   \n",
       "11 2019-09-20 03:50:18    https://makeexperience.wordpress.com/tag/ee-uu/   \n",
       "13 2018-08-18 11:01:22  http://www.pescamediterraneo2.com/foros/profil...   \n",
       "\n",
       "    word_count  \n",
       "1          111  \n",
       "6          137  \n",
       "8          746  \n",
       "11         323  \n",
       "13        1865  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the filtered data\n",
    "redact_pii_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e50d88-8a61-46c0-b42f-94a22a3ad4d3",
   "metadata": {},
   "source": [
    "Let's now save the fileted data. We need to create the folder to save the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b610ca1-5d53-4d00-aa78-60c8e019bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "redact_pii_data_path = os.path.join(curated_data_path, \"03_redact_pii_data_path\")\n",
    "\n",
    "! mkdir -p {redact_pii_data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8259bb6-64ac-46d7-9813-de0e06ec5005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to disk complete for 1 partition(s)\n"
     ]
    }
   ],
   "source": [
    "# save\n",
    "redact_pii_dataset.to_json(redact_pii_data_path, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7dcb0bc4-b310-4bb8-8a3d-a2f3e8806296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: jq: command not found\n"
     ]
    }
   ],
   "source": [
    "# check the saved file\n",
    "! head -n 1 {redact_pii_data_path}/file.jsonl |jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f0d3c-4646-4a24-9e86-de0bc153111d",
   "metadata": {},
   "source": [
    "The current PII removal s Nemo Curator implementation is limited to HPC clusters using Slurm as the resource manager. Check the [documentation](https://github.com/NVIDIA/NeMo-Curator/blob/main/docs/user-guide/personalidentifiableinformationidentificationandremoval.rst) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a1f1a1-55e1-407b-a494-c17bb58c9258",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "In this notebook, you have used Nemo Curator to apply a sequence of basic text curation steps designed to clean and preprocess the dataset.\n",
    "\n",
    "Before moving on to the next notebook, make sure to stop the Dask cluster. Please run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73b44b3b-5db7-4a3f-929a-c3311eb5e431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)  # automatically restarts kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74536337-a5a3-4e54-9f9c-f7049b7f684e",
   "metadata": {},
   "source": [
    "\n",
    "We are now ready to move to the next notebook to explore advanced data preparation steps. \n",
    "\n",
    "Let's move to the [02_advanced_data_processing](02_advanced_data_processing.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b1e5e3-60b1-47d7-87e3-96c8d73e99e0",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
