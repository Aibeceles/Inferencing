{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ee8ff11-6c0b-455b-a574-bf7170a18def",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f1c75-fe21-4823-ace7-2ec4a93de023",
   "metadata": {},
   "source": [
    "# 2. Advanced Data Processing\n",
    "\n",
    "\n",
    "In this notebook, we will use NeMo Curator to perform several crutial data cleaning steps, such as language detection and filtering, topic classification, and deduplication. \n",
    "\n",
    "This notebook is structured as follows:\n",
    "- First, we will explore language detection and filtering to separate our multilingual dataset by language.\n",
    "- Next, we will dive into topic classification to categorize the datasets into relevant themes.\n",
    "- Finally, we will explore document deduplication, covering both exact and fuzzy methods.\n",
    "\n",
    "\n",
    "**[2.1 Language Separation](#2.1-Language-Separation)<br>**\n",
    "**[2.2 Domain Classification](#2.2-Domain-Classification)<br>**\n",
    "**[2.3 Documents Deduplication](#2.3-Deduplication)<br>**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb815d94",
   "metadata": {},
   "source": [
    "## CUDA Environment Setup (One-Time)\n",
    "\n",
    "This cell creates symlinks so CuPy can find all CUDA headers in one location.  \n",
    "**Run this once** per environment setup - it's safe to run multiple times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4275fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up CUDA headers...\n",
      "‚úÖ CUDA headers symlinked: /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/nvidia/cuda_runtime/include -> /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/nvidia/cuda_nvcc/include\n",
      "‚úÖ cuda_fp16.h accessible at: /home/aibeceles/inferencing/.venv/lib/python3.12/site-packages/nvidia/cuda_nvcc/include/cuda_fp16.h\n",
      "‚úÖ CUDA environment ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup CUDA paths for CuPy/cuDF compatibility\n",
    "venv = Path(sys.prefix) / \"lib\" / \"python3.12\" / \"site-packages\" / \"nvidia\"\n",
    "\n",
    "cuda_nvcc = venv / \"cuda_nvcc\"\n",
    "cuda_nvcc_include = venv / \"cuda_nvcc\" / \"include\"\n",
    "cuda_runtime_include = venv / \"cuda_runtime\" / \"include\"\n",
    "\n",
    "# Create include directory if it doesn't exist\n",
    "cuda_nvcc_include.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create symlinks to make all CUDA headers accessible in one location\n",
    "print(\"Setting up CUDA headers...\")\n",
    "result = subprocess.run(\n",
    "    f\"ln -sf {cuda_runtime_include}/* {cuda_nvcc_include}/\",\n",
    "    shell=True, capture_output=True, text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"‚úÖ CUDA headers symlinked: {cuda_runtime_include} -> {cuda_nvcc_include}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Symlink failed, copying headers instead...\")\n",
    "    import shutil\n",
    "    for header in cuda_runtime_include.glob(\"*.h\"):\n",
    "        shutil.copy2(header, cuda_nvcc_include / header.name)\n",
    "    print(f\"‚úÖ Copied {len(list(cuda_runtime_include.glob('*.h')))} header files\")\n",
    "\n",
    "# Verify cuda_fp16.h is accessible\n",
    "if (cuda_nvcc_include / \"cuda_fp16.h\").exists():\n",
    "    print(f\"‚úÖ cuda_fp16.h accessible at: {cuda_nvcc_include}/cuda_fp16.h\")\n",
    "else:\n",
    "    print(f\"‚ùå ERROR: cuda_fp16.h not found!\")\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_PATH\"] = str(cuda_nvcc)\n",
    "os.environ[\"CUDA_HOME\"] = str(cuda_nvcc)\n",
    "print(f\"‚úÖ CUDA environment ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bdf4c0-2133-441e-b12c-f28ad3aa3b37",
   "metadata": {},
   "source": [
    "***************\n",
    "### Environment Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70757a02-0847-4100-9dbd-10dbf90d2873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore any warning\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b62ac08-2531-4b98-9bab-12673a2840e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU cluster note\n",
    "# - This notebook uses a single-GPU Dask cluster (good default for 8GB GPUs)\n",
    "# - We enable cuDF spilling to reduce OOM risk\n",
    "# Run the next cell to start the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7737ea37-2c40-4d9f-9859-825b1a493085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from nemo_curator.utils.distributed_utils import get_client, get_num_workers\n",
    "\n",
    "# Enable CUDF spilling to prevent OOM errors on 8GB GPU\n",
    "os.environ[\"CUDF_SPILL\"] = \"1\"\n",
    "\n",
    "def pre_imports():\n",
    "    \"\"\"Set CUDA environment and import cudf on each worker\"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Set CUDA paths on worker (same as main process)\n",
    "    venv = Path(sys.prefix) / \"lib\" / \"python3.12\" / \"site-packages\" / \"nvidia\"\n",
    "    cuda_nvcc = venv / \"cuda_nvcc\"\n",
    "    \n",
    "    os.environ[\"CUDA_PATH\"] = str(cuda_nvcc)\n",
    "    os.environ[\"CUDA_HOME\"] = str(cuda_nvcc)\n",
    "    \n",
    "    # Import cudf with CUDA environment set\n",
    "    import cudf\n",
    "    print(f\"‚úì Worker ready - CUDA_PATH={os.environ.get('CUDA_PATH')}\")\n",
    "\n",
    "# Initialize GPU cluster with memory-safe settings\n",
    "client = get_client(\n",
    "    cluster_type=\"gpu\", \n",
    "    set_torch_to_use_rmm=False  # Disable RMM for stability\n",
    ")\n",
    "\n",
    "print(f\"‚úì GPU cluster ready!\")\n",
    "print(f\"Number of dask workers: {get_num_workers(client)}\")\n",
    "print(f\"üìä Dashboard: {client.dashboard_link}\")\n",
    "\n",
    "# Set up workers with CUDA environment\n",
    "client.run(pre_imports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904cadc6-7228-45fd-b7d7-bf6c165f35d2",
   "metadata": {},
   "source": [
    "## GPU Cluster Configuration\n",
    "\n",
    "The GPU Dask cluster is started in the previous cell with settings tuned for an 8GB GPU:\n",
    "\n",
    "- **1 GPU worker** to avoid fragmentation\n",
    "- **cuDF spilling enabled**\n",
    "- **RMM disabled** for stability\n",
    "\n",
    "üí° Monitor GPU usage with `watch -n 1 nvidia-smi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067edcdf-34f3-476f-8309-a2726be88c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (intentionally blank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10d38a4-c4bf-4819-a3e4-e85dc96e0884",
   "metadata": {},
   "source": [
    "Let's load the multilingual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b611fce2-537e-4a89-8fb5-c6624b774528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.datasets import DocumentDataset\n",
    "\n",
    "multilingual_data_path = \"./original_data\"\n",
    "multilingual_dataset = DocumentDataset.read_json(\n",
    "    multilingual_data_path, add_filename=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518de0e9-9385-4acb-a497-4e5f16c97813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data\n",
    "multilingual_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37184b1-73f6-48b4-a41a-e88b662e332e",
   "metadata": {},
   "source": [
    "## 2.1 Language Separation\n",
    "\n",
    "In this section, we will use a language classification model by [fasttext](https://fasttext.cc/docs/en/language-identification.html). \n",
    "\n",
    "\n",
    "Let's first create the output folders and download the fasttext model for text language detection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b982ba8-0d1e-424c-9d38-446eef55d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "language_base_output_path = \"./curated/04_language_separation\"\n",
    "language_separated_output_path = os.path.join(language_base_output_path, \"language\")\n",
    "\n",
    "# Create directories (with parents as needed)\n",
    "os.makedirs(language_base_output_path, exist_ok=True)\n",
    "os.makedirs(language_separated_output_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b5f42b-752e-492c-908e-38738a970f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_separated_output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28fdc0b-32b1-4ee4-bfb2-239cbb458dff",
   "metadata": {},
   "source": [
    "Let's create the filter which uses the downloaded fasttext model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5599b5-9e9b-4d5a-b862-932a4f5bd3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download fasttext language classification model\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "model_path = \"lid.176.bin\"\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"Downloading FastText language model (131 MB)...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\"\n",
    "    urllib.request.urlretrieve(url, model_path)\n",
    "    print(f\"‚úì Model downloaded to {model_path}\")\n",
    "else:\n",
    "    print(f\"‚úì FastText model already exists at {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f093c3cc-c109-473d-bc6a-70070f568edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import ScoreFilter\n",
    "from nemo_curator.filters import FastTextLangId\n",
    "\n",
    "lang_filter = FastTextLangId(\"lid.176.bin\")\n",
    "language_field = \"language\"\n",
    "language_id_pipeline = ScoreFilter(\n",
    "    lang_filter, score_field=language_field, score_type=\"object\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1623a093-35aa-47f0-9fb0-a4326ce57cd2",
   "metadata": {},
   "source": [
    "Now, let's apply the language detection filter on our multilingual dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f41b5-b5c5-4077-8898-5d989238cc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply language separation to our multilingual dataset\n",
    "filtered_dataset = language_id_pipeline(multilingual_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541db02c-5f57-4ef1-9414-fc7a8473ee7f",
   "metadata": {},
   "source": [
    "Let's check the detected language for each sample. \n",
    "\n",
    "Notice the new fields `language` in the output with the language code `FR/EN/ES`and the classification score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b39506-eb40-44aa-a337-a135042c401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the detected language per item\n",
    "filtered_dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07606f48-a5d3-45f1-a6e4-19e9792b2977",
   "metadata": {},
   "source": [
    "Let's separate documents by the language label and save each language separately. This will create sub-folders for each languages under the output path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a96f78-6563-47d9-84c9-1c53d50ad70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save separated languages and get stats\n",
    "from nemo_curator.utils.file_utils import separate_by_metadata\n",
    "\n",
    "filtered_dataset.df[language_field] = filtered_dataset.df[language_field].apply(\n",
    "    lambda score: score[1], meta=(language_field, \"object\")\n",
    ")\n",
    "language_stats = separate_by_metadata(\n",
    "    filtered_dataset.df, language_separated_output_path, metadata_field=language_field\n",
    ").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7abd47e-990d-4e8c-a828-d0dee12920cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the language distribution stats\n",
    "print(f\"Number of document:{len(multilingual_dataset)}\")\n",
    "print(f\"Number of filtered document:{len(filtered_dataset)}\")\n",
    "\n",
    "print(\"Language separation stats and  \", language_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220d8423-d974-4b25-8778-8231b4ccf8e8",
   "metadata": {},
   "source": [
    "We can check the output jsonl file per language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e521686-a780-4218-a32c-272fc1c3b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check first element for French\n",
    "! head -n 1 {language_separated_output_path}/FR/file.jsonl | jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa068ada-9e72-4d4d-91ee-8fb7cc44d8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check first element for spanish\n",
    "! head -n 1 {language_separated_output_path}/ES/file.jsonl |jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6ca859-98b8-4de9-aa6e-0fa1808feb06",
   "metadata": {},
   "source": [
    "## 2.2 Domain Classification\n",
    "\n",
    "Nemo Curator supports various text classification models allowing data annotation, useful for cleaning and data blending. Check the documentation for [distributed data classification](https://github.com/NVIDIA/NeMo-Curator/blob/main/tutorials/distributed_data_classification/README.md).\n",
    "\n",
    "\n",
    "Each classifier is available on Hugging Face Hub. When run with NeMo Curator, they are accelerated using RAPIDS [CrossFit](https://github.com/rapidsai/crossfit) library.\n",
    "\n",
    "\n",
    "In this section, we will experiment with the `MultilingualDomainClassifier` a Multilingual Domain Classifier that support 52 languages and annotate 26 domain classes:\n",
    "\n",
    "`Arts_and_Entertainment`, `Autos_and_Vehicles`, `Adult`,`Beauty_and_Fitness`, `Books_and_Literature`, `Business_and_Industrial`, `Computers_and_Electronics`, `Finance`, `Food_and_Drink`, `Games`, `Health`, `Hobbies_and_Leisure`, `Home_and_Garden`, `Internet_and_Telecom`, `Jobs_and_Education`, `Law_and_Government`, `News`, `Online_Communities`, `People_and_Society`, `Pets_and_Animals`, `Real_Estate`, `Science`, `Sensitive_Subjects`, `Shopping`, `Sports`, `Travel_and_Transportation`\n",
    "\n",
    "The model architecture is a transformer-based encoder Deberta V3 Base available on Hugging Face Hub. Learn more about the classifier [MultilingualDomainClassifier Model's Card](https://huggingface.co/nvidia/multilingual-domain-classifier).\n",
    "\n",
    "\n",
    "Let's set the output folder for domain classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18d423c-9f35-4357-927d-6fab48b62547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import dask_cudf\n",
    "from nemo_curator.classifiers import MultilingualDomainClassifier\n",
    "\n",
    "domain_output_path = \"./curated/05_domain_classification\"\n",
    "\n",
    "# Create directory (with parents if needed)\n",
    "os.makedirs(domain_output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e170397c-7fec-4c40-99ce-2ea56f0d9057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (not needed anymore)\n",
    "# CUDA headers are handled by the \"CUDA Environment Setup\" cell at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b85f8-103c-4617-866d-58af68427898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (not needed anymore)\n",
    "# Keeping this cell empty avoids confusing CuPy debug settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fd61cd-8451-43e2-a4a3-612bc040d70e",
   "metadata": {},
   "source": [
    "First, let's apply the Multilingual Domain Classifier on a toy multilingual dataset. Let's create the dataset with multiple languages and topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4639764f-6854-4614-a7a4-95451875622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrame\n",
    "text = [\n",
    "    # French\n",
    "    \"Il adore les chats.\",\n",
    "    # English\n",
    "    \"Investing in index funds is a popular strategy for long-term financial growth.\",\n",
    "    # Spanish\n",
    "    \"Ir de compras en el centro comercial es una excelente manera de encontrar ofertas y descubrir nuevas tiendas.\",\n",
    "    # Polish\n",
    "    \"Dziƒôki wykorzystaniu analizy danych programy treningowe dla sportowc√≥w sta≈Çy siƒô bardziej wyrafinowane.\",\n",
    "    # Arabic\n",
    "    \".ÿ™ŸÇÿØŸÖ ÿßŸÑÿ™ÿ∑Ÿàÿ±ÿßÿ™ ÿßŸÑÿ≠ÿØŸäÿ´ÿ© ŸÅŸä ÿßŸÑÿπŸÑÿßÿ¨ ÿßŸÑÿ¨ŸäŸÜŸä ÿ£ŸÖŸÑÿßŸã ÿ¨ÿØŸäÿØŸãÿß ŸÑÿπŸÑÿßÿ¨ ÿßŸÑÿßÿ∂ÿ∑ÿ±ÿßÿ®ÿßÿ™ ÿßŸÑŸàÿ±ÿßÿ´Ÿäÿ©\",\n",
    "]\n",
    "df = cudf.DataFrame({\"text\": text})\n",
    "\n",
    "toy_dataset = DocumentDataset(dask_cudf.from_cudf(df, npartitions=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d07561f-9b22-4af9-813b-040d548ef251",
   "metadata": {},
   "source": [
    "We can define the `MultilingualDomainClassifier` filter as follows. \n",
    "\n",
    "On its first run, it will download the DeBERTa model from the Hugging Face Hub (~500MB).\n",
    "\n",
    "**Memory Settings for 8GB GPU:**\n",
    "- `batch_size=64` (reduced from 1024)\n",
    "- `max_mem_gb=6` (limits memory during model fitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae8db38-7b1f-49ce-aea6-92dcdd1d51d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the classifier with memory-safe settings for an 8GB GPU\n",
    "#\n",
    "# IMPORTANT: CrossFit fits a GPU \"memory estimate curve\" the first time it sees a model.\n",
    "# On 8GB cards this can OOM during cleanup due to a PyTorch issue.\n",
    "# We avoid that by pre-seeding CrossFit's cache (`mem_model.pkl`) with a small regression model.\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from crossfit.dataset.home import CF_HOME\n",
    "from nemo_curator.classifiers import MultilingualDomainClassifier\n",
    "\n",
    "# Reduce allocator fragmentation risk\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True,max_split_size_mb:128\")\n",
    "\n",
    "# Pre-seed CrossFit memory model cache so it doesn't run GPU calibration\n",
    "base_model = \"microsoft/mdeberta-v3-base\"\n",
    "model_cfg = AutoConfig.from_pretrained(base_model)\n",
    "cache_dir = Path(CF_HOME) / \"memory\" / model_cfg._name_or_path\n",
    "mem_model_path = cache_dir / \"mem_model.pkl\"\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Delete old cache to force regeneration with conservative values\n",
    "if mem_model_path.exists():\n",
    "    mem_model_path.unlink()\n",
    "    print(\"‚úì Removed old CrossFit memory cache\")\n",
    "\n",
    "# Features: [batch_size, seq_len, seq_len^2] -> predicted memory (MB)\n",
    "# Use conservative values for 8GB GPU\n",
    "X = np.array(\n",
    "    [\n",
    "        [1, 1, 1],\n",
    "        [8, 128, 128**2],\n",
    "        [16, 256, 256**2],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "y = np.array([256, 1024, 2048], dtype=np.float32)  # MB\n",
    "mem_model = LinearRegression().fit(X, y)\n",
    "joblib.dump(mem_model, mem_model_path)\n",
    "print(f\"‚úì Wrote CrossFit memory cache: {mem_model_path}\")\n",
    "\n",
    "# Clear GPU memory before starting\n",
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"‚úì GPU memory cleared\")\n",
    "\n",
    "# Now create the classifier with VERY conservative settings for 8GB GPU\n",
    "domain_classifier = MultilingualDomainClassifier(\n",
    "    batch_size=8,       # Very small batch size to avoid OOM\n",
    "    max_chars=512,      # Shorter sequences use less memory\n",
    "    autocast=True,      # Use mixed precision\n",
    "    max_mem_gb=4,       # Conservative memory limit\n",
    ")\n",
    "print(\"‚úì Domain classifier created with batch_size=8, max_chars=512\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d9b726-51e1-4da0-89ad-075b47353aca",
   "metadata": {},
   "source": [
    "Now, let's run the filter on our multilingual multi topics toy samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac187d-02c7-4f04-8c1e-6edb5b7a85d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result_domain = domain_classifier(dataset=toy_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a65249-94f4-4783-826a-50438521cecb",
   "metadata": {},
   "source": [
    "Check the outputs. Notice the new field `domain_pred`. Example of expected outputs: \n",
    "```\n",
    "Il adore les chats.\t                                Pets_and_Animals\n",
    "Investing in index funds is a popular strategy...\tFinance\n",
    "Ir de compras en el centro comercial es una ex...\tShopping\n",
    "Dziƒôki wykorzystaniu analizy danych programy t...\tSports\n",
    ".ÿ™ŸÇÿØŸÖ ÿßŸÑÿ™ÿ∑Ÿàÿ±ÿßÿ™ ÿßŸÑÿ≠ÿØŸäÿ´ÿ© ŸÅŸä ÿßŸÑÿπŸÑÿßÿ¨ ÿßŸÑÿ¨ŸäŸÜŸä ÿ£ŸÖŸÑÿßŸã ...\t        Health\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42cf0fe-ca35-458a-81fa-53d548bb3a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results\n",
    "result_domain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0540530-6bd5-457e-a289-e31ee4969248",
   "metadata": {},
   "source": [
    "Now, let's use the `MultilingualDomainClassifier` to process our previously filtered multilingual corpus (French and Spanish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cd91c6-6e1b-4fcf-903c-2ba1ce575c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the filtered data\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "\n",
    "multilingual_data_path = \"./curated/01_clean_and_unify\"\n",
    "multilingual_dataset = DocumentDataset.read_json(multilingual_data_path, backend=\"cudf\")\n",
    "\n",
    "# Domain classification\n",
    "multilingual_result_domain = domain_classifier(dataset=multilingual_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2918da7-d4c9-4147-b35b-caf71cb132cf",
   "metadata": {},
   "source": [
    "Let's check the output. Expected to see an aditional field `domain_pred`:\n",
    "```\n",
    "text                                            \t\tdomain_pred\n",
    "Dragon Ball: Le 20e film de la sage sortira le...\t\tArts_and_Entertainment\n",
    "Cours D'histoire Des √âtats Europ√©ens: Depuis L...\t\tBooks_and_Literature\n",
    "Se realiz√≥ una jornada de promoci√≥n del buentr...\t\tPeople_and_Society\n",
    "...\n",
    "```\n",
    "\n",
    "Execute the following cell to review the topic predictions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d4bc41-0aa7-477c-9b7b-a61f8629c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the domain classification\n",
    "multilingual_result_domain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b22c32d-4775-4ce5-bfc9-d425a131e40a",
   "metadata": {},
   "source": [
    "Let's now save the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10addeac-df75-4262-8ef0-1d7a787a2aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "result_domain.to_json(domain_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54154522-e708-4011-8eba-df81108c284d",
   "metadata": {},
   "source": [
    "We can check the saved outputs by executing the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a68e47c-e938-4ad7-b7cc-01972ea22ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 1 {domain_output_path}/0.part | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f8810f-f7e8-4510-95ef-b29f97e904b0",
   "metadata": {},
   "source": [
    "## 2.3 Deduplication\n",
    "\n",
    "Document-level deduplication aims to reduce the occurrence of duplicate and near-duplicate documents in a dataset. This is crucial for datasets cleaning, reducing redundancy, and ensuring that models are trained on diverse and unique data.\n",
    "\n",
    "In this section, we will explore both the Exact and Fuzzy deduplication. Both functionalities are supported in NeMo Curator and accelerated using the [RAPIDS](https://rapids.ai/) library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d188eeb3-4fbe-4eb2-8438-783e1a6ed1fe",
   "metadata": {},
   "source": [
    "\n",
    "Remember, we created our multilingual (Spanish and French) dataset by deduplicating each sample once.\n",
    "Before running deduplication, we need to ensure that each document in the dataset has a unique ID. We can use the `add_id` module within NeMo Curator to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d59e23e-9db7-4f54-a764-d8f2801ca596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output folders\n",
    "from nemo_curator import AddId\n",
    "\n",
    "data_dir = \"curated/06_add_id\"\n",
    "added_id_output_path = os.path.join(data_dir, \"add_id/cleaned\")\n",
    "!mkdir -p {data_dir}\n",
    "\n",
    "dataset_fr = DocumentDataset.read_json(\n",
    "    os.path.join(language_separated_output_path, \"FR/\"), add_filename=True\n",
    ")\n",
    "dataset_es = DocumentDataset.read_json(\n",
    "    os.path.join(language_separated_output_path, \"ES/\"), add_filename=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e6077d-4373-43a3-b68a-cc4302a99c73",
   "metadata": {},
   "source": [
    "### 2.3.1 Add Unique ID\n",
    "\n",
    "Let's start by adding a unique ID for out dataset separated per language (Spanish and French)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7d53f7-ca0b-4d87-a0d3-bdb060106fef",
   "metadata": {},
   "source": [
    "Let's run the `AddId` on the French corpus by running the next cell. The Format of output ID will be `<prefix>_<id>` where `prefix` is provided and `id` is a generated unique number. \n",
    "\n",
    "Let's apply the `AddId` function to the French corpus by running the next cell. The output ID format will be `<prefix>_<id>`, where `prefix` is specified by the user, and `id` is a uniquely generated number.\n",
    "\n",
    "\n",
    "Example of expected output:\n",
    "```\n",
    "text\t                                         \t\tid\n",
    "Dragon Ball: Le 20e film de la sage sortira le...\t\tFR_data-0000000000\n",
    "Cours D'histoire Des √âtats Europ√©ens: Depuis L...\t\tFR_data-0000000001\n",
    "...\n",
    "```\n",
    "\n",
    "Execute the following cell to apply `AddId` to the French corpus, user prefix here is set to `FR_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e488b70b-a4be-4fad-82b2-739e4e31b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define user's prefix\n",
    "FR_add_ID_id_prefix = \"FR_data\"\n",
    "\n",
    "add_id = AddId(id_field=\"id\", id_prefix=FR_add_ID_id_prefix, start_index=0)\n",
    "id_dataset_fr = add_id(dataset_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f17f49-950d-417a-9dc0-86c097431a2c",
   "metadata": {},
   "source": [
    "Let's check the outputs. Notice the new field `id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74fbfc4-8b21-4ca9-9677-40df6e488645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check outputs\n",
    "id_dataset_fr.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a455f9-30da-4aed-89a6-cbc4249aa455",
   "metadata": {},
   "source": [
    "We can save the outputs in their designated folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e9dff1-a8b7-4e38-bf4e-98804fd42c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_dataset_fr.to_json(os.path.join(added_id_output_path, \"FR/\"), write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d9f6d-ae21-4db4-b43d-5bab02408d61",
   "metadata": {},
   "source": [
    "#### Exercice:  Add Unique ID for Spanish data.\n",
    "Make sure to replace the `# Your code here`. If you get stuck, refer to the solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c0051b-3ccb-4c42-9bc6-d98c7a59215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_add_ID_id_prefix = # Your code here\n",
    "\n",
    "add_id = AddId(id_field=\"id\", id_prefix=ES_add_ID_id_prefix, start_index=0)\n",
    "id_dataset_es = # Your code here\n",
    "\n",
    "# save to relevant folder\n",
    "id_dataset_es.to_json(os.path.join(added_id_output_path, \"ES/\"), write_to_filename=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c830c81b-6a15-4bcd-9036-3030cc044838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "ES_add_ID_id_prefix = \"ES_data\"\n",
    "\n",
    "add_id = AddId(id_field=\"id\", id_prefix=ES_add_ID_id_prefix, start_index=0)\n",
    "id_dataset_es = add_id(dataset_es)\n",
    "\n",
    "# save to relevant folder\n",
    "id_dataset_es.to_json(os.path.join(added_id_output_path, \"ES/\"), write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc0cdc8-5408-4cef-a462-3e92a8161f4e",
   "metadata": {},
   "source": [
    "### 2.3.2 Exact Deduplication\n",
    "\n",
    "Exact Deduplication consists in identifying and removing duplicate documents that are exactly identical within a dataset. This process helps eliminate redundant data, prevents models from overfitting on repeated examples, and ensures that training and test sets do not contain the same samples, which could otherwise lead to misleading evaluation metrics.\n",
    "\n",
    "In [NeMo Curator](https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/gpudeduplication.html), exact deduplication works by hashing each document and keeping only one document per hash, and it can be run on both GPU ([CuDF](https://docs.rapids.ai/api/cudf)) and CPU ([Pandas](https://pandas.pydata.org/)) based backends.\n",
    "\n",
    "\n",
    "Let's create the folders for the exact deduplication. We will save the output results in `/data`, temporary files in `/cache`, and logs in `/log`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a066f0a2-cd4b-4440-b49f-9444e246ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_es = \"curated/07_Deduplicate/exact/ES\"\n",
    "\n",
    "exact_dedup_log_dir_es = os.path.join(data_dir_es, \"log\")\n",
    "exact_dedup_cache_dir_es = os.path.join(data_dir_es, \"cache\")\n",
    "exact_dedup_output_dir_es = os.path.join(data_dir_es, \"data\")\n",
    "\n",
    "# Create all required directories\n",
    "os.makedirs(exact_dedup_log_dir_es, exist_ok=True)\n",
    "os.makedirs(exact_dedup_cache_dir_es, exist_ok=True)\n",
    "os.makedirs(exact_dedup_output_dir_es, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e6fff9-aa31-4bee-a770-8848f6868d82",
   "metadata": {},
   "source": [
    "Before running exact deduplication in NeMo Curator, the dataset needs to present a unique ID for each document (sample). We already added these unique IDs in the previous step in the field `\"id\"`.\n",
    "\n",
    "We will be running the exact deduplication on the GPU using cudf backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87637c3b-5700-44b4-994f-6854f28db506",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_field = \"id\"\n",
    "input_dataset_es = DocumentDataset.read_json(\n",
    "    os.path.join(added_id_output_path, \"ES/\"), backend=\"cudf\", add_filename=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035c15f-e01f-464b-abff-a11f418fb6a0",
   "metadata": {},
   "source": [
    "Execute the next cell to run the exact deduplication on the Spanish dataset. This should take about 10 seconds to process.\n",
    "\n",
    "We can use `perform_removal=True` to apply the duplicate removal directly on the dataset. But, for the sake of this exercise, we will first show the deduplication identifification before actually applying the removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13fc88f-cdac-414d-887e-19fc815bcdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from nemo_curator.modules import ExactDuplicates\n",
    "\n",
    "# run exact deducplicate\n",
    "exact_dup_es = ExactDuplicates(\n",
    "    logger=exact_dedup_log_dir_es,\n",
    "    id_field=\"id\",\n",
    "    text_field=\"text\",\n",
    "    hash_method=\"md5\",\n",
    "    cache_dir=exact_dedup_cache_dir_es,\n",
    ")\n",
    "duplicates_es = exact_dup_es(dataset=input_dataset_es)\n",
    "exact_docs_to_remove_es = duplicates_es.df.map_partitions(\n",
    "    lambda x: x[x._hashes.duplicated(keep=\"first\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b237f77-8155-48e4-99bd-1285c7584711",
   "metadata": {},
   "source": [
    "Check how many detected documents have duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ed545-0511-45bc-8a9a-0576ae413222",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of documents in the original data:{len(input_dataset_es)}\")\n",
    "print(f\"Number of documents to be removed:{len(exact_docs_to_remove_es)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5291954-f400-416a-8024-21c55309f7ee",
   "metadata": {},
   "source": [
    "Check some duplicate documents: \n",
    "\n",
    "Example of output: \n",
    "```\n",
    "     id                  _hashes\n",
    "18   ES_data-0000000146 2f610eed57653fbe68328fbaf3274c2a\n",
    "20   ES_data-0000000148  e473009ec2e1a246de93fea08488ca4c\n",
    "21   ES_data-0000000149  066347c8a96bc73056a9f172e4d9710\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f26eec-c730-438a-8e0a-986482095294",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_docs_to_remove_es.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e6e26f-4dd5-47b7-92a7-d7ff66f5cbcc",
   "metadata": {},
   "source": [
    "Now, apply the deduplication removal and save the results to the output data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955713b8-c1b7-41e5-9a77-e6c357624164",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_es = input_dataset_es.df[\n",
    "    ~input_dataset_es.df[id_field].isin(exact_docs_to_remove_es[id_field].compute())\n",
    "]\n",
    "DocumentDataset(result_es).to_json(exact_dedup_output_dir_es, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b9a68c-f952-4c98-9b02-d521bb60ea93",
   "metadata": {},
   "source": [
    "Check saved output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb8df5-aee1-4f2b-b1d3-90c40e1a9e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 1 {exact_dedup_output_dir_es}/file.jsonl |jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c786ff-f2ea-459b-afe1-9b57044722b6",
   "metadata": {},
   "source": [
    "#### Exercice: Run Exact Desuplication for the French data.\n",
    "\n",
    "Run the same exact deduplication for the French data. \n",
    "\n",
    "Let's first create the relevant folders and set the dataset and id field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eefbd1d-e72f-492f-953b-3d384184d399",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_fr = \"curated/07_Deduplicate/exact/FR\"\n",
    "\n",
    "exact_dedup_log_dir_fr = os.path.join(data_dir_fr, \"log\")\n",
    "exact_dedup_cache_dir_fr = os.path.join(data_dir_fr, \"cache\")\n",
    "exact_dedup_output_dir_fr = os.path.join(data_dir_fr, \"data\")\n",
    "!mkdir -p {exact_dedup_log_dir_fr}\n",
    "!mkdir -p {exact_dedup_cache_dir_fr}\n",
    "!mkdir -p {exact_dedup_output_dir_fr}\n",
    "\n",
    "id_field = \"id\"\n",
    "input_dataset_fr = DocumentDataset.read_json(\n",
    "    os.path.join(added_id_output_path, \"FR/\"), backend=\"cudf\", add_filename=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80423c61-dcea-4667-9a28-65bbf8a8d113",
   "metadata": {},
   "source": [
    "Run the deduplication. Make sure to replace the `# Your code here`. If you get stuck, refer to the solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a011c0af-0d20-42c5-a7c8-a6a2c503b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run exact deduplicate\n",
    "exact_dup_fr = # Your code here\n",
    "duplicates_fr = # Your code here\n",
    "exact_docs_to_remove_fr = # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d0b1f-efae-4188-9004-36d173c63128",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "# run exact deducplicate\n",
    "exact_dup_fr = ExactDuplicates(\n",
    "    logger=exact_dedup_log_dir_fr,\n",
    "    id_field=\"id\",\n",
    "    text_field=\"text\",\n",
    "    hash_method=\"md5\",\n",
    "    cache_dir=exact_dedup_cache_dir_fr,\n",
    ")\n",
    "\n",
    "duplicates_fr = exact_dup_fr(dataset=input_dataset_fr)\n",
    "exact_docs_to_remove_fr = duplicates_fr.df.map_partitions(\n",
    "    lambda x: x[x._hashes.duplicated(keep=\"first\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd34d96c-23fb-47b5-8988-4f25b50bafc6",
   "metadata": {},
   "source": [
    "Check how many detected documents have duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1cf7c8-4808-4f5d-9d1b-5e1a9f3191d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of documents in the original data:{len(input_dataset_fr)}\")\n",
    "print(f\"Number of documents to be removed:{len(exact_docs_to_remove_fr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2c9bb6-d4c8-4251-8559-2db39cf5d2ac",
   "metadata": {},
   "source": [
    "Now, apply the deduplication removal and save the results to the output data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87db1055-26e3-47a4-a24d-f401af247652",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_fr = input_dataset_fr.df[\n",
    "    ~input_dataset_fr.df[id_field].isin(exact_docs_to_remove_fr[id_field].compute())\n",
    "]\n",
    "DocumentDataset(result_fr).to_json(exact_dedup_output_dir_fr, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78744bc-547d-4d81-bd7d-573a8e31bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cluster.close()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d198c5-0c7a-4b44-889d-7c2de5d80ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)  # automatically restarts kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa300f62-3410-4ff8-9bc0-286b46ea2b70",
   "metadata": {},
   "source": [
    "### 2.3.3 Fuzzy Deduplication\n",
    "\n",
    "Removing near-duplicates is referred to as fuzzy deduplication at the document level, which is based on Jaccard similarity scores.\n",
    "\n",
    "This approach can be broken down into the following stages:\n",
    "- **Stage 1 - Minhash + LSH:** The first step involves generating MinHash signatures for the documents. NeMo Curator currently supports character-based n-grams for MinHashing. Then, the Locality Sensitive Hashing (LSH) is performed to identify candidate duplicates.\n",
    "- **Stage 2 - LSH Buckets to Graph edgelist:** LSH buckets are directly converted to edges for the connected components computation.\n",
    "- **Stage 3 - Connect Components:** Since LSH is an approximate method, documents that are near duplicates may end up in different buckets, with some overlapping documents between them. A GPU-accelerated connected components algorithm is used to identify all connected components in the graph formed by the edges between documents within the same bucket. The output of this step is a list of document IDs and the groups they belong to.\n",
    "\n",
    "All documents within the same group are considered near duplicates, and results can then be used to remove them from the corpus.\n",
    "For more information, refer to the Deduplication documentation of [NeMo Curator](https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/gpudeduplication.html).\n",
    "\n",
    "\n",
    "There are no near-duplicates in out example datasets. However, to demonstrate the process, let's run fuzzy deduplication on the French dataset and go through the steps involved.\n",
    "\n",
    "Let's create fisrt the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdd3cc93-65fc-4467-b4a4-f5cd26682c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "fuzzy_dedup_log_dir_fr = \"curated/07_Deduplicate/fuzzy_wrapper/FR\"\n",
    "os.makedirs(fuzzy_dedup_log_dir_fr, exist_ok=True)\n",
    "\n",
    "data_dir = \"curated/06_add_id\"\n",
    "added_id_output_path = os.path.join(data_dir, \"add_id/cleaned\")\n",
    "os.makedirs(added_id_output_path, exist_ok=True)  # Creates \"curated/06_add_id/add_id/cleaned\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b998c6-62c1-4b24-9f60-9462410ee43e",
   "metadata": {},
   "source": [
    "## Restart GPU Cluster for Fuzzy Deduplication\n",
    "\n",
    "Let's start a fresh Dask GPU cluster optimized for fuzzy deduplication. Make sure the previous cluster is stopped.\n",
    "\n",
    "**Configuration:**\n",
    "- **1 GPU worker** (optimal for 8GB GPU)\n",
    "- **Device memory limit: 6GB** (leaves headroom for system operations)\n",
    "- **CUDF spilling enabled** for handling large graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee8c1970-36cf-4e11-a80a-8a0ebe331696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fuzzy dedup GPU cluster ready!\n",
      "Workers: 1\n",
      "üìä Dashboard: http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dask.distributed import Client\n",
    "from nemo_curator.utils.import_utils import gpu_only_import, gpu_only_import_from\n",
    "\n",
    "cudf = gpu_only_import(\"cudf\")\n",
    "dask_cudf = gpu_only_import(\"dask_cudf\")\n",
    "LocalCUDACluster = gpu_only_import_from(\"dask_cuda\", \"LocalCUDACluster\")\n",
    "\n",
    "# Create GPU cluster optimized for 8GB GPU memory\n",
    "cluster = LocalCUDACluster(\n",
    "    n_workers=1,                    # Single worker for 8GB GPU\n",
    "    device_memory_limit=\"6GB\",      # Leave 2GB headroom\n",
    "    rmm_pool_size=\"6GB\",            # RMM memory pool\n",
    "    protocol=\"tcp\",                 # Use TCP for stability\n",
    ")\n",
    "client = Client(cluster)\n",
    "\n",
    "print(f\"‚úì Fuzzy dedup GPU cluster ready!\")\n",
    "print(f\"Workers: {len(cluster.workers)}\")\n",
    "print(f\"üìä Dashboard: {client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a25539-2c7a-4338-a0ac-60912b739d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì CUDF spilling enabled - large operations will spill to disk if needed\n"
     ]
    }
   ],
   "source": [
    "# Enable CUDF spilling for memory-intensive operations\n",
    "os.environ[\"CUDF_SPILL\"] = \"1\"\n",
    "print(\"‚úì CUDF spilling enabled - large operations will spill to disk if needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4796c6-1002-4692-bcc3-6e58f5b30aa9",
   "metadata": {},
   "source": [
    "We will use the `FuzzyDuplicates` method from NeMo Curator to run the fuzzy deduplication process on the French dataset. This will allow us to identify and handle any near-duplicates based on similarity scores.\n",
    "\n",
    "You should see the three stages logged during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "274cd642-b865-405e-a51f-ffa0c6bb505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_dedup_log_dir_fr = \"curated/07_Deduplicate/fuzzy_wrapper/FR\"\n",
    "\n",
    "data_dir = \"curated/06_add_id\"\n",
    "added_id_output_path = os.path.join(data_dir, \"add_id/cleaned\")\n",
    "input_fr = os.path.join(added_id_output_path, \"FR/file.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18dc1758-894d-44d6-a3fe-a6ae057820c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing fuzzy deduplication...\n",
      "Loading French dataset...\n",
      "Reading 1 files with blocksize='1gb' / files_per_partition=None\n",
      "Running fuzzy deduplication (this may take a few minutes)...\n",
      "Stage 1: Starting Minhash + LSH computation\n",
      "Stage 1: Minhash + LSH complete!\n",
      "Stage 2: Starting LSH Buckets to Graph Edgelist\n",
      "Stage 2: Starting LSH Buckets to Graph Edgelist Complete!\n",
      "Stage 3: Connected Components across buckets\n",
      "Stage 3: Connected Components across buckets complete!\n",
      "Reading 1 files with blocksize=None / files_per_partition=1\n",
      "‚úì Fuzzy deduplication complete!\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator import FuzzyDuplicates, FuzzyDuplicatesConfig\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "\n",
    "# Memory-optimized configuration for 8GB GPU\n",
    "config = FuzzyDuplicatesConfig(\n",
    "    cache_dir=fuzzy_dedup_log_dir_fr,  # must be cleared between runs\n",
    "    id_field=\"id\",\n",
    "    text_field=\"text\",\n",
    "    seed=42,\n",
    "    char_ngrams=24,\n",
    "    num_buckets=20,\n",
    "    hashes_per_bucket=13,\n",
    "    use_64_bit_hash=False,\n",
    "    buckets_per_shuffle=1,              # Reduced from 2 for memory efficiency\n",
    "    false_positive_check=False,\n",
    ")\n",
    "\n",
    "print(\"Initializing fuzzy deduplication...\")\n",
    "# Initialize the deduplication object\n",
    "FuzzyDups = FuzzyDuplicates(config=config, logger=\"./\")\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading French dataset...\")\n",
    "dataset_fr = DocumentDataset.read_json(\n",
    "    input_files=input_fr,\n",
    "    backend=\"cudf\",  # FuzzyDuplicates only supports datasets with the cuDF backend.\n",
    ")\n",
    "\n",
    "# Run Fuzzy Duplicate detection\n",
    "print(\"Running fuzzy deduplication (this may take a few minutes)...\")\n",
    "duplicate_docs = FuzzyDups(dataset_fr)\n",
    "print(\"‚úì Fuzzy deduplication complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56671fc-d54d-4bb7-80aa-c03ed578c2da",
   "metadata": {},
   "source": [
    "The result from the connected components stage is a list of document IDs and the group they belong to. All documents in the same group are considered near duplicates. \n",
    "\n",
    "```\n",
    "id\t                group\n",
    "FR_data-0000000062\t46\n",
    "FR_data-0000000013\t47\n",
    "FR_data-0000000104\t160\n",
    "FR_data-0000000185\t161\n",
    "FR_data-0000000155\t65\n",
    "...\n",
    "```\n",
    "Let's check the outputs. Notice the `group` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bc464a1-54a9-48ab-b2af-b523f34b5a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>FR_data-0000000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71</td>\n",
       "      <td>FR_data-0000000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>177</td>\n",
       "      <td>FR_data-0000000036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   group                  id\n",
       "0     68  FR_data-0000000074\n",
       "1     71  FR_data-0000000028\n",
       "2    177  FR_data-0000000036"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_docs.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baad0c2-827f-4664-bc89-70a5f3432340",
   "metadata": {},
   "source": [
    "These groups can be then used to remove the near duplicates from the corpus.\n",
    "\n",
    "Let's run that by executing the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2160e29-08b2-4665-9827-b8f5e42275dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_to_remove = duplicate_docs.df.map_partitions(\n",
    "    lambda x: x[x.group.duplicated(keep=\"first\")]\n",
    ")\n",
    "result = dataset_fr.df[~dataset_fr.df[\"id\"].isin(docs_to_remove[\"id\"].compute())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bbfc1b-a174-4fcd-9475-768c260f7739",
   "metadata": {},
   "source": [
    "Check how many detected documents have duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "835bb183-790f-48d5-aeb7-e6e3fd66236d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the original data : 194\n",
      "Number of documents to be removed : 97\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents in the original data : {len(dataset_fr)}\")\n",
    "print(f\"Number of documents to be removed : {len(result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff85b50-bcb5-4419-9cd9-9d98834bb997",
   "metadata": {},
   "source": [
    "#### [Optional] Explore further Deduplication on downstream tasks\n",
    "\n",
    "Large Language Models are typically evaluated based on their performance on downstream tasks using unseen test data. However, when working with extensive datasets, there is a risk of test data leaking into the model's training set. \n",
    "\n",
    "To mitigate this, NeMo Curator provides a Decontamination strategy, in order to ensure that any document sections appearing in downstream tasks are removed from the training set. \n",
    "\n",
    "You can explore this in more detail in the [task decontamination](https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/taskdecontamination.html) of NeMo Curator documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076bdf0-1e24-4301-8397-2bd44c26c187",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "\n",
    "In this notebook, you have used NeMo Curator to apply several data cleaning steps, including language detection and filtering, topic classification and document deduplication. These steps help ensure that the dataset is clean, diverse, and free from redundant data, improving the quality of the data used for training and evaluation.\n",
    "\n",
    "Before moving on to the next notebook, make sure to stop the Dask cluster. Please run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec8d11a1-c3dc-4fd3-9379-64cd7cc32c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)  # automatically restarts kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17257a2d-958c-44b0-80e3-aba254a12bba",
   "metadata": {},
   "source": [
    "Move to the next notebook to explore synthetic data generation with NeMo Curator. This will allow us to learn how to create artificial data for various tasks, enhancing the diversity and richness of our dataset.\n",
    "\n",
    "Let's move to the [synthetic_data_generation](03_synthetic_data_generation.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99528953-b137-4660-8a86-db1a0715e350",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
