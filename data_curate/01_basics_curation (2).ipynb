{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb478833-19ac-41a1-923f-6cc9f64181d5",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0936ea03-3f38-49e9-91c3-2d370769c8a8",
   "metadata": {},
   "source": [
    "# 1. Basics of Data Curation\n",
    "\n",
    "******\n",
    "\n",
    "Generative AI developemet requires a havy data curation process. The quality the model largely depends on the quality of the data used for training. NVIDIA NeMo Curator is an open-source framework designed to streamline this process by preparing large-scale, high-quality datasets for pretraining and continuous training.\n",
    "\n",
    "NeMo Curator offers built-in workflows for curating data from various public sources such as Common Crawl, Wikipedia, and arXiv. At the same time, it provides the flexibility to customize pipelines to suit the specific needs of your project.\n",
    "\n",
    "This notebook guides the process of basic data preparation involved in most Language Models developements: \n",
    "\n",
    "**[1.1 Text Cleaning and Unification](#1.1-Text-Cleaning-and-Unification)<br>**\n",
    "**[1.2 Document Size Filtering](#1.2-Document-Size-Filtering)<br>**\n",
    "**[1.3 Filter Personally Identifiable Information (PII)](#1.3-Filter-Personally-Identifiable-Information-(PII))<br>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cfc4b2-8fd0-4cbf-86ae-094e6c8fead9",
   "metadata": {},
   "source": [
    "***************\n",
    "### Environment Setup\n",
    "\n",
    "For large-scale data processing, NeMo Curator provides both GPU and CPU based modules. Understanding how these modules interact and how to configure your environment is key to optimizing performance.\n",
    "\n",
    "CPU-based modules rely on [Dask](https://www.dask.org/) to distribute computations across multi-node clusters while GPU-accelerated modules uses [RAPIDS](https://rapids.ai/) to handle large-scale datasets efficiently.\n",
    "\n",
    "Let's check first your current environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "652486cf-06ef-43af-b136-96cda3b7714c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:             x86_64\n",
      "  CPU op-mode(s):         32-bit, 64-bit\n",
      "  Address sizes:          39 bits physical, 48 bits virtual\n",
      "  Byte Order:             Little Endian\n",
      "CPU(s):                   32\n",
      "  On-line CPU(s) list:    0-31\n",
      "Vendor ID:                GenuineIntel\n",
      "  Model name:             Intel(R) Core(TM) i9-14900HX\n",
      "    CPU family:           6\n",
      "    Model:                183\n",
      "    Thread(s) per core:   2\n",
      "    Core(s) per socket:   16\n",
      "    Socket(s):            1\n",
      "    Stepping:             1\n",
      "    BogoMIPS:             4838.40\n",
      "    Flags:                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge m\n",
      "                          ca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht sysc\n",
      "                          all nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xt\n",
      "                          opology tsc_reliable nonstop_tsc cpuid tsc_known_freq \n",
      "                          pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2\n",
      "                          apic movbe popcnt tsc_deadline_timer aes xsave avx f16\n",
      "                          c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibr\n",
      "                          s ibpb stibp ibrs_enhanced tpr_shadow ept vpid ept_ad \n",
      "                          fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid r\n",
      "                          dseed adx smap clflushopt clwb sha_ni xsaveopt xsavec \n",
      "                          xgetbv1 xsaves avx_vnni vnmi umip waitpkg gfni vaes vp\n",
      "                          clmulqdq rdpid movdiri movdir64b fsrm md_clear seriali\n",
      "                          ze flush_l1d arch_capabilities\n",
      "Virtualization features:  \n",
      "  Virtualization:         VT-x\n",
      "  Hypervisor vendor:      Microsoft\n",
      "  Virtualization type:    full\n",
      "Caches (sum of all):      \n",
      "  L1d:                    768 KiB (16 instances)\n",
      "  L1i:                    512 KiB (16 instances)\n",
      "  L2:                     32 MiB (16 instances)\n",
      "  L3:                     36 MiB (1 instance)\n",
      "NUMA:                     \n",
      "  NUMA node(s):           1\n",
      "  NUMA node0 CPU(s):      0-31\n",
      "Vulnerabilities:          \n",
      "  Gather data sampling:   Not affected\n",
      "  Itlb multihit:          Not affected\n",
      "  L1tf:                   Not affected\n",
      "  Mds:                    Not affected\n",
      "  Meltdown:               Not affected\n",
      "  Mmio stale data:        Not affected\n",
      "  Reg file data sampling: Mitigation; Clear Register File\n",
      "  Retbleed:               Mitigation; Enhanced IBRS\n",
      "  Spec rstack overflow:   Not affected\n",
      "  Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prct\n",
      "                          l\n",
      "  Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointe\n",
      "                          r sanitization\n",
      "  Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditiona\n",
      "                          l; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\n",
      "  Srbds:                  Not affected\n",
      "  Tsx async abort:        Not affected\n"
     ]
    }
   ],
   "source": [
    "# check CPU details\n",
    "! lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d45a837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor: x86_64\n",
      "Physical cores: 16\n",
      "Total cores: 32\n",
      "Max Frequency: 0.00 MHz\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import psutil  # you may need to install: pip install psutil\n",
    "\n",
    "print(f\"Processor: {platform.processor()}\")\n",
    "print(f\"Physical cores: {psutil.cpu_count(logical=False)}\")\n",
    "print(f\"Total cores: {psutil.cpu_count(logical=True)}\")\n",
    "print(f\"Max Frequency: {psutil.cpu_freq().max:.2f} MHz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4de6562-aad0-4374-a674-470a86ae665a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan  2 13:01:40 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.75                 Driver Version: 566.24         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...    On  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   51C    P5              6W /   80W |     625MiB /   8188MiB |     16%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# check GPU details\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba34833-3574-421b-81f7-bd2fd38190cd",
   "metadata": {},
   "source": [
    "NeMo Curator provides a simple function `get_client` that can be used to start a local Dask cluster or connect to an existing one.  Let's initialize the Dask Cluster. \n",
    "\n",
    "The next cell starts a Dask `LocalCluster` on your CPU. It can be reused for all modules except for deduplication, which requires a GPU cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f7f130-88b8-4e2c-b2f0-1a26bcfebcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.utils.distributed_utils import get_client\n",
    "\n",
    "# Start Dask cluster with limited workers for WSL stability\n",
    "client = get_client(\n",
    "    cluster_type=\"cpu\",\n",
    "    n_workers=4,  # Reduced from default (32 cores) to prevent worker crashes\n",
    "    threads_per_worker=2,  # Limit threads per worker\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b1c0923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nemo-curator\n",
      "  Downloading nemo_curator-1.0.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: absl-py<3.0.0,>=2.0.0 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nemo-curator) (2.1.0)\n",
      "Collecting comment_parser (from nemo-curator)\n",
      "  Downloading comment_parser-1.2.4.tar.gz (8.3 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting cosmos-xenna==0.1.2 (from nemo-curator)\n",
      "  Downloading cosmos_xenna-0.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nemo-curator) (2024.12.0)\n",
      "Collecting jieba==0.42.1 (from nemo-curator)\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "     ---------------------------------------- 0.0/19.2 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 3.4/19.2 MB 16.8 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 6.8/19.2 MB 16.1 MB/s eta 0:00:01\n",
      "     -------------------- ------------------ 10.0/19.2 MB 16.3 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 13.6/19.2 MB 16.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 17.0/19.2 MB 16.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 19.2/19.2 MB 16.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting loguru (from nemo-curator)\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting mecab-python3 (from nemo-curator)\n",
      "  Downloading mecab_python3-1.0.12-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: pandas>=2.1.0 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nemo-curator) (2.2.3)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nemo-curator) (20.0.0)\n",
      "Collecting ray>=2.49 (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading ray-2.53.0-cp312-cp312-win_amd64.whl.metadata (23 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nemo-curator) (2.5.1+cu118)\n",
      "Collecting transformers==4.55.2 (from nemo-curator)\n",
      "  Downloading transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: attrs in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from cosmos-xenna==0.1.2->nemo-curator) (24.3.0)\n",
      "Collecting cattrs (from cosmos-xenna==0.1.2->nemo-curator)\n",
      "  Downloading cattrs-25.3.0-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from cosmos-xenna==0.1.2->nemo-curator) (3.1.4)\n",
      "Collecting pulp (from cosmos-xenna==0.1.2->nemo-curator)\n",
      "  Downloading pulp-3.3.0-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting tabulate (from cosmos-xenna==0.1.2->nemo-curator)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers==4.55.2->nemo-curator) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers==4.55.2->nemo-curator)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers==4.55.2->nemo-curator) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers==4.55.2->nemo-curator) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers==4.55.2->nemo-curator) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers==4.55.2->nemo-curator) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers==4.55.2->nemo-curator) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers==4.55.2->nemo-curator) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers==4.55.2->nemo-curator) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers==4.55.2->nemo-curator) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas>=2.1.0->nemo-curator) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas>=2.1.0->nemo-curator) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas>=2.1.0->nemo-curator) (2024.2)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ray>=2.49->ray[data,default]>=2.49->nemo-curator) (8.1.7)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ray>=2.49->ray[data,default]>=2.49->nemo-curator) (4.23.0)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray>=2.49->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading msgpack-1.1.2-cp312-cp312-win_amd64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: protobuf>=3.20.3 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ray>=2.49->ray[data,default]>=2.49->nemo-curator) (5.29.2)\n",
      "Requirement already satisfied: aiohttp>=3.7 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ray[data,default]>=2.49->nemo-curator) (3.11.11)\n",
      "Collecting aiohttp_cors (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading aiohttp_cors-0.8.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting colorful (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading colorful-0.5.8-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting py-spy>=0.4.0 (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading py_spy-0.4.1-py2.py3-none-win_amd64.whl.metadata (510 bytes)\n",
      "Requirement already satisfied: grpcio>=1.42.0 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ray[data,default]>=2.49->nemo-curator) (1.71.0)\n",
      "Collecting opencensus (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting opentelemetry-sdk>=1.30.0 (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-prometheus (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading opentelemetry_exporter_prometheus-0.60b1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-proto (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pydantic!=2.0.*,!=2.1.*,!=2.10.*,!=2.11.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3 (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: prometheus_client>=0.7.1 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ray[data,default]>=2.49->nemo-curator) (0.22.1)\n",
      "Collecting smart_open (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading virtualenv-20.35.4-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting python-magic==0.4.24 (from comment_parser->nemo-curator)\n",
      "  Downloading python_magic-0.4.24-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from loguru->nemo-curator) (0.4.6)\n",
      "Collecting win32-setctime>=1.0.0 (from loguru->nemo-curator)\n",
      "  Downloading win32_setctime-1.2.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->nemo-curator) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->nemo-curator) (3.2.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->nemo-curator) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->nemo-curator) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy==1.13.1->torch->nemo-curator) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp>=3.7->ray[data,default]>=2.49->nemo-curator) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp>=3.7->ray[data,default]>=2.49->nemo-curator) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp>=3.7->ray[data,default]>=2.49->nemo-curator) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp>=3.7->ray[data,default]>=2.49->nemo-curator) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp>=3.7->ray[data,default]>=2.49->nemo-curator) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp>=3.7->ray[data,default]>=2.49->nemo-curator) (1.18.3)\n",
      "Collecting opentelemetry-api==1.39.1 (from opentelemetry-sdk>=1.30.0->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk>=1.30.0->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting importlib-metadata<8.8.0,>=6.0 (from opentelemetry-api==1.39.1->opentelemetry-sdk>=1.30.0->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading importlib_metadata-8.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.10.*,!=2.11.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3->ray[data,default]>=2.49->nemo-curator) (0.7.0)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic!=2.0.*,!=2.1.*,!=2.10.*,!=2.11.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3->ray[data,default]>=2.49->nemo-curator)\n",
      "  Using cached pydantic_core-2.41.5-cp312-cp312-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch->nemo-curator)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic!=2.0.*,!=2.1.*,!=2.10.*,!=2.11.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3->ray[data,default]>=2.49->nemo-curator)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.8.2->pandas>=2.1.0->nemo-curator) (1.17.0)\n",
      "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[data,default]>=2.49->nemo-curator) (4.3.6)\n",
      "Collecting attrs (from cosmos-xenna==0.1.2->nemo-curator)\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->cosmos-xenna==0.1.2->nemo-curator) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jsonschema->ray>=2.49->ray[data,default]>=2.49->nemo-curator) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jsonschema->ray>=2.49->ray[data,default]>=2.49->nemo-curator) (0.36.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jsonschema->ray>=2.49->ray[data,default]>=2.49->nemo-curator) (0.22.3)\n",
      "Collecting opencensus-context>=0.1.3 (from opencensus->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from opencensus->ray[data,default]>=2.49->nemo-curator) (2.24.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers==4.55.2->nemo-curator) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers==4.55.2->nemo-curator) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers==4.55.2->nemo-curator) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers==4.55.2->nemo-curator) (2024.12.14)\n",
      "Requirement already satisfied: wrapt in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from smart_open->ray[data,default]>=2.49->nemo-curator) (1.17.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,default]>=2.49->nemo-curator) (1.69.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,default]>=2.49->nemo-curator) (1.26.1)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,default]>=2.49->nemo-curator) (2.38.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,default]>=2.49->nemo-curator) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,default]>=2.49->nemo-curator) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,default]>=2.49->nemo-curator) (4.9)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api==1.39.1->opentelemetry-sdk>=1.30.0->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,default]>=2.49->nemo-curator) (0.6.1)\n",
      "Downloading nemo_curator-1.0.0-py3-none-any.whl (441 kB)\n",
      "Downloading cosmos_xenna-0.1.2-py3-none-any.whl (185 kB)\n",
      "Downloading transformers-4.55.2-py3-none-any.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 3.4/11.3 MB 16.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.8/11.3 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.2/11.3 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 16.0 MB/s eta 0:00:00\n",
      "Downloading ray-2.53.0-cp312-cp312-win_amd64.whl (27.2 MB)\n",
      "   ---------------------------------------- 0.0/27.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 3.1/27.2 MB 16.8 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 4.5/27.2 MB 11.2 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 6.6/27.2 MB 10.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 9.4/27.2 MB 12.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 11.0/27.2 MB 10.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 14.4/27.2 MB 11.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 17.8/27.2 MB 12.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 19.9/27.2 MB 12.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 19.9/27.2 MB 12.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 23.3/27.2 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.7/27.2 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 27.2/27.2 MB 11.4 MB/s eta 0:00:00\n",
      "Downloading python_magic-0.4.24-py2.py3-none-any.whl (12 kB)\n",
      "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "Downloading mecab_python3-1.0.12-cp312-cp312-win_amd64.whl (502 kB)\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 566.1/566.1 kB 19.0 MB/s eta 0:00:00\n",
      "Downloading msgpack-1.1.2-cp312-cp312-win_amd64.whl (72 kB)\n",
      "Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n",
      "Downloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n",
      "Downloading py_spy-0.4.1-py2.py3-none-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 14.2 MB/s eta 0:00:00\n",
      "Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading virtualenv-20.35.4-py3-none-any.whl (6.0 MB)\n",
      "   ---------------------------------------- 0.0/6.0 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 3.4/6.0 MB 15.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.0/6.0 MB 15.3 MB/s eta 0:00:00\n",
      "Downloading win32_setctime-1.2.0-py3-none-any.whl (4.1 kB)\n",
      "Downloading aiohttp_cors-0.8.1-py3-none-any.whl (25 kB)\n",
      "Downloading cattrs-25.3.0-py3-none-any.whl (70 kB)\n",
      "Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Downloading colorful-0.5.8-py2.py3-none-any.whl (201 kB)\n",
      "Downloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
      "Downloading opentelemetry_exporter_prometheus-0.60b1-py3-none-any.whl (13 kB)\n",
      "Downloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\n",
      "Downloading pulp-3.3.0-py3-none-any.whl (16.4 MB)\n",
      "   ---------------------------------------- 0.0/16.4 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 3.4/16.4 MB 16.7 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 6.6/16.4 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 10.0/16.4 MB 16.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 13.4/16.4 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.4/16.4 MB 16.1 MB/s eta 0:00:00\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)\n",
      "Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading importlib_metadata-8.7.1-py3-none-any.whl (27 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Building wheels for collected packages: jieba, comment_parser\n",
      "  Building wheel for jieba (setup.py): started\n",
      "  Building wheel for jieba (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314474 sha256=cdad4b52e3139166cad7629fd604839260b50f762cde1e165266fee38c1642e6\n",
      "  Stored in directory: c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local\\pip\\cache\\wheels\\08\\a1\\a3\\5c8ac52cc2f5782ffffc34c95c57c8e5ecb3063dc69541ee7c\n",
      "  Building wheel for comment_parser (setup.py): started\n",
      "  Building wheel for comment_parser (setup.py): finished with status 'done'\n",
      "  Created wheel for comment_parser: filename=comment_parser-1.2.4-py3-none-any.whl size=11895 sha256=406935f6e28e33023c02ad8a0c41dde9ccd68fc008aba7ee58b8c77bebdc4eca\n",
      "  Stored in directory: c:\\users\\tomas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local\\pip\\cache\\wheels\\26\\cc\\a5\\bce19d6af1aea3f5ac143d10713fc7b9f918874266042cab39\n",
      "Successfully built jieba comment_parser\n",
      "Installing collected packages: py-spy, opencensus-context, mecab-python3, jieba, distlib, zipp, win32-setctime, virtualenv, typing-extensions, tabulate, smart_open, python-magic, pulp, opentelemetry-proto, msgpack, colorful, attrs, typing-inspection, pydantic-core, loguru, importlib-metadata, huggingface-hub, comment_parser, cattrs, pydantic, opentelemetry-api, aiohttp_cors, transformers, opentelemetry-semantic-conventions, opencensus, ray, opentelemetry-sdk, opentelemetry-exporter-prometheus, cosmos-xenna, nemo-curator\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 24.3.0\n",
      "    Uninstalling attrs-24.3.0:\n",
      "      Successfully uninstalled attrs-24.3.0\n",
      "  Attempting uninstall: typing-inspection\n",
      "    Found existing installation: typing-inspection 0.4.0\n",
      "    Uninstalling typing-inspection-0.4.0:\n",
      "      Successfully uninstalled typing-inspection-0.4.0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.33.1\n",
      "    Uninstalling pydantic_core-2.33.1:\n",
      "      Successfully uninstalled pydantic_core-2.33.1\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.28.1\n",
      "    Uninstalling huggingface-hub-0.28.1:\n",
      "      Successfully uninstalled huggingface-hub-0.28.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.11.2\n",
      "    Uninstalling pydantic-2.11.2:\n",
      "      Successfully uninstalled pydantic-2.11.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.48.2\n",
      "    Uninstalling transformers-4.48.2:\n",
      "      Successfully uninstalled transformers-4.48.2\n",
      "Successfully installed aiohttp_cors-0.8.1 attrs-25.4.0 cattrs-25.3.0 colorful-0.5.8 comment_parser-1.2.4 cosmos-xenna-0.1.2 distlib-0.4.0 huggingface-hub-0.36.0 importlib-metadata-8.7.1 jieba-0.42.1 loguru-0.7.3 mecab-python3-1.0.12 msgpack-1.1.2 nemo-curator-1.0.0 opencensus-0.11.4 opencensus-context-0.1.3 opentelemetry-api-1.39.1 opentelemetry-exporter-prometheus-0.60b1 opentelemetry-proto-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 pulp-3.3.0 py-spy-0.4.1 pydantic-2.12.5 pydantic-core-2.41.5 python-magic-0.4.24 ray-2.53.0 smart_open-7.5.0 tabulate-0.9.0 transformers-4.55.2 typing-extensions-4.15.0 typing-inspection-0.4.2 virtualenv-20.35.4 win32-setctime-1.2.0 zipp-3.23.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\tomas\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install nemo-curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80987733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9.0\n"
     ]
    }
   ],
   "source": [
    "import nemo_curator\n",
    "print(nemo_curator.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7accb842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nemo-curator\n",
      "  Downloading nemo_curator-1.0.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting absl-py<3.0.0,>=2.0.0 (from nemo-curator)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting comment_parser (from nemo-curator)\n",
      "  Downloading comment_parser-1.2.4.tar.gz (8.3 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cosmos-xenna==0.1.2 (from nemo-curator)\n",
      "  Downloading cosmos_xenna-0.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting fsspec (from nemo-curator)\n",
      "  Downloading fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jieba==0.42.1 (from nemo-curator)\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting loguru (from nemo-curator)\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting mecab-python3 (from nemo-curator)\n",
      "  Downloading mecab_python3-1.0.12-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pandas>=2.1.0 (from nemo-curator)\n",
      "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Collecting pyarrow (from nemo-curator)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting ray>=2.49 (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading ray-2.53.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting torch (from nemo-curator)\n",
      "  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting transformers==4.55.2 (from nemo-curator)\n",
      "  Downloading transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: attrs in ./nemo_env/lib/python3.12/site-packages (from cosmos-xenna==0.1.2->nemo-curator) (25.4.0)\n",
      "Collecting cattrs (from cosmos-xenna==0.1.2->nemo-curator)\n",
      "  Downloading cattrs-25.3.0-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: jinja2 in ./nemo_env/lib/python3.12/site-packages (from cosmos-xenna==0.1.2->nemo-curator) (3.1.6)\n",
      "Collecting pulp (from cosmos-xenna==0.1.2->nemo-curator)\n",
      "  Downloading pulp-3.3.0-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting tabulate (from cosmos-xenna==0.1.2->nemo-curator)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting filelock (from transformers==4.55.2->nemo-curator)\n",
      "  Downloading filelock-3.20.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers==4.55.2->nemo-curator)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.17 (from transformers==4.55.2->nemo-curator)\n",
      "  Downloading numpy-2.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./nemo_env/lib/python3.12/site-packages (from transformers==4.55.2->nemo-curator) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./nemo_env/lib/python3.12/site-packages (from transformers==4.55.2->nemo-curator) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.55.2->nemo-curator)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in ./nemo_env/lib/python3.12/site-packages (from transformers==4.55.2->nemo-curator) (2.32.5)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.55.2->nemo-curator)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers==4.55.2->nemo-curator)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.55.2->nemo-curator)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./nemo_env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.55.2->nemo-curator) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers==4.55.2->nemo-curator)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./nemo_env/lib/python3.12/site-packages (from pandas>=2.1.0->nemo-curator) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=2.1.0->nemo-curator)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./nemo_env/lib/python3.12/site-packages (from pandas>=2.1.0->nemo-curator) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in ./nemo_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=2.1.0->nemo-curator) (1.17.0)\n",
      "Collecting click>=7.0 (from ray>=2.49->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: jsonschema in ./nemo_env/lib/python3.12/site-packages (from ray>=2.49->ray[data,default]>=2.49->nemo-curator) (4.25.1)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray>=2.49->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading msgpack-1.1.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting protobuf>=3.20.3 (from ray>=2.49->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting aiohttp>=3.7 (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting aiohttp_cors (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading aiohttp_cors-0.8.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting colorful (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading colorful-0.5.8-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting py-spy>=0.4.0 (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading py_spy-0.4.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (510 bytes)\n",
      "Collecting grpcio>=1.42.0 (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading grpcio-1.76.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting opencensus (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting opentelemetry-sdk>=1.30.0 (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-prometheus (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading opentelemetry_exporter_prometheus-0.60b1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-proto (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pydantic!=2.0.*,!=2.1.*,!=2.10.*,!=2.11.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3 (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: prometheus_client>=0.7.1 in ./nemo_env/lib/python3.12/site-packages (from ray[data,default]>=2.49->nemo-curator) (0.23.1)\n",
      "Collecting smart_open (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading virtualenv-20.35.4-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=2.0.*,!=2.1.*,!=2.10.*,!=2.11.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic!=2.0.*,!=2.1.*,!=2.10.*,!=2.11.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic!=2.0.*,!=2.1.*,!=2.10.*,!=2.11.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp>=3.7->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp>=3.7->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp>=3.7->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.7->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp>=3.7->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp>=3.7->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: idna>=2.0 in ./nemo_env/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp>=3.7->ray[data,default]>=2.49->nemo-curator) (3.11)\n",
      "Collecting opentelemetry-api==1.39.1 (from opentelemetry-sdk>=1.30.0->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk>=1.30.0->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting importlib-metadata<8.8.0,>=6.0 (from opentelemetry-api==1.39.1->opentelemetry-sdk>=1.30.0->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading importlib_metadata-8.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api==1.39.1->opentelemetry-sdk>=1.30.0->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in ./nemo_env/lib/python3.12/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[data,default]>=2.49->nemo-curator) (4.5.1)\n",
      "Collecting python-magic==0.4.24 (from comment_parser->nemo-curator)\n",
      "  Downloading python_magic-0.4.24-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./nemo_env/lib/python3.12/site-packages (from jinja2->cosmos-xenna==0.1.2->nemo-curator) (3.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./nemo_env/lib/python3.12/site-packages (from jsonschema->ray>=2.49->ray[data,default]>=2.49->nemo-curator) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./nemo_env/lib/python3.12/site-packages (from jsonschema->ray>=2.49->ray[data,default]>=2.49->nemo-curator) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./nemo_env/lib/python3.12/site-packages (from jsonschema->ray>=2.49->ray[data,default]>=2.49->nemo-curator) (0.30.0)\n",
      "Collecting opencensus-context>=0.1.3 (from opencensus->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-api-core<3.0.0,>=1.0.0 (from opencensus->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading google_api_core-2.28.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading proto_plus-1.27.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-auth<3.0.0,>=2.14.1 (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading google_auth-2.45.0-py2.py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting cachetools<7.0,>=2.0.0 (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading cachetools-6.2.4-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./nemo_env/lib/python3.12/site-packages (from requests->transformers==4.55.2->nemo-curator) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./nemo_env/lib/python3.12/site-packages (from requests->transformers==4.55.2->nemo-curator) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./nemo_env/lib/python3.12/site-packages (from requests->transformers==4.55.2->nemo-curator) (2025.11.12)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting wrapt (from smart_open->ray[data,default]>=2.49->nemo-curator)\n",
      "  Downloading wrapt-2.0.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: setuptools in ./nemo_env/lib/python3.12/site-packages (from torch->nemo-curator) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch->nemo-curator)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch->nemo-curator)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch->nemo-curator)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch->nemo-curator)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch->nemo-curator)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch->nemo-curator)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch->nemo-curator)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch->nemo-curator)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch->nemo-curator)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch->nemo-curator)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch->nemo-curator)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch->nemo-curator)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch->nemo-curator)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch->nemo-curator)\n",
      "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch->nemo-curator)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch->nemo-curator)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch->nemo-curator)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.1 (from torch->nemo-curator)\n",
      "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch->nemo-curator)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading nemo_curator-1.0.0-py3-none-any.whl (441 kB)\n",
      "Downloading cosmos_xenna-0.1.2-py3-none-any.whl (185 kB)\n",
      "Downloading transformers-4.55.2-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Downloading numpy-2.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading ray-2.53.0-cp312-cp312-manylinux2014_x86_64.whl (72.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading msgpack-1.1.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (427 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "Downloading grpcio-1.76.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n",
      "Downloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n",
      "Downloading importlib_metadata-8.7.1-py3-none-any.whl (27 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "Downloading py_spy-0.4.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading virtualenv-20.35.4-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)\n",
      "Downloading filelock-3.20.1-py3-none-any.whl (16 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading aiohttp_cors-0.8.1-py3-none-any.whl (25 kB)\n",
      "Downloading cattrs-25.3.0-py3-none-any.whl (70 kB)\n",
      "Downloading colorful-0.5.8-py2.py3-none-any.whl (201 kB)\n",
      "Downloading python_magic-0.4.24-py2.py3-none-any.whl (12 kB)\n",
      "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "Downloading mecab_python3-1.0.12-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (591 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.4/591.4 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
      "Downloading google_api_core-2.28.1-py3-none-any.whl (173 kB)\n",
      "Downloading google_auth-2.45.0-py2.py3-none-any.whl (233 kB)\n",
      "Downloading cachetools-6.2.4-py3-none-any.whl (11 kB)\n",
      "Downloading googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Downloading proto_plus-1.27.0-py3-none-any.whl (50 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading opentelemetry_exporter_prometheus-0.60b1-py3-none-any.whl (13 kB)\n",
      "Downloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\n",
      "Downloading pulp-3.3.0-py3-none-any.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m  \u001b[33m0:00:11\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-2.0.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (121 kB)\n",
      "Building wheels for collected packages: jieba, comment_parser\n",
      "  Building wheel for jieba (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314509 sha256=8d32c64391e1f1c6fecfa060678559ff88c97df94db08c217ddc752ae249e1c5\n",
      "  Stored in directory: /home/aibeceles/.cache/pip/wheels/08/a1/a3/5c8ac52cc2f5782ffffc34c95c57c8e5ecb3063dc69541ee7c\n",
      "  Building wheel for comment_parser (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for comment_parser: filename=comment_parser-1.2.4-py3-none-any.whl size=11913 sha256=5b96f57bf840dde124637e40481de401e249ad565d1fa417df0c73f2616e14b4\n",
      "  Stored in directory: /home/aibeceles/.cache/pip/wheels/26/cc/a5/bce19d6af1aea3f5ac143d10713fc7b9f918874266042cab39\n",
      "Successfully built jieba comment_parser\n",
      "Installing collected packages: pytz, py-spy, opencensus-context, nvidia-cusparselt-cu12, mpmath, mecab-python3, jieba, distlib, colorful, zipp, wrapt, typing-inspection, triton, tqdm, tabulate, sympy, safetensors, regex, python-magic, pydantic-core, pyasn1, pyarrow, pulp, protobuf, propcache, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, msgpack, loguru, hf-xet, grpcio, fsspec, frozenlist, filelock, click, cattrs, cachetools, annotated-types, aiohappyeyeballs, absl-py, yarl, virtualenv, smart_open, rsa, pydantic, pyasn1-modules, proto-plus, pandas, opentelemetry-proto, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, importlib-metadata, huggingface-hub, googleapis-common-protos, comment_parser, aiosignal, tokenizers, opentelemetry-api, nvidia-cusolver-cu12, google-auth, aiohttp, transformers, torch, ray, opentelemetry-semantic-conventions, google-api-core, aiohttp_cors, opentelemetry-sdk, opencensus, opentelemetry-exporter-prometheus, cosmos-xenna, nemo-curator\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m73/84\u001b[0m [transformers]\u001b[33mWARNING: Package(s) not found: nemo-curator\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install nemo-curator\n",
    "! pip show nemo-curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad38150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea8f3322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeMo Curator version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import nemo_curator\n",
    "print(f\"NeMo Curator version: {nemo_curator.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794d3c8c-df05-4291-a17e-beda92da813f",
   "metadata": {},
   "source": [
    "Lear more about Nemo Curator's CPU and GPU Modules with Dask in the dedicated [documentation](https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/cpuvsgpu.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebab4e1-a211-4845-82b3-f779defda8b1",
   "metadata": {},
   "source": [
    "## 1.1 Multilingual Datasets\n",
    "\n",
    "In this notebook, we will use a subset of the [MC4](https://huggingface.co/datasets/allenai/c4), the C4 Multilingual Dataset.\n",
    "\n",
    "For the sake of this exercice, to create a more diverse dataset:\n",
    "- We merged Spanish and French samples (100 per language)\n",
    "- We duplicated all samples (making 200 samples per language)\n",
    "- We shuffled the samples\n",
    "\n",
    "So, we have 400 samples, 200 from each language. The structure is a JSON format with 3 filed: `text`, `timestamp` and `url`. \n",
    "\n",
    "Let's have a look at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a8427d4-8e4d-4731-8e5b-b417f9165f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dataset file path\n",
    "multilingual_data = \"./original_data/file.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "271915b3-498f-4445-a415-05d8b599e566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 ./original_data/file.json\n"
     ]
    }
   ],
   "source": [
    "# check number of samples\n",
    "! wc -l {multilingual_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72dee403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 ./original_data/file.json\n"
     ]
    }
   ],
   "source": [
    "# Count lines in a file\n",
    "with open(multilingual_data, 'r', encoding='utf-8') as f:\n",
    "    line_count = sum(1 for line in f)\n",
    "print(f\"{line_count} {multilingual_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d37a2ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: powershell: command not found\n"
     ]
    }
   ],
   "source": [
    "# In a Jupyter cell\n",
    "! powershell -Command \"(Get-Content '{multilingual_data}').Count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "852ec88c-5ac8-40c8-a0c2-3647ec086182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: jq: command not found\n"
     ]
    }
   ],
   "source": [
    "# show the 3 first samples\n",
    "! head -n 3 {multilingual_data} | jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b224be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"Dragon Ball: Le 20e film de la sage sortira le 14 d\\u00e9cembre, premi\\u00e8re image teaser sur Buzz, insolite et culture\\nDragon Ball: Le 20e film de la sage sortira le 14 d\\u00e9cembre, premi\\u00e8re image teaser\\nLe 20e film Dragon Ball sortira le vendredi 14 d\\u00e9cembre 2018. La premi\\u00e8re affiche teaser montre un Gok\\u00fb jeune adulte, environ celui de la fin de Dragon Ball et le d\\u00e9but de Dragon Ball Z. \\u00c0 lire aussi >>> Le gouvernement mexicain pr\\u00e9voit la diffusion sur place publique des \\u00e9pisodes 130 et 131 de Dragon [\\u2026]...\\nLire la suite du buzz sur bleachmx Source : bleachmx - 12/03/2018 22:31 - trending_up142\\nfilm comm\\u00e9moration Akira Toriyama dbz dragon ball Dragon Ball Super dragon ball z affiche Dragon Ball Super Anime V Jump D\\u00e9cembre 2018 Dragon Ball Z Battle of Gods Dragon Ball Z Fukkatsu No [F] Dragon Ball Z La R\\u00e9surrection de [F] Potins Films\\nLe site Deadline indique ce Jeudi que le film d\\u2019animation Dragon Ball Super \\u2013 Broly a rapport\\u00e9, selon les estimations, plus de 7 millions de dollars pour sa premi\\u00e8re journ\\u00e9e d\\u2019exploitation aux \\u00c9tats-Unis. Il pr\\u00e9voit que le film rapporte plus de 15 millions de dollars pour sa premi\\u00e8re semaine. \\u00c0 voir aussi >>> Dragon Ball [\\u2026] ...\\nSource : bleachmx - 18/01/2019 00:16 - trending_up 15\\nDragon Ball Super Broly: Le film d\\u2019animation pr\\u00e9vu dans 90 pays - bleachmx\\nDragon Ball Super \\u2013 Broly: Le film adapt\\u00e9 en manga et en light novel - bleachmx\\nLors du DRAGON BALL Games SUPER Showcase il a \\u00e9t\\u00e9 annonc\\u00e9 que le je vid\\u00e9o Super Dragon Ball Heroes: World Mission sortira sur Nintendo Switch et PC (via Steam) le 5 avril 2019 en occident. Le trailer sous-titr\\u00e9 a \\u00e9t\\u00e9 pr\\u00e9sent\\u00e9 ainsi qu\\u2019une vid\\u00e9o de gameplays. La vid\\u00e9o pr\\u00e9sente Cirrus (Shiirus \\u2013 Shiirasu), le nouveau [\\u2026] ...\\nSource : bleachmx - 15/01/2019 01:45 - trending_up 22\\nSuper Dragon Ball Heroes: World Mission : Deuxi\\u00e8me trailer du jeu vid\\u00e9o - bleachmx\\nDes avants-premi\\u00e8res pour le film Dragon Ball Super Broly dans les cin\\u00e9mas CGR\\nLe film Dragon Ball Super Broly est attendu comme le messie par les fans de l'univers d'Akira Toriyama. Apr\\u00e8s les annonces d'avants-premi\\u00e8res au ...\\nSource : manga-news - 08/01/2019 11:00 - trending_up 15\\nLes avants-premi\\u00e8res fran\\u00e7aises du film Dragon Ball Super Broly d\\u00e9voil\\u00e9es\\nParticuli\\u00e8rement attendu, le film Dragon Ball Super Broly sortira dans les cin\\u00e9mas de France le 13 mars. Mais avant \\u00e7a, plusieurs avant-premi\\u00e8res ...\\nSource : manga-news - 04/01/2019 15:16 - trending_up 16\\nAvant-premi?res de Dragon Ball Super Broly dans toute la France (Les 23 et 24 janvier 2019)\\nLes 23 et 24 janvier 2019 Le film Dragon Ball Super Broly est sorti au Japon le 13 d\\u00e9cembre 2018 et para\\u00eetra \\u00e9galement au cin\\u00e9ma en France le 13 mars 2019. Avant cette date, plusieurs avant-premi\\u00e8res son programm\\u00e9s en janvier. Mercredi 23 et jeudi 24 janvier Le Grand Rex (Paris) Jeudi 24 janvier Path\\u00e9 : St Herblain, Path\\u00e9 Nantes-Atlantis (44, Loire-Atlantique) Lab\\u00e8ge, Gaumont (31, Haute-Garonne) Toulouse, Gaumont Wilson ( 31, Haute-Garonne) Belle-Epine, Path\\u00e9 (94, Thiais) ... ...\\nSource : animint - 04/01/2019 14:46 - trending_up 26\\nOfficiel : le film Dragon Ball Super : Broly en France le 13 mars (VOSTFR et VF) ! - animeland\\nDragon Ball Super \\u2013 Broly : Le film dans les cin\\u00e9mas fran\\u00e7ais en XXXX 2019 - bleachmx\\nDragon Ball Super \\u2013 Broly : Le film dans les cin\\u00e9mas fran\\u00e7ais en XXXX 2019 ? - bleachmx\\nSuper Dragon Ball Heroes : \\u00c9pisode 6, preview et date de sortie de l\\u2019\\u00e9pisode 7\\nLe site officiel de l\\u2019anime promotionnel Super Dragon Ball Heroes: Universal Mission a mis en ligne une affiche, un synopsis ainsi qu\\u2019un teaser vid\\u00e9o pour l\\u2019\\u00e9pisode 7. L\\u2019\\u00e9pisode 7 sortira le 10 janvier 2019. \\u00c0 voir aussi >>> Dragon Ball Super \\u2013 Broly : 2 milliards de yens de recettes, le film bat des records [\\u2026] ...\\nSource : bleachmx - 30/12/2018 00:02 - trending_up 39\\nSuper Dragon Ball Heroes : \\u00c9pisode 5, preview date de sortie de l\\u2019\\u00e9pisode 6 - bleachmx\\nLe jeu Super Dragon Ball Heroes : World Mission dat\\u00e9 en Occident - manga-news\\nDragon Ball Super \\u2013 Broly : 2 milliards de yens de recettes, le film bat des records au box office japonais\\nLe journal Mainichi Shimbun a annonc\\u00e9 dans ses pages que le film d\\u2019animation Dragon Ball Super: Broly a rapport\\u00e9 2 milliards de yens (18,1 millions de dollars $) en 11 jours au box office japonais. Il est le film de la franchise a avoir atteint le plus rapidement la barre des 2 milliards de yens. [\\u2026] ...\\nSource : bleachmx - 26/12/2018 17:46 - trending_up 15\\nSuper Dragon Ball Heroes : \\u00c9pisode 6, preview et date de sortie de l\\u2019\\u00e9pisode 7 - bleachmx\\nDragon Ball Super \\u2013 Broly: Un Vegeta enfin respect\\u00e9, les premi\\u00e8res minutes \\u00e9mouvantes et l\\u2019avant-premi\\u00e8re mondiale - bleachmx\\nLe film animation Dragon Ball Super Broly rapporte 15 millions \\u20ac en 11 jours au Japon\\nLe webjournal japonais Mantan Web a r\\u00e9v\\u00e9l\\u00e9 que le film animation Dragon Ball Super Broly a g\\u00e9n\\u00e9r\\u00e9 2 milliards de yen de recettes (15,9 millions \\u20ac) en 11 jours ! ...\\nSource : adala-news - 25/12/2018 11:01 - trending_up 43\\nLe film animation Dragon Ball Super Broly, en Trailer 2 - adala-news\\nLe film animation Dragon Ball Super Broly, en Trailer 3 - adala-news\\nTop 5 des films animation Dragon Ball les plus populaires au Japon\\nLa Toei a d\\u00e9voil\\u00e9 le classement des films animation Dragon Ball pr\\u00e9f\\u00e9r\\u00e9s des japonais ! ...\\nSource : adala-news - 24/12/2018 00:03 - trending_up 28\\nPremiers chiffres du film animation Dragon Ball Super Broly au Japon - adala-news\\nLe film animation Dragon Ball Super Broly rapporte 15 millions \\u20ac en 11 jours au Japon - adala-news\\nDragon Ball Super Chapitre Scan 043 VF\\nLe chapitre 43 de Dragon Ball Super pour ce mois de d\\u00e9cembre 2018 pour cl\\u00f4turer l\\u2019ann\\u00e9e. Le film Dragon Ball Super \\u2013 Broly est enfin sorti au Japon et le manga entame l\\u2019Arc du Prisonnier de la Patrouille Galactique. C\\u2019est donc une nouvelle fois le film qui fait la couverture du magazine. Toutes les pages [\\u2026] ...\\nSource : bleachmx - 21/12/2018 04:00 - trending_up 38\",\n",
      "  \"timestamp\": 1548042730000,\n",
      "  \"url\": \"https://cultinfos.com/buzz/332814-dragon-ball-20e-film-de-sage-sortira-14-decembre-premiere-image-teaser\"\n",
      "}\n",
      "{\n",
      "  \"text\": \"Cours D'histoire Des \\u00c9tats Europ\\u00e9ens: Depuis Le Bouleversement De L'empire Romain D'occident Jusqu'en 1789, Volume 1 (French Edition) \\u27a0 Maximilien Samson Frederic Schoell | \\u0411\\u0443\\u043a\\u0432\\u043e\\u0435\\u0434 ISBN 978-5-8792-8565-9\\nCours D'histoire Des \\u00c9tats Europ\\u00e9ens: Depuis Le Bouleversement De L'empire Romain D'occident Jusqu'en 1789, Volume 1 (French Edition)\\nISBN: 978-5-8792-8565-9\\n\\u041a\\u043e\\u0434: pod 1702886\\n\\u0410\\u0432\\u0442\\u043e\\u0440\\u044b: Samson, Schoell\\n\\u0421 \\u0442\\u043e\\u0432\\u0430\\u0440\\u043e\\u043c \\u00abCours D'histoire Des \\u00c9tats Europ\\u00e9ens: Depuis Le Bouleversement De L'empire Romain D'occident Jusqu'en 1789, Volume 1 (French Edition)\\u00bb \\u0447\\u0430\\u0441\\u0442\\u043e \\u043f\\u043e\\u043a\\u0443\\u043f\\u0430\\u044e\\u0442\\n\\u0415\\u0441\\u043b\\u0438 \\u0412\\u044b \\u043e\\u0431\\u043d\\u0430\\u0440\\u0443\\u0436\\u0438\\u043b\\u0438 \\u043e\\u0448\\u0438\\u0431\\u043a\\u0443 \\u0432 \\u043e\\u043f\\u0438\\u0441\\u0430\\u043d\\u0438\\u0438 \\u0442\\u043e\\u0432\\u0430\\u0440\\u0430 \\u00abCours D'histoire Des \\u00c9tats Europ\\u00e9ens: Depuis Le Bouleversement De L'empire Romain D'occident Jusqu'en 1789, Volume 1 (French Edition)\\u00bb Maximilien Samson Frederic Schoell, \\u0432\\u044b\\u0434\\u0435\\u043b\\u0438\\u0442\\u0435 \\u0435\\u0451 \\u043c\\u044b\\u0448\\u043a\\u043e\\u0439 \\u0438 \\u043d\\u0430\\u0436\\u043c\\u0438\\u0442\\u0435: Ctrl+Enter. \\u0421\\u043f\\u0430\\u0441\\u0438\\u0431\\u043e!\",\n",
      "  \"timestamp\": 1547767539000,\n",
      "  \"url\": \"https://www.bookvoed.ru/book?id=1433688\"\n",
      "}\n",
      "{\n",
      "  \"text\": \"Se realiz\\u00f3 una jornada de promoci\\u00f3n del buentrato hacia los adultos mayores en R\\u00edo Gallegos - Ministerio de Desarrollo Social\\nSe realiz\\u00f3 una jornada de promoci\\u00f3n del buentrato hacia los adultos mayores en R\\u00edo Gallegos\\nFue el fin de semana pasado en la capital santacruce\\u00f1a. El s\\u00e1bado tuvo lugar una capacitaci\\u00f3n sobre deporte social. El domingo m\\u00e1s de 350 personas participaron de actividades recreativas, charlas y talleres.\\nSe realiz\\u00f3 la jornada de promoci\\u00f3n del buentrato hacia los adultos mayores en R\\u00edo Gallegos.\\nFue el fin de semana pasado en el SUM de Vialidad Nacional de la capital santacruce\\u00f1a.\\nEl s\\u00e1bado tuvo lugar una capacitaci\\u00f3n sobre deporte social.\\nEl domingo m\\u00e1s de 350 personas participaron de actividades recreativas, charlas y talleres.\\nEl Ministerio de Desarrollo Social a trav\\u00e9s del Centro de Referencia de Santa Cruz (CDR) llev\\u00f3 a cabo una jornada de promoci\\u00f3n del buentrato hacia los adultos mayores en la ciudad de R\\u00edo Gallegos. Las actividades se desarrollaron durante el fin de semana pasado en el marco del \\u201cD\\u00eda mundial contra el abuso y el maltrato a los mayores\\u201d.\\nEl s\\u00e1bado, desde el proyecto \\u201cMadurar en Positivo\\u201d perteneciente al Plan Nacional de Deporte Social, se realiz\\u00f3 una capacitaci\\u00f3n en el sal\\u00f3n de usos m\\u00faltiples de Vialidad Nacional dirigida a m\\u00e1s de 50 ciudadanos. Asimismo el domingo, m\\u00e1s de 350 adultos mayores participaron de actividades recreativas y disfrutaron de juegos de kerm\\u00e9s, tejo, sapo, bingo, ping pong, v\\u00f3leibol adaptado, bowling y talleres.\\nLa Subsecretar\\u00eda de Responsabilidad Social brind\\u00f3 una charla para concientizar acerca de la importancia de reducir los factores de riesgo asociados a la vida sedentaria, promoviendo h\\u00e1bitos saludables para mejorar la calidad de vida. Por su parte, la Direcci\\u00f3n de Deporte Social llev\\u00f3 a cabo el \\u201ctaller de la risa, donde a trav\\u00e9s de t\\u00e9cnicas de juegos la directora Nacional, Patricia Borrillo habl\\u00f3 de las herramientas para que las personas mayores aprendan a distanciarse de las preocupaciones y cuenten con distintas alternativas ante la resoluci\\u00f3n de un problema.\\nA su vez, los presentes pudieron acceder de manera gratuita, a una unidad m\\u00f3vil sanitaria del programa \\u201cArgentina Sonr\\u00ede\\u201d del Ministerio de Salud de Naci\\u00f3n. Tambi\\u00e9n se hicieron controles de glucemia, presi\\u00f3n y vacunaci\\u00f3n. Por la tarde, se presentaron diversos n\\u00fameros art\\u00edsticos para compartir en familia: el grupo Papelnonos, el coro del Centro de Jubilados \\u201cEl Despertar\\u201d y el grupo de danzas del Centro de Jubilados \\u201cLa Amistad\\u201d y del centro \\u201cEncuentro de Amigos\\u201d.\\nAl finalizar la jornada, el referente del CDR local, Ariel Fern\\u00e1ndez, destac\\u00f3 las acciones conjuntas entre todos los organismos presentes, afirmando que la actividad no tiene que ver con el trabajo de un ministerio sino con \\u201cun proyecto pol\\u00edtico con una mirada integral\\u201d.\\nLa cartera social a trav\\u00e9s de los CDR y en articulaci\\u00f3n con todos los organismos del Gobierno nacional, concibe a las personas mayores como protagonistas del cambio social y promotores de la cultura del buentrato. De esta manera, se trabajan acciones diarias en pos de mejorar la calidad de vida de los adultos mayores.\\n\\\"Sean transgresores, no le digan que s\\u00ed a todo. Acu\\u00e9rdense la experiencia no se jubila. El desaf\\u00edo es que todos los adultos mayores sean sujetos activos de derecho\\\".\\nLos mayores ense\\u00f1an a los chicos los juegos de su infancia\\nEl Hogar Balestra cumpli\\u00f3 90 a\\u00f1os\\nConoc\\u00e9 m\\u00e1s sobre Adultos mayores\\nEncontr\\u00e1 d\\u00f3nde consultar sobre Adultos mayores\",\n",
      "  \"timestamp\": 1524296308000,\n",
      "  \"url\": \"http://www.desarrollosocial.gob.ar/noticias/se-realizo-una-jornada-de-promocion-del-buentrato-hacia-los-adultos-mayores-en-rio-gallegos/\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Read first 3 lines\n",
    "with open(multilingual_data, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 3:\n",
    "            break\n",
    "        print(json.dumps(json.loads(line), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d22236-bc9e-4668-ae3c-d35bdbf6f5de",
   "metadata": {},
   "source": [
    "Notice, **languages are not annotated in the dataset**, allowing us to leverage AI-based language separation later in the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f276db-4372-4775-b0a0-08b94fe5c3f9",
   "metadata": {},
   "source": [
    "Let's now create a document dataset from a pandas data frame. For more information on the arguments see Dask’s from_pandas documentation\n",
    "\n",
    "NeMo Curator's `DocumentDataset` employs Dask's distributed dataframes to mangage large datasets across multiple nodes and allow for easy restarting of interrupted curation. `DocumentDataset` supports reading and writing to sharded *jsonl* and *parquet* files both on local disk and from remote sources such as S3 bukets.\n",
    "\n",
    "Let's load our multilingual dataset with Nemo Curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3d2d25e-5a7d-4639-9e17-bf09759726c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e230e1fb-898a-47e1-84d0-a2578cd01a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 files with blocksize='1gb' / files_per_partition=None\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.datasets import DocumentDataset\n",
    "\n",
    "multilingual_data_path = \"./original_data\"\n",
    "multilingual_dataset = DocumentDataset.read_json(\n",
    "    multilingual_data_path, add_filename=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2d0481e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: findstr: command not found\n",
      "ERROR: Pipe to stdout was broken\n",
      "Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "! pip list | findstr nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a183c62b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453208e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b89f475e-0284-4681-aec1-a536e0bd751c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Dragon Ball: Le 20e film de la sage sortira le...</td>\n",
       "      <td>2019-01-21 03:52:10</td>\n",
       "      <td>https://cultinfos.com/buzz/332814-dragon-ball-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Cours D'histoire Des États Européens: Depuis L...</td>\n",
       "      <td>2019-01-17 23:25:39</td>\n",
       "      <td>https://www.bookvoed.ru/book?id=1433688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Se realizó una jornada de promoción del buentr...</td>\n",
       "      <td>2018-04-21 07:38:28</td>\n",
       "      <td>http://www.desarrollosocial.gob.ar/noticias/se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name                                               text  \\\n",
       "0  file.json  Dragon Ball: Le 20e film de la sage sortira le...   \n",
       "1  file.json  Cours D'histoire Des États Européens: Depuis L...   \n",
       "2  file.json  Se realizó una jornada de promoción del buentr...   \n",
       "\n",
       "            timestamp                                                url  \n",
       "0 2019-01-21 03:52:10  https://cultinfos.com/buzz/332814-dragon-ball-...  \n",
       "1 2019-01-17 23:25:39            https://www.bookvoed.ru/book?id=1433688  \n",
       "2 2018-04-21 07:38:28  http://www.desarrollosocial.gob.ar/noticias/se...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilingual_dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe1ef92-efd2-434b-96d3-2722bcfcc955",
   "metadata": {},
   "source": [
    "## 1.2 Basic Text cleaning and Unification\n",
    "\n",
    "NeMo Curator provides various `DocumentModifier` implementations such as the `UnicodeReformatter` which uses [ftfy](https://pypi.org/project/ftfy/) (fixes text for you) to resolve all unicode issues in the dataset. \n",
    "\n",
    "It is also possible to implement your custom text cleaner using `DocumentModifier`. For instance, we can standardize inconsistent quotation marks that appear very often in curated large dataset, remove HTML, URLs, and email tags, etc.\n",
    "\n",
    "\n",
    "Let's first create the output folders to save the cleaned step outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf4df995-300b-470d-847b-78839ca859b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set dataset file path\n",
    "curated_data_path = \"./curated\"\n",
    "clean_and_unify_data_path = os.path.join(curated_data_path, \"01_clean_and_unify\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(curated_data_path, exist_ok=True)\n",
    "os.makedirs(clean_and_unify_data_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb4a71-2347-4cd3-a5e9-0711434a467d",
   "metadata": {},
   "source": [
    "Let's now implement a custom text cleaner `QuotationTagUnifier`.\n",
    "\n",
    "It is designed to modify text documents by normalizing quotation marks and removing unwanted elements. \n",
    "\n",
    "The result is a cleaned and standardized text output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643dc7a4-d9e7-464d-864b-baa653cab173",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bd0a641-4063-4781-8d0d-e5e697ad50f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import dask\n",
    "import pandas as pd\n",
    "from nemo_curator.modifiers import DocumentModifier, UnicodeReformatter\n",
    "from nemo_curator.modules.modify import Modify\n",
    "\n",
    "\n",
    "class QuotationTagUnifier(DocumentModifier):\n",
    "    def modify_document(self, text: str) -> str:\n",
    "        text = text.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "        text = text.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "        text = text.replace(\"\\t\", \" \")\n",
    "        text = re.sub(\n",
    "            r\"(<[^>]+>)|(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)\",\n",
    "            \"\",\n",
    "            text,\n",
    "        )\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9838a49-15d8-4bfd-b5ea-c6c7d09652ba",
   "metadata": {},
   "source": [
    "Next, we can chain modifiers together using the `Sequential` class, which takes a list of operations to be run sequentially and applies them to a given `DocumentDataset`.ipynb_checkpoints/\n",
    " \n",
    "Let's call this sequence the `cleaners`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35cd6bbb-0be0-4089-88d7-e46ca1d21c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import Sequential\n",
    "\n",
    "cleaners = Sequential(\n",
    "    [\n",
    "        # Apply: Unify all the quotation marks and remove tags\n",
    "        Modify(QuotationTagUnifier()),\n",
    "        # Apply: Unify all unicode\n",
    "        Modify(UnicodeReformatter()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d43d8e7-a892-46e8-9719-1ea2363d6577",
   "metadata": {},
   "source": [
    "Let's run that on a toy example with few sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cf51cce-2ff4-4ded-a54d-a12773a8074f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ryan went out to play ‘footbal’</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He is very  \\t  happy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Visit &lt;a href='www.example.com'&gt;example.com&lt;/a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                    Ryan went out to play ‘footbal’\n",
       "1                             He is very  \\t  happy.\n",
       "2  Visit <a href='www.example.com'>example.com</a..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the toy samples\n",
    "dataframe_toy = pd.DataFrame(\n",
    "    {\n",
    "        \"text\": [\n",
    "            \"Ryan went out to play ‘footbal’\",\n",
    "            \"He is very  \\t  happy.\",\n",
    "            \"Visit <a href='www.example.com'>example.com</a> for more information or contact us at info@example.com\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset_toy = DocumentDataset(dask.dataframe.from_pandas(dataframe_toy, npartitions=1))\n",
    "\n",
    "# check the samples\n",
    "dataset_toy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705224b3-31d4-4702-97a6-8e2f572b7a6f",
   "metadata": {},
   "source": [
    "Now, let's apply our sequence of cleaners to the toy samples. To execute this sequence on the Dask cluster, we use `.persist()`, which keeps the transformed data in memory for optimized processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc070516-28b6-425d-a9b6-d3792f3ed8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_clean_and_unify = cleaners(dataset_toy).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855bc55b-3a28-4825-baa3-cea46491c084",
   "metadata": {},
   "source": [
    "Let's check the output.\n",
    "\n",
    "Expected output are samples with normalized quotations, removed tabs and HTML, URL and Email tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d78c5a8-4548-4106-8f4d-cab9ffc18eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ryan went out to play 'footbal'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He is very     happy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Visit example.com for more information or cont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                    Ryan went out to play 'footbal'\n",
       "1                              He is very     happy.\n",
       "2  Visit example.com for more information or cont..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check cleaned toy samples\n",
    "dataset_test_clean_and_unify.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52122f-9a73-421a-a3b7-0b4dd234cb2b",
   "metadata": {},
   "source": [
    "Now, let's apply this cleaning step to our multilingual dataset. We can achieve this by creating a sequence of curation steps, starting with the cleaning sequence as the first function in our data curation pipeline.\n",
    "\n",
    "Run the next cell to create the cleaning step as a function that would be the first curation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e999870-cbe9-4d28-8b02-8d6e6a8f80c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the sequence of cleaning operations as a function\n",
    "def clean_and_unify(dataset: DocumentDataset) -> DocumentDataset:\n",
    "    cleaners = Sequential(\n",
    "        [\n",
    "            # Apply: Unify all the quotation marks and remove tags\n",
    "            Modify(QuotationTagUnifier()),\n",
    "            # Apply: Unify all unicode\n",
    "            Modify(UnicodeReformatter()),\n",
    "        ]\n",
    "    )\n",
    "    return cleaners(dataset)\n",
    "\n",
    "\n",
    "# sequence of data curation setps. so far, only cclean_and_unify is defined\n",
    "curation_steps = Sequential([clean_and_unify])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af0dae-640c-48ad-9023-25d19a4caa80",
   "metadata": {},
   "source": [
    "Let's now execute this step on out multilingual dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29d03f50-ddfa-4cb7-86bf-e06f9bd70c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the pipeline...\n",
      "CPU times: user 28 ms, sys: 0 ns, total: 28 ms\n",
      "Wall time: 26.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Executing the pipeline...\")\n",
    "\n",
    "dataset_clean_and_unify = curation_steps(multilingual_dataset).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bee639b-99d7-4026-a87a-90c8dd960335",
   "metadata": {},
   "source": [
    "Let's check some outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc175867-7d83-4452-98df-f79bce5d1297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Dragon Ball: Le 20e film de la sage sortira le...</td>\n",
       "      <td>2019-01-21 03:52:10</td>\n",
       "      <td>https://cultinfos.com/buzz/332814-dragon-ball-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Cours D'histoire Des États Européens: Depuis L...</td>\n",
       "      <td>2019-01-17 23:25:39</td>\n",
       "      <td>https://www.bookvoed.ru/book?id=1433688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Se realizó una jornada de promoción del buentr...</td>\n",
       "      <td>2018-04-21 07:38:28</td>\n",
       "      <td>http://www.desarrollosocial.gob.ar/noticias/se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Restaurantes con Web Y Telefono Y Dias Y Horar...</td>\n",
       "      <td>2020-08-11 16:33:05</td>\n",
       "      <td>http://mendoza.guia.clarin.com/restaurantes-co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Responsable qualité - Intérim : Emploi et recr...</td>\n",
       "      <td>2020-08-07 01:17:37</td>\n",
       "      <td>https://images3.meteojob.com/Emploi-Interim-Re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name                                               text  \\\n",
       "0  file.json  Dragon Ball: Le 20e film de la sage sortira le...   \n",
       "1  file.json  Cours D'histoire Des États Européens: Depuis L...   \n",
       "2  file.json  Se realizó una jornada de promoción del buentr...   \n",
       "3  file.json  Restaurantes con Web Y Telefono Y Dias Y Horar...   \n",
       "4  file.json  Responsable qualité - Intérim : Emploi et recr...   \n",
       "\n",
       "            timestamp                                                url  \n",
       "0 2019-01-21 03:52:10  https://cultinfos.com/buzz/332814-dragon-ball-...  \n",
       "1 2019-01-17 23:25:39            https://www.bookvoed.ru/book?id=1433688  \n",
       "2 2018-04-21 07:38:28  http://www.desarrollosocial.gob.ar/noticias/se...  \n",
       "3 2020-08-11 16:33:05  http://mendoza.guia.clarin.com/restaurantes-co...  \n",
       "4 2020-08-07 01:17:37  https://images3.meteojob.com/Emploi-Interim-Re...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_clean_and_unify.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2abe75-18db-44eb-b881-be16d046fb47",
   "metadata": {},
   "source": [
    "We can save the created Document into a json file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d14fede-e581-4235-81c3-7ace79a593a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to disk complete for 1 partition(s)\n"
     ]
    }
   ],
   "source": [
    "# save output to json\n",
    "dataset_clean_and_unify.to_json(clean_and_unify_data_path, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2aea2e91-550d-494a-8794-533531c341c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: jq: command not found\n"
     ]
    }
   ],
   "source": [
    "! head -n 1 {clean_and_unify_data_path}/file.jsonl | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89228aa9-641f-4a1d-96b5-61fccb703832",
   "metadata": {},
   "source": [
    "## Dataset document size Filtering\n",
    "\n",
    "Extremely short documents may lack sufficient context or information for the model to learn meaningful concepts. By filtering out such documents, we can ensure that the data used for training is sufficiently informative and balanced.\n",
    "\n",
    "Let's explore how to apply word counts and filtering using NeMo Curator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f59e1ac-4a33-4305-abe6-25d3e3ab596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "from nemo_curator import ScoreFilter\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.filters import (\n",
    "    DocumentFilter,\n",
    "    RepeatingTopNGramsFilter,\n",
    "    WordCountFilter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89f424f0-2db3-47b4-a919-666aea19d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncompleteDocumentFilter(DocumentFilter):\n",
    "    \"\"\"\n",
    "    If the document doesn't end with a terminating punctuation mark, then discard.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Accepted document terminators.\n",
    "        self._story_terminators = {\".\", \"!\", \"?\", '\"', \"”\"}\n",
    "\n",
    "    def score_document(self, text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Determines if a document's score is valid based on the last character of the text.\n",
    "        Args:\n",
    "            text (str): The document text.\n",
    "        Returns:\n",
    "            bool: True if the document's score is valid, False otherwise.\n",
    "        \"\"\"\n",
    "        return text.strip()[-1] in self._story_terminators\n",
    "\n",
    "    def keep_document(self, score) -> bool:\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d402e2b-9b38-4a0d-877b-a62a935f1c89",
   "metadata": {},
   "source": [
    "The following code defines a function, `filter_dataset`, that cleans a `DocumentDataset` by applying several filters:\n",
    "\n",
    "- **Word Count Filter**: Removes documents with fewer than 80 words by default.\n",
    "- **Incomplete Document Filter**: Removes incomplete documents.\n",
    "- **Repeating N-Grams Filters**: Removes documents with excessive repetition of word sequences (2-grams, 3-grams, 4-grams) above certain thresholds (20%, 18%, 16% respectively).\n",
    "\n",
    "These filters are applied sequentially to refine the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c610d1cb-f44f-42fc-bb37-8759a50b1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(dataset: DocumentDataset) -> DocumentDataset:\n",
    "    filters = Sequential(\n",
    "        [\n",
    "            ScoreFilter(\n",
    "                WordCountFilter(min_words=80),\n",
    "                text_field=\"text\",\n",
    "                score_field=\"word_count\",\n",
    "            ),\n",
    "            ScoreFilter(IncompleteDocumentFilter(), text_field=\"text\"),\n",
    "            ScoreFilter(\n",
    "                RepeatingTopNGramsFilter(n=2, max_repeating_ngram_ratio=0.2),\n",
    "                text_field=\"text\",\n",
    "            ),\n",
    "            ScoreFilter(\n",
    "                RepeatingTopNGramsFilter(n=3, max_repeating_ngram_ratio=0.18),\n",
    "                text_field=\"text\",\n",
    "            ),\n",
    "            ScoreFilter(\n",
    "                RepeatingTopNGramsFilter(n=4, max_repeating_ngram_ratio=0.16),\n",
    "                text_field=\"text\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return filters(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ee9ddd-57b3-4cac-8351-fb8b7fb5acef",
   "metadata": {},
   "source": [
    "Let's now apply that on our multilingual dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ef91e32-c9ea-4bf4-b1ab-016ee9413e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the pipeline...\n",
      "CPU times: user 109 ms, sys: 12.9 ms, total: 122 ms\n",
      "Wall time: 119 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "curation_steps = Sequential([clean_and_unify, filter_dataset])\n",
    "\n",
    "print(\"Executing the pipeline...\")\n",
    "filtered_dataset = curation_steps(multilingual_dataset).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e0c2dd-0e63-4105-aeee-a41b12f0fb06",
   "metadata": {},
   "source": [
    "We can check the outputs. Notice that a new field named `word_count` has been added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72e03f52-5757-41b9-9e09-05c3cb5e67ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Cours D'histoire Des États Européens: Depuis L...</td>\n",
       "      <td>2019-01-17 23:25:39</td>\n",
       "      <td>https://www.bookvoed.ru/book?id=1433688</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Copy-Paste ejecutado en el Windows Phone 7.[Ví...</td>\n",
       "      <td>2019-07-20 07:52:44</td>\n",
       "      <td>https://geeksroom.com/2010/12/copy-paste-ejecu...</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Agenda de eventos y actividades en Barcelona p...</td>\n",
       "      <td>2018-07-22 22:13:01</td>\n",
       "      <td>http://barcelona.carpediem.cd/events/?dt=06.04...</td>\n",
       "      <td>746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>file.json</td>\n",
       "      <td>EE.UU | EE.UU\\nenero 22, 2014 Juan Pedro Sánch...</td>\n",
       "      <td>2019-09-20 03:50:18</td>\n",
       "      <td>https://makeexperience.wordpress.com/tag/ee-uu/</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>file.json</td>\n",
       "      <td>jandrobell - Pesca Mediterraneo 2\\njandrobell\\...</td>\n",
       "      <td>2018-08-18 11:01:22</td>\n",
       "      <td>http://www.pescamediterraneo2.com/foros/profil...</td>\n",
       "      <td>1865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    file_name                                               text  \\\n",
       "1   file.json  Cours D'histoire Des États Européens: Depuis L...   \n",
       "6   file.json  Copy-Paste ejecutado en el Windows Phone 7.[Ví...   \n",
       "8   file.json  Agenda de eventos y actividades en Barcelona p...   \n",
       "11  file.json  EE.UU | EE.UU\\nenero 22, 2014 Juan Pedro Sánch...   \n",
       "13  file.json  jandrobell - Pesca Mediterraneo 2\\njandrobell\\...   \n",
       "\n",
       "             timestamp                                                url  \\\n",
       "1  2019-01-17 23:25:39            https://www.bookvoed.ru/book?id=1433688   \n",
       "6  2019-07-20 07:52:44  https://geeksroom.com/2010/12/copy-paste-ejecu...   \n",
       "8  2018-07-22 22:13:01  http://barcelona.carpediem.cd/events/?dt=06.04...   \n",
       "11 2019-09-20 03:50:18    https://makeexperience.wordpress.com/tag/ee-uu/   \n",
       "13 2018-08-18 11:01:22  http://www.pescamediterraneo2.com/foros/profil...   \n",
       "\n",
       "    word_count  \n",
       "1          111  \n",
       "6          137  \n",
       "8          746  \n",
       "11         323  \n",
       "13        1865  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737b41e2-79a7-4a5e-a823-1aa07d29926b",
   "metadata": {},
   "source": [
    "Let's save the output, we need to create the folder first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7840e33-7414-4819-99bc-60e3df51c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_path = os.path.join(curated_data_path, \"02_filter_dataset\")\n",
    "\n",
    "! mkdir -p {filtered_data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee80a5b3-8542-42a7-b818-3390653702f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to disk complete for 1 partition(s)\n"
     ]
    }
   ],
   "source": [
    "# save output to json\n",
    "filtered_dataset.to_json(filtered_data_path, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf4874-8107-41f0-8358-971011b17fe1",
   "metadata": {},
   "source": [
    "Check the saved file by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f89490af-abab-44ab-919e-9c2847f9384c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: jq: command not found\n"
     ]
    }
   ],
   "source": [
    "! head -n 1 {filtered_data_path}/file.jsonl | jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c80b85c-27b1-4d48-a8cb-dfe6c6273319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr_core_news_sm\n",
      "  Downloading fr_core_news_sm-3.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Downloading fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "\u001b[2K   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.3/16.3 MB 43.3 MB/s eta 0:00:00\n",
      "\u001b[?25hInstalling collected packages: fr_core_news_sm\n",
      "Successfully installed fr_core_news_sm-3.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install fr_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8dd4da-9f59-454c-a4c5-725d8cc2a1db",
   "metadata": {},
   "source": [
    "### 1.3 PII Identification and Removal\n",
    "\n",
    "The Personal Identifiable Information (PII) identification tool is designed to remove sensitive data from datasets.\n",
    "\n",
    "The identification leverages [presidio_analyzer](https://pypi.org/project/presidio-analyzer/) a Python based service for detecting PII entities in text.\n",
    "\n",
    "Let's try to analyze a toy sample: *My name is Dana and my number is 212-555-5555*\n",
    "\n",
    "Expected output is the type `PERSON` and `PHONE_NUMBER` and the char start and end position.\n",
    "\n",
    "```\n",
    " type: PERSON, start: 11, end: 15, score: 0.85,\n",
    " type: PHONE_NUMBER, start: 33, end: 45, score: 0.75\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2aaef24e-c838-408c-9242-f40cfc11146d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-md==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_md-3.7.0/de_core_news_md-3.7.0-py3-none-any.whl (44.4 MB)\n",
      "\u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.4/44.4 MB 66.2 MB/s eta 0:00:00\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from de-core-news-md==3.7.0) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (0.4.3)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (0.21.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (2.12.5)\n",
      "Requirement already satisfied: jinja2 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (2025.11.12)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (14.2.0)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (0.21.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (7.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (2.19.2)\n",
      "Requirement already satisfied: wrapt in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (2.0.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (0.1.2)\n",
      "Installing collected packages: de-core-news-md\n",
      "Successfully installed de-core-news-md-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_md')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "Collecting es-core-news-md==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.7.0/es_core_news_md-3.7.0-py3-none-any.whl (42.3 MB)\n",
      "\u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.3/42.3 MB 64.0 MB/s eta 0:00:00\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from es-core-news-md==3.7.0) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.4.3)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.21.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.12.5)\n",
      "Requirement already satisfied: jinja2 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2025.11.12)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (14.2.0)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.21.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (7.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.19.2)\n",
      "Requirement already satisfied: wrapt in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.0.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.1.2)\n",
      "Installing collected packages: es-core-news-md\n",
      "Successfully installed es-core-news-md-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_md')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "[type: PERSON, start: 11, end: 15, score: 0.85, type: PHONE_NUMBER, start: 33, end: 45, score: 0.75]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "\n",
    "# Hide deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Set up the engine, loads the NLP module (spaCy model by default) and other PII recognizers\n",
    "LANGUAGES_CONFIG_FILE = \"./languages-config.yml\"\n",
    "# Create NLP engine based on configuration file\n",
    "provider = NlpEngineProvider(conf_file=LANGUAGES_CONFIG_FILE)\n",
    "nlp_engine_with_spanish = provider.create_engine()\n",
    "\n",
    "analyzer = AnalyzerEngine(\n",
    "    supported_languages=[\"en\", \"es\", \"fr\"], nlp_engine=nlp_engine_with_spanish\n",
    ")\n",
    "\n",
    "results = analyzer.analyze(\n",
    "    text=\"My name is Dana and my number is 212-555-5555\",\n",
    "    entities=[\"PHONE_NUMBER\", \"PERSON\"],\n",
    "    language=\"en\",\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a163574-b7e2-4100-a054-ec53c0a2ff89",
   "metadata": {},
   "source": [
    "Run the analyzer for French sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "300f2672-9671-4fde-8ac7-270b4c81cbaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[type: EMAIL_ADDRESS, start: 14, end: 29, score: 1.0,\n",
       " type: URL, start: 18, end: 29, score: 0.5]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.analyze(text=\"Mon email est mon@example.com\", language=\"fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfef904-5323-411f-9b04-7fc8cbf95f81",
   "metadata": {},
   "source": [
    "Try your own examples in these three languages for accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "000f9d69-1563-48a7-b2ea-4299489c62ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[type: EMAIL_ADDRESS, start: 23, end: 43, score: 1.0,\n",
       " type: URL, start: 34, end: 43, score: 0.5]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = \"my email address is at tomaspries@gmail.com\"\n",
    "analyzer.analyze(text=input, language=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa84a63-5062-44bc-b948-f85f246d7e42",
   "metadata": {},
   "source": [
    "Nemo Curator integrates PII Identification and Removal efficiently leveraging Dask for parallelization. The tool currently supports the identification and removal of the following sensitive data types:\n",
    "\n",
    "`ADDRESS`,`CREDIT_CARD`,`EMAIL_ADDRESS`,`DATE_TIME`,`IP_ADDRESS`,`LOCATION`,`PERSON`,`URL`,`US_SSN`,`US_PASSPORT`,`US_DRIVER_LICENSE`,`PHONE_NUMBER`,\n",
    "\n",
    "Let;s run the Nemo Curator PII Identification `PiiModifier` on a toy sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eff2cc51-7177-4224-a637-5c167c6f7e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ryan went out to play football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>His email is ryan@example.com and phone is 212...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                     Ryan went out to play football\n",
       "1  His email is ryan@example.com and phone is 212..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create toy samples with PII data\n",
    "dataframe_toy = pd.DataFrame(\n",
    "    {\n",
    "        \"text\": [\n",
    "            \"Ryan went out to play football\",\n",
    "            \"His email is ryan@example.com and phone is 212-555-5555\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "dataset_toy = DocumentDataset(dask.dataframe.from_pandas(dataframe_toy, npartitions=1))\n",
    "\n",
    "dataset_toy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "de519ec7-ad64-44a1-b0ea-72ea09c5dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 13:20:17,271 - distributed.nanny - WARNING - Restarting worker\n",
      "2026-01-02 13:20:17,278 - distributed.nanny - WARNING - Restarting worker\n",
      "2026-01-02 13:20:17,324 - distributed.nanny - WARNING - Restarting worker\n",
      "2026-01-02 13:20:18,066 - distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d3f12d-ef3f-48a5-bd1a-1902de2d8c3d",
   "metadata": {},
   "source": [
    "Let's build and apply the `PiiModifier` on the toy sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e1b52e11-bfd9-4aa5-b721-cff7e67ff6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.modifiers.pii_modifier import PiiModifier\n",
    "\n",
    "modifier = PiiModifier(\n",
    "    batch_size=2000,\n",
    "    language=\"en\",\n",
    "    supported_entities=[\"PERSON\", \"EMAIL_ADDRESS\", \"PHONE_NUMBER\"],\n",
    "    anonymize_action=\"replace\",\n",
    "    device=\"cpu\" # ADD THIS LINE - Force CPU usage\n",
    ")\n",
    "\n",
    "modify = Modify(modifier)\n",
    "modified_dataset = modify(dataset_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "071a86f9-1e93-47e9-a1f8-e88f182bc386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 13:20:33,772 - distributed.worker - ERROR - Compute Failed\n",
      "Key:       ('modify_document-3e3a6604bba9b67bf50c243ae92a51ce', 0)\n",
      "State:     executing\n",
      "Task:  <Task ('modify_document-3e3a6604bba9b67bf50c243ae92a51ce', 0) apply_and_enforce(..., ...)>\n",
      "Exception: \"ValueError('Cannot use GPU, CuPy is not installed')\"\n",
      "Traceback: '  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/dask/dataframe/core.py\", line 98, in apply_and_enforce\\n    df = func(*args, **kwargs)\\n         ^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/nemo_curator/modifiers/pii_modifier.py\", line 79, in modify_document\\n    deidentifier = load_object_on_worker(\"deidentifier\", self.load_deidentifier, {})\\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/nemo_curator/utils/distributed_utils.py\", line 1103, in load_object_on_worker\\n    obj = load_object_function(**load_object_kwargs)\\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/nemo_curator/modifiers/pii_modifier.py\", line 95, in load_deidentifier\\n    spacy.require_gpu()\\n  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/thinc/util.py\", line 230, in require_gpu\\n    raise ValueError(\"Cannot use GPU, CuPy is not installed\")\\n'\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot use GPU, CuPy is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# check modified data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodified_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/nemo_curator/datasets/doc_dataset.py:51\u001b[39m, in \u001b[36mDocumentDataset.head\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhead\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m5\u001b[39m) -> Union[\u001b[33m\"\u001b[39m\u001b[33mcudf.DataFrame\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpd.DataFrame\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:696\u001b[39m, in \u001b[36mFrameBase.head\u001b[39m\u001b[34m(self, n, npartitions, compute)\u001b[39m\n\u001b[32m    694\u001b[39m out = new_collection(expr.Head(\u001b[38;5;28mself\u001b[39m, n=n, npartitions=npartitions))\n\u001b[32m    695\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compute:\n\u001b[32m--> \u001b[39m\u001b[32m696\u001b[39m     out = \u001b[43mout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/dask/base.py:378\u001b[39m, in \u001b[36mDaskMethodsMixin.compute\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m    355\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[32m    356\u001b[39m \n\u001b[32m    357\u001b[39m \u001b[33;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    376\u001b[39m \u001b[33;03m    dask.compute\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     (result,) = \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/dask/base.py:686\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    683\u001b[39m     expr = expr.optimize()\n\u001b[32m    684\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(flatten(expr.__dask_keys__()))\n\u001b[32m--> \u001b[39m\u001b[32m686\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/nemo_curator/modifiers/pii_modifier.py:79\u001b[39m, in \u001b[36mmodify_document\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     72\u001b[39m logging.basicConfig(\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mformat\u001b[39m=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     74\u001b[39m     level=logging.INFO,\n\u001b[32m     75\u001b[39m     datefmt=\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     76\u001b[39m )\n\u001b[32m     77\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m deidentifier = load_object_on_worker(\u001b[33m\"\u001b[39m\u001b[33mdeidentifier\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.load_deidentifier, {})\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     81\u001b[39m     output: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = deidentifier.deidentify_text_batch(text.tolist(), \u001b[38;5;28mself\u001b[39m.batch_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/nemo_curator/utils/distributed_utils.py:1103\u001b[39m, in \u001b[36mload_object_on_worker\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1101\u001b[39m     obj = \u001b[38;5;28mgetattr\u001b[39m(worker, attr)\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m     obj = load_object_function(**load_object_kwargs)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(worker, attr, obj)\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/nemo_curator/modifiers/pii_modifier.py:95\u001b[39m, in \u001b[36mload_deidentifier\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device == \u001b[33m\"\u001b[39m\u001b[33mgpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     spacy.require_gpu()\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnemo_curator\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpii\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01malgorithm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PiiDeidentifier\n\u001b[32m     98\u001b[39m deidentifier: PiiDeidentifier = PiiDeidentifier(\n\u001b[32m     99\u001b[39m     language=\u001b[38;5;28mself\u001b[39m.language,\n\u001b[32m    100\u001b[39m     supported_entities=\u001b[38;5;28mself\u001b[39m.supported_entities,\n\u001b[32m    101\u001b[39m     anonymize_action=\u001b[38;5;28mself\u001b[39m.anonymize_action,\n\u001b[32m    102\u001b[39m     **\u001b[38;5;28mself\u001b[39m.kwargs,\n\u001b[32m    103\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/thinc/util.py:230\u001b[39m, in \u001b[36mrequire_gpu\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot use GPU, PyTorch is not installed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m platform.system() != \u001b[33m\"\u001b[39m\u001b[33mDarwin\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_cupy:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot use GPU, CuPy is not installed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_gpu:\n\u001b[32m    232\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo GPU devices detected\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Cannot use GPU, CuPy is not installed"
     ]
    }
   ],
   "source": [
    "# check modified data\n",
    "modified_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1654a77-4330-4902-99bf-056be4920dcb",
   "metadata": {},
   "source": [
    "Now, let's integrate this PII identification step into our curation sequence and apply it to the multilingual dataset. This will ensure that sensitive data is properly detected and removed while maintaining data quality. \n",
    "\n",
    "Let's create first the `redact_pii` function for PII identification and removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7d0f90-6efa-4ea9-a9e4-c1849e0a637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.modifiers.pii_modifier import PiiModifier\n",
    "\n",
    "\n",
    "def redact_pii(dataset: DocumentDataset) -> DocumentDataset:\n",
    "    redactor = Modify(\n",
    "        PiiModifier(\n",
    "            supported_entities=[\"PERSON\"],\n",
    "            anonymize_action=\"replace\",\n",
    "            device=\"cpu\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return redactor(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6622c883-8fb5-4951-84e7-9f0a80a7ce29",
   "metadata": {},
   "source": [
    "Let's now run the sequence of curation steps including the PII removal function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0c780b-2632-4ff9-9ddd-7a889bf1f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "curation_steps = Sequential([clean_and_unify, filter_dataset, redact_pii])\n",
    "\n",
    "print(\"Executing the pipeline...\")\n",
    "redact_pii_dataset = curation_steps(multilingual_dataset).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ce773a-9520-44a9-bad4-9915d8a808d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the filtered data\n",
    "redact_pii_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e50d88-8a61-46c0-b42f-94a22a3ad4d3",
   "metadata": {},
   "source": [
    "Let's now save the fileted data. We need to create the folder to save the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b610ca1-5d53-4d00-aa78-60c8e019bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "redact_pii_data_path = os.path.join(curated_data_path, \"03_redact_pii_data_path\")\n",
    "\n",
    "! mkdir -p {redact_pii_data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8259bb6-64ac-46d7-9813-de0e06ec5005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "redact_pii_dataset.to_json(redact_pii_data_path, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb0bc4-b310-4bb8-8a3d-a2f3e8806296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the saved file\n",
    "! head -n 1 {redact_pii_data_path}/file.jsonl |jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f0d3c-4646-4a24-9e86-de0bc153111d",
   "metadata": {},
   "source": [
    "The current PII removal s Nemo Curator implementation is limited to HPC clusters using Slurm as the resource manager. Check the [documentation](https://github.com/NVIDIA/NeMo-Curator/blob/main/docs/user-guide/personalidentifiableinformationidentificationandremoval.rst) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a1f1a1-55e1-407b-a494-c17bb58c9258",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "In this notebook, you have used Nemo Curator to apply a sequence of basic text curation steps designed to clean and preprocess the dataset.\n",
    "\n",
    "Before moving on to the next notebook, make sure to stop the Dask cluster. Please run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b44b3b-5db7-4a3f-929a-c3311eb5e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)  # automatically restarts kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74536337-a5a3-4e54-9f9c-f7049b7f684e",
   "metadata": {},
   "source": [
    "\n",
    "We are now ready to move to the next notebook to explore advanced data preparation steps. \n",
    "\n",
    "Let's move to the [02_advanced_data_processing](02_advanced_data_processing.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b1e5e3-60b1-47d7-87e3-96c8d73e99e0",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
