{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ee8ff11-6c0b-455b-a574-bf7170a18def",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f1c75-fe21-4823-ace7-2ec4a93de023",
   "metadata": {},
   "source": [
    "# 2. Advanced Data Processing\n",
    "\n",
    "\n",
    "In this notebook, we will use NeMo Curator to perform several crutial data cleaning steps, such as language detection and filtering, topic classification, and deduplication. \n",
    "\n",
    "This notebook is structured as follows:\n",
    "- First, we will explore language detection and filtering to separate our multilingual dataset by language.\n",
    "- Next, we will dive into topic classification to categorize the datasets into relevant themes.\n",
    "- Finally, we will explore document deduplication, covering both exact and fuzzy methods.\n",
    "\n",
    "\n",
    "**[2.1 Language Separation](#2.1-Language-Separation)<br>**\n",
    "**[2.2 Domain Classification](#2.2-Domain-Classification)<br>**\n",
    "**[2.3 Documents Deduplication](#2.3-Deduplication)<br>**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bdf4c0-2133-441e-b12c-f28ad3aa3b37",
   "metadata": {},
   "source": [
    "***************\n",
    "### Environment Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70757a02-0847-4100-9dbd-10dbf90d2873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore any warning\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904cadc6-7228-45fd-b7d7-bf6c165f35d2",
   "metadata": {},
   "source": [
    "The next cell starts a Dask LocalCluster on your GPU cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067edcdf-34f3-476f-8309-a2726be88c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/cudf/utils/_ptxcompiler.py:64: UserWarning: Error getting driver and runtime versions:\n",
      "\n",
      "stdout:\n",
      "\n",
      "\n",
      "\n",
      "stderr:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 7, in <module>\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/numba_cuda/numba/cuda/cudadrv/runtime.py\", line 111, in get_version\n",
      "    self.cudaRuntimeGetVersion(ctypes.byref(rtver))\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/numba_cuda/numba/cuda/cudadrv/runtime.py\", line 65, in __getattr__\n",
      "    self._initialize()\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/numba_cuda/numba/cuda/cudadrv/runtime.py\", line 51, in _initialize\n",
      "    self.lib = open_cudalib('cudart')\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/numba_cuda/numba/cuda/cudadrv/libs.py\", line 84, in open_cudalib\n",
      "    return ctypes.CDLL(path)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/ctypes/__init__.py\", line 379, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: libcudart.so: cannot open shared object file: No such file or directory\n",
      "\n",
      "\n",
      "Not patching Numba\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuDF Spilling is enabled\n",
      "Number of dask worker:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:40403': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 17:16:00,247 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 6 memory: 3729 MB fds: 157>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/tornado/ioloop.py\", line 945, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/system_monitor.py\", line 210, in update\n",
      "    gpu_metrics = nvml.real_time()\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 370, in real_time\n",
      "    \"utilization\": _get_utilization(h),\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 339, in _get_utilization\n",
      "    return pynvml.nvmlDeviceGetUtilizationRates(h).gpu\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 3603, in nvmlDeviceGetUtilizationRates\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 1059, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_Unknown: Unknown Error\n",
      "2026-01-02 17:16:11,045 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 6 memory: 3729 MB fds: 158>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/tornado/ioloop.py\", line 945, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/system_monitor.py\", line 210, in update\n",
      "    gpu_metrics = nvml.real_time()\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 370, in real_time\n",
      "    \"utilization\": _get_utilization(h),\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 339, in _get_utilization\n",
      "    return pynvml.nvmlDeviceGetUtilizationRates(h).gpu\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 3603, in nvmlDeviceGetUtilizationRates\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 1059, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_Unknown: Unknown Error\n",
      "2026-01-02 17:16:32,247 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 6 memory: 3729 MB fds: 158>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/tornado/ioloop.py\", line 945, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/system_monitor.py\", line 210, in update\n",
      "    gpu_metrics = nvml.real_time()\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 370, in real_time\n",
      "    \"utilization\": _get_utilization(h),\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 339, in _get_utilization\n",
      "    return pynvml.nvmlDeviceGetUtilizationRates(h).gpu\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 3603, in nvmlDeviceGetUtilizationRates\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 1059, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_Unknown: Unknown Error\n",
      "2026-01-02 17:17:04,020 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 4 memory: 3729 MB fds: 158>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/tornado/ioloop.py\", line 945, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/system_monitor.py\", line 210, in update\n",
      "    gpu_metrics = nvml.real_time()\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 370, in real_time\n",
      "    \"utilization\": _get_utilization(h),\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 339, in _get_utilization\n",
      "    return pynvml.nvmlDeviceGetUtilizationRates(h).gpu\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 3603, in nvmlDeviceGetUtilizationRates\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 1059, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_Unknown: Unknown Error\n",
      "2026-01-02 17:17:14,748 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 6 memory: 3729 MB fds: 158>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/tornado/ioloop.py\", line 945, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/system_monitor.py\", line 210, in update\n",
      "    gpu_metrics = nvml.real_time()\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 370, in real_time\n",
      "    \"utilization\": _get_utilization(h),\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 339, in _get_utilization\n",
      "    return pynvml.nvmlDeviceGetUtilizationRates(h).gpu\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 3603, in nvmlDeviceGetUtilizationRates\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 1059, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_Unknown: Unknown Error\n",
      "2026-01-02 17:17:15,021 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 6 memory: 3729 MB fds: 158>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/tornado/ioloop.py\", line 945, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/system_monitor.py\", line 210, in update\n",
      "    gpu_metrics = nvml.real_time()\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 370, in real_time\n",
      "    \"utilization\": _get_utilization(h),\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 339, in _get_utilization\n",
      "    return pynvml.nvmlDeviceGetUtilizationRates(h).gpu\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 3603, in nvmlDeviceGetUtilizationRates\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 1059, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_Unknown: Unknown Error\n",
      "2026-01-02 17:17:25,747 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 6 memory: 3729 MB fds: 158>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/tornado/ioloop.py\", line 945, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/system_monitor.py\", line 210, in update\n",
      "    gpu_metrics = nvml.real_time()\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 370, in real_time\n",
      "    \"utilization\": _get_utilization(h),\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 339, in _get_utilization\n",
      "    return pynvml.nvmlDeviceGetUtilizationRates(h).gpu\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 3603, in nvmlDeviceGetUtilizationRates\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 1059, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_Unknown: Unknown Error\n",
      "2026-01-02 17:17:26,020 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 4 memory: 3729 MB fds: 158>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/tornado/ioloop.py\", line 945, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/system_monitor.py\", line 210, in update\n",
      "    gpu_metrics = nvml.real_time()\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 370, in real_time\n",
      "    \"utilization\": _get_utilization(h),\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 339, in _get_utilization\n",
      "    return pynvml.nvmlDeviceGetUtilizationRates(h).gpu\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 3603, in nvmlDeviceGetUtilizationRates\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 1059, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_Unknown: Unknown Error\n",
      "2026-01-02 17:17:36,748 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 6 memory: 3729 MB fds: 158>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/tornado/ioloop.py\", line 945, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/system_monitor.py\", line 210, in update\n",
      "    gpu_metrics = nvml.real_time()\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 370, in real_time\n",
      "    \"utilization\": _get_utilization(h),\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 339, in _get_utilization\n",
      "    return pynvml.nvmlDeviceGetUtilizationRates(h).gpu\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 3603, in nvmlDeviceGetUtilizationRates\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 1059, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_Unknown: Unknown Error\n",
      "2026-01-02 17:17:37,021 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 4 memory: 3729 MB fds: 158>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/tornado/ioloop.py\", line 945, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/system_monitor.py\", line 210, in update\n",
      "    gpu_metrics = nvml.real_time()\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 370, in real_time\n",
      "    \"utilization\": _get_utilization(h),\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 339, in _get_utilization\n",
      "    return pynvml.nvmlDeviceGetUtilizationRates(h).gpu\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 3603, in nvmlDeviceGetUtilizationRates\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 1059, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_Unknown: Unknown Error\n",
      "2026-01-02 17:17:47,521 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 6 memory: 3729 MB fds: 158>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/tornado/ioloop.py\", line 945, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/system_monitor.py\", line 210, in update\n",
      "    gpu_metrics = nvml.real_time()\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 370, in real_time\n",
      "    \"utilization\": _get_utilization(h),\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 339, in _get_utilization\n",
      "    return pynvml.nvmlDeviceGetUtilizationRates(h).gpu\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 3603, in nvmlDeviceGetUtilizationRates\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 1059, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_Unknown: Unknown Error\n",
      "2026-01-02 17:17:58,247 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 4 memory: 3729 MB fds: 158>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/tornado/ioloop.py\", line 945, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/system_monitor.py\", line 210, in update\n",
      "    gpu_metrics = nvml.real_time()\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 370, in real_time\n",
      "    \"utilization\": _get_utilization(h),\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 339, in _get_utilization\n",
      "    return pynvml.nvmlDeviceGetUtilizationRates(h).gpu\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 3603, in nvmlDeviceGetUtilizationRates\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 1059, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_Unknown: Unknown Error\n",
      "2026-01-02 17:18:30,521 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 4 memory: 3729 MB fds: 158>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/tornado/ioloop.py\", line 945, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/system_monitor.py\", line 210, in update\n",
      "    gpu_metrics = nvml.real_time()\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 370, in real_time\n",
      "    \"utilization\": _get_utilization(h),\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 339, in _get_utilization\n",
      "    return pynvml.nvmlDeviceGetUtilizationRates(h).gpu\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 3603, in nvmlDeviceGetUtilizationRates\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 1059, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_Unknown: Unknown Error\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.utils.distributed_utils import get_client, get_num_workers\n",
    "\n",
    "\n",
    "def pre_imports():\n",
    "    import cudf\n",
    "\n",
    "\n",
    "client = get_client(cluster_type=\"gpu\", set_torch_to_use_rmm=False)\n",
    "\n",
    "print(f\"Number of dask worker:{get_num_workers(client)}\")\n",
    "client.run(pre_imports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10d38a4-c4bf-4819-a3e4-e85dc96e0884",
   "metadata": {},
   "source": [
    "Let's load the multilingual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b611fce2-537e-4a89-8fb5-c6624b774528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 files with blocksize='1gb' / files_per_partition=None\n"
     ]
    }
   ],
   "source": [
    "from nemo_curator.datasets import DocumentDataset\n",
    "\n",
    "multilingual_data_path = \"./original_data\"\n",
    "multilingual_dataset = DocumentDataset.read_json(\n",
    "    multilingual_data_path, add_filename=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "518de0e9-9385-4acb-a497-4e5f16c97813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Dragon Ball: Le 20e film de la sage sortira le...</td>\n",
       "      <td>2019-01-21 03:52:10</td>\n",
       "      <td>https://cultinfos.com/buzz/332814-dragon-ball-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Cours D'histoire Des États Européens: Depuis L...</td>\n",
       "      <td>2019-01-17 23:25:39</td>\n",
       "      <td>https://www.bookvoed.ru/book?id=1433688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Se realizó una jornada de promoción del buentr...</td>\n",
       "      <td>2018-04-21 07:38:28</td>\n",
       "      <td>http://www.desarrollosocial.gob.ar/noticias/se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Restaurantes con Web Y Telefono Y Dias Y Horar...</td>\n",
       "      <td>2020-08-11 16:33:05</td>\n",
       "      <td>http://mendoza.guia.clarin.com/restaurantes-co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Responsable qualité - Intérim : Emploi et recr...</td>\n",
       "      <td>2020-08-07 01:17:37</td>\n",
       "      <td>https://images3.meteojob.com/Emploi-Interim-Re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name                                               text  \\\n",
       "0  file.json  Dragon Ball: Le 20e film de la sage sortira le...   \n",
       "1  file.json  Cours D'histoire Des États Européens: Depuis L...   \n",
       "2  file.json  Se realizó una jornada de promoción del buentr...   \n",
       "3  file.json  Restaurantes con Web Y Telefono Y Dias Y Horar...   \n",
       "4  file.json  Responsable qualité - Intérim : Emploi et recr...   \n",
       "\n",
       "            timestamp                                                url  \n",
       "0 2019-01-21 03:52:10  https://cultinfos.com/buzz/332814-dragon-ball-...  \n",
       "1 2019-01-17 23:25:39            https://www.bookvoed.ru/book?id=1433688  \n",
       "2 2018-04-21 07:38:28  http://www.desarrollosocial.gob.ar/noticias/se...  \n",
       "3 2020-08-11 16:33:05  http://mendoza.guia.clarin.com/restaurantes-co...  \n",
       "4 2020-08-07 01:17:37  https://images3.meteojob.com/Emploi-Interim-Re...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the data\n",
    "multilingual_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37184b1-73f6-48b4-a41a-e88b662e332e",
   "metadata": {},
   "source": [
    "## 2.1 Language Separation\n",
    "\n",
    "In this section, we will use a language classification model by [fasttext](https://fasttext.cc/docs/en/language-identification.html). \n",
    "\n",
    "\n",
    "Let's first create the output folders and download the fasttext model for text language detection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b982ba8-0d1e-424c-9d38-446eef55d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "language_base_output_path = \"./curated/04_language_separation\"\n",
    "language_separated_output_path = os.path.join(language_base_output_path, \"language\")\n",
    "\n",
    "# Create directories (with parents as needed)\n",
    "os.makedirs(language_base_output_path, exist_ok=True)\n",
    "os.makedirs(language_separated_output_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79b5f42b-752e-492c-908e-38738a970f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./curated/04_language_separation/language'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_separated_output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28fdc0b-32b1-4ee4-bfb2-239cbb458dff",
   "metadata": {},
   "source": [
    "Let's create the filter which uses the downloaded fasttext model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5599b5-9e9b-4d5a-b862-932a4f5bd3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download fasttext language classification model(this needs to be done hidden in the env)\n",
    "# !wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin -P {language_separated_output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f093c3cc-c109-473d-bc6a-70070f568edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import ScoreFilter\n",
    "from nemo_curator.filters import FastTextLangId\n",
    "\n",
    "lang_filter = FastTextLangId(\"lid.176.bin\")\n",
    "language_field = \"language\"\n",
    "language_id_pipeline = ScoreFilter(\n",
    "    lang_filter, score_field=language_field, score_type=\"object\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1623a093-35aa-47f0-9fb0-a4326ce57cd2",
   "metadata": {},
   "source": [
    "Now, let's apply the language detection filter on our multilingual dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a7f41b5-b5c5-4077-8898-5d989238cc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply language separation to our multilingual dataset\n",
    "filtered_dataset = language_id_pipeline(multilingual_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541db02c-5f57-4ef1-9414-fc7a8473ee7f",
   "metadata": {},
   "source": [
    "Let's check the detected language for each sample. \n",
    "\n",
    "Notice the new fields `language` in the output with the language code `FR/EN/ES`and the classification score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4b39506-eb40-44aa-a337-a135042c401c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Dragon Ball: Le 20e film de la sage sortira le...</td>\n",
       "      <td>2019-01-21 03:52:10</td>\n",
       "      <td>https://cultinfos.com/buzz/332814-dragon-ball-...</td>\n",
       "      <td>[0.9175292253494263, FR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Cours D'histoire Des États Européens: Depuis L...</td>\n",
       "      <td>2019-01-17 23:25:39</td>\n",
       "      <td>https://www.bookvoed.ru/book?id=1433688</td>\n",
       "      <td>[0.5166642069816589, FR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file.json</td>\n",
       "      <td>Se realizó una jornada de promoción del buentr...</td>\n",
       "      <td>2018-04-21 07:38:28</td>\n",
       "      <td>http://www.desarrollosocial.gob.ar/noticias/se...</td>\n",
       "      <td>[0.9740189909934998, ES]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name                                               text  \\\n",
       "0  file.json  Dragon Ball: Le 20e film de la sage sortira le...   \n",
       "1  file.json  Cours D'histoire Des États Européens: Depuis L...   \n",
       "2  file.json  Se realizó una jornada de promoción del buentr...   \n",
       "\n",
       "            timestamp                                                url  \\\n",
       "0 2019-01-21 03:52:10  https://cultinfos.com/buzz/332814-dragon-ball-...   \n",
       "1 2019-01-17 23:25:39            https://www.bookvoed.ru/book?id=1433688   \n",
       "2 2018-04-21 07:38:28  http://www.desarrollosocial.gob.ar/noticias/se...   \n",
       "\n",
       "                   language  \n",
       "0  [0.9175292253494263, FR]  \n",
       "1  [0.5166642069816589, FR]  \n",
       "2  [0.9740189909934998, ES]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the detected language per item\n",
    "filtered_dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07606f48-a5d3-45f1-a6e4-19e9792b2977",
   "metadata": {},
   "source": [
    "Let's separate documents by the language label and save each language separately. This will create sub-folders for each languages under the output path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62a96f78-6563-47d9-84c9-1c53d50ad70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save separated languages and get stats\n",
    "from nemo_curator.utils.file_utils import separate_by_metadata\n",
    "\n",
    "filtered_dataset.df[language_field] = filtered_dataset.df[language_field].apply(\n",
    "    lambda score: score[1], meta=(language_field, \"object\")\n",
    ")\n",
    "language_stats = separate_by_metadata(\n",
    "    filtered_dataset.df, language_separated_output_path, metadata_field=language_field\n",
    ").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7abd47e-990d-4e8c-a828-d0dee12920cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of document:400\n",
      "Number of filtered document:396\n",
      "Language separation stats and   {'FR': 194, 'ES': 194, 'EN': 8}\n"
     ]
    }
   ],
   "source": [
    "# check the language distribution stats\n",
    "print(f\"Number of document:{len(multilingual_dataset)}\")\n",
    "print(f\"Number of filtered document:{len(filtered_dataset)}\")\n",
    "\n",
    "print(\"Language separation stats and  \", language_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220d8423-d974-4b25-8778-8231b4ccf8e8",
   "metadata": {},
   "source": [
    "We can check the output jsonl file per language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e521686-a780-4218-a32c-272fc1c3b0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: jq: command not found\n"
     ]
    }
   ],
   "source": [
    "# check first element for French\n",
    "! head -n 1 {language_separated_output_path}/FR/file.jsonl |jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa068ada-9e72-4d4d-91ee-8fb7cc44d8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: jq: command not found\n"
     ]
    }
   ],
   "source": [
    "# check first element for spanish\n",
    "! head -n 1 {language_separated_output_path}/ES/file.jsonl |jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6ca859-98b8-4de9-aa6e-0fa1808feb06",
   "metadata": {},
   "source": [
    "## 2.2 Domain Classification\n",
    "\n",
    "Nemo Curator supports various text classification models allowing data annotation, useful for cleaning and data blending. Check the documentation for [distributed data classification](https://github.com/NVIDIA/NeMo-Curator/blob/main/tutorials/distributed_data_classification/README.md).\n",
    "\n",
    "\n",
    "Each classifier is available on Hugging Face Hub. When run with NeMo Curator, they are accelerated using RAPIDS [CrossFit](https://github.com/rapidsai/crossfit) library.\n",
    "\n",
    "\n",
    "In this section, we will experiment with the `MultilingualDomainClassifier` a Multilingual Domain Classifier that support 52 languages and annotate 26 domain classes:\n",
    "\n",
    "`Arts_and_Entertainment`, `Autos_and_Vehicles`, `Adult`,`Beauty_and_Fitness`, `Books_and_Literature`, `Business_and_Industrial`, `Computers_and_Electronics`, `Finance`, `Food_and_Drink`, `Games`, `Health`, `Hobbies_and_Leisure`, `Home_and_Garden`, `Internet_and_Telecom`, `Jobs_and_Education`, `Law_and_Government`, `News`, `Online_Communities`, `People_and_Society`, `Pets_and_Animals`, `Real_Estate`, `Science`, `Sensitive_Subjects`, `Shopping`, `Sports`, `Travel_and_Transportation`\n",
    "\n",
    "The model architecture is a transformer-based encoder Deberta V3 Base available on Hugging Face Hub. Learn more about the classifier [MultilingualDomainClassifier Model's Card](https://huggingface.co/nvidia/multilingual-domain-classifier).\n",
    "\n",
    "\n",
    "Let's set the output folder for domain classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d18d423c-9f35-4357-927d-6fab48b62547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import dask_cudf\n",
    "from nemo_curator.classifiers import MultilingualDomainClassifier\n",
    "\n",
    "domain_output_path = \"./curated/05_domain_classification\"\n",
    "\n",
    "# Create directory (with parents if needed)\n",
    "os.makedirs(domain_output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fd61cd-8451-43e2-a4a3-612bc040d70e",
   "metadata": {},
   "source": [
    "First, let's apply the Multilingual Domain Classifier on a toy multilingual dataset. Let's create the dataset with multiple languages and topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4639764f-6854-4614-a7a4-95451875622b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error at: /tmp/pip-build-env-i5c_dpkz/normal/lib/python3.12/site-packages/librmm/include/rmm/cuda_stream_view.hpp:106: cudaErrorMemoryAllocation out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create sample DataFrame\u001b[39;00m\n\u001b[32m      2\u001b[39m text = [\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# French\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIl adore les chats.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m.تقدم التطورات الحديثة في العلاج الجيني أملاً جديدًا لعلاج الاضطرابات الوراثية\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m df = \u001b[43mcudf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m toy_dataset = DocumentDataset(dask_cudf.from_cudf(df, npartitions=\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/cudf/utils/performance_tracking.py:51\u001b[39m, in \u001b[36m_performance_tracking.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nvtx.enabled():\n\u001b[32m     44\u001b[39m     stack.enter_context(\n\u001b[32m     45\u001b[39m         nvtx.annotate(\n\u001b[32m     46\u001b[39m             message=func.\u001b[34m__qualname__\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m         )\n\u001b[32m     50\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/cudf/core/dataframe.py:709\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy, nan_as_null)\u001b[39m\n\u001b[32m    707\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcopy is not currently implemented.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m709\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m({}, index=\u001b[43mcudf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nan_as_null \u001b[38;5;129;01mis\u001b[39;00m no_default:\n\u001b[32m    711\u001b[39m     nan_as_null = \u001b[38;5;129;01mnot\u001b[39;00m cudf.get_option(\u001b[33m\"\u001b[39m\u001b[33mmode.pandas_compatible\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/cudf/core/index.py:94\u001b[39m, in \u001b[36mIndexMeta.__call__\u001b[39m\u001b[34m(cls, data, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m     90\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtupleize_cols is currently not supported.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     91\u001b[39m     )\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m Index:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mas_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43marbitrary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(data, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/cudf/utils/performance_tracking.py:51\u001b[39m, in \u001b[36m_performance_tracking.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nvtx.enabled():\n\u001b[32m     44\u001b[39m     stack.enter_context(\n\u001b[32m     45\u001b[39m         nvtx.annotate(\n\u001b[32m     46\u001b[39m             message=func.\u001b[34m__qualname__\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m         )\n\u001b[32m     50\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/cudf/core/index.py:3832\u001b[39m, in \u001b[36mas_index\u001b[39m\u001b[34m(arbitrary, nan_as_null, copy, name, dtype)\u001b[39m\n\u001b[32m   3829\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mIndex data must be 1-dimensional and list-like\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3830\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3831\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Index._from_column(\n\u001b[32m-> \u001b[39m\u001b[32m3832\u001b[39m         \u001b[43mcolumn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43marbitrary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   3833\u001b[39m         name=name,\n\u001b[32m   3834\u001b[39m     )\n\u001b[32m   3835\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3836\u001b[39m     idx = idx.astype(dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/cudf/core/column/column.py:3005\u001b[39m, in \u001b[36mas_column\u001b[39m\u001b[34m(arbitrary, nan_as_null, dtype, length)\u001b[39m\n\u001b[32m   2997\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2998\u001b[39m         cudf.get_option(\u001b[33m\"\u001b[39m\u001b[33mdefault_integer_bitwidth\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2999\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m arbitrary.dtype.kind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33miu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   (...)\u001b[39m\u001b[32m   3002\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m arbitrary.dtype.kind == \u001b[33m\"\u001b[39m\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3003\u001b[39m     ):\n\u001b[32m   3004\u001b[39m         dtype = _maybe_convert_to_default_type(arbitrary.dtype)\n\u001b[32m-> \u001b[39m\u001b[32m3005\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mas_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43marbitrary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/cudf/core/column/column.py:2594\u001b[39m, in \u001b[36mas_column\u001b[39m\u001b[34m(arbitrary, nan_as_null, dtype, length)\u001b[39m\n\u001b[32m   2591\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m pa.types.is_null(arbitrary.type):\n\u001b[32m   2592\u001b[39m     \u001b[38;5;66;03m# default \"empty\" type\u001b[39;00m\n\u001b[32m   2593\u001b[39m     dtype = CUDF_STRING_DTYPE\n\u001b[32m-> \u001b[39m\u001b[32m2594\u001b[39m col = \u001b[43mColumnBase\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_arrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43marbitrary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2596\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2597\u001b[39m     col = col.astype(dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/cudf/core/column/column.py:829\u001b[39m, in \u001b[36mColumnBase.from_arrow\u001b[39m\u001b[34m(cls, array)\u001b[39m\n\u001b[32m    818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cudf.core.column.CategoricalColumn(\n\u001b[32m    819\u001b[39m         data=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    820\u001b[39m         size=codes.size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    825\u001b[39m         children=(codes,),\n\u001b[32m    826\u001b[39m     )\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    828\u001b[39m     result = \u001b[38;5;28mcls\u001b[39m.from_pylibcudf(\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m         \u001b[43mplc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_arrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m.columns()[\u001b[32m0\u001b[39m]\n\u001b[32m    830\u001b[39m     )\n\u001b[32m    831\u001b[39m     \u001b[38;5;66;03m# TODO: cudf_dtype_from_pa_type may be less necessary for some types\u001b[39;00m\n\u001b[32m    832\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result._with_type_metadata(\n\u001b[32m    833\u001b[39m         cudf_dtype_from_pa_type(array.type)\n\u001b[32m    834\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/functools.py:909\u001b[39m, in \u001b[36msingledispatch.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires at least \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    907\u001b[39m                     \u001b[33m'\u001b[39m\u001b[33m1 positional argument\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m909\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32minterop.pyx:152\u001b[39m, in \u001b[36mpylibcudf.interop._from_arrow_table\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mtable.pyx:59\u001b[39m, in \u001b[36mpylibcudf.table.Table.from_libcudf\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcolumn.pyx:149\u001b[39m, in \u001b[36mpylibcudf.column.Column.from_libcudf\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mdevice_buffer.pyx:206\u001b[39m, in \u001b[36mrmm.pylibrmm.device_buffer.DeviceBuffer.c_from_unique_ptr\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mstream.pyx:71\u001b[39m, in \u001b[36mrmm.pylibrmm.stream.Stream.c_synchronize\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error at: /tmp/pip-build-env-i5c_dpkz/normal/lib/python3.12/site-packages/librmm/include/rmm/cuda_stream_view.hpp:106: cudaErrorMemoryAllocation out of memory"
     ]
    }
   ],
   "source": [
    "# Create sample DataFrame\n",
    "text = [\n",
    "    # French\n",
    "    \"Il adore les chats.\",\n",
    "    # English\n",
    "    \"Investing in index funds is a popular strategy for long-term financial growth.\",\n",
    "    # Spanish\n",
    "    \"Ir de compras en el centro comercial es una excelente manera de encontrar ofertas y descubrir nuevas tiendas.\",\n",
    "    # Polish\n",
    "    \"Dzięki wykorzystaniu analizy danych programy treningowe dla sportowców stały się bardziej wyrafinowane.\",\n",
    "    # Arabic\n",
    "    \".تقدم التطورات الحديثة في العلاج الجيني أملاً جديدًا لعلاج الاضطرابات الوراثية\",\n",
    "]\n",
    "df = cudf.DataFrame({\"text\": text})\n",
    "\n",
    "toy_dataset = DocumentDataset(dask_cudf.from_cudf(df, npartitions=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d07561f-9b22-4af9-813b-040d548ef251",
   "metadata": {},
   "source": [
    "We can define the `MultilingualDomainClassifier` filter as follows. \n",
    "\n",
    "On its first run, it will download the DeBERTa model from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eae8db38-7b1f-49ce-aea6-92dcdd1d51d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# create the classifier\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m domain_classifier = \u001b[43mMultilingualDomainClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/nemo_curator/classifiers/domain.py:226\u001b[39m, in \u001b[36mMultilingualDomainClassifier.__init__\u001b[39m\u001b[34m(self, filter_by, batch_size, text_field, pred_column, prob_column, max_chars, device_type, autocast, max_mem_gb)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(  \u001b[38;5;66;03m# noqa: PLR0913\u001b[39;00m\n\u001b[32m    215\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    216\u001b[39m     filter_by: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    224\u001b[39m     max_mem_gb: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    225\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmultilingual\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilter_by\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilter_by\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_field\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpred_column\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpred_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprob_column\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprob_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_chars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_chars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m        \u001b[49m\u001b[43mautocast\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautocast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_mem_gb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_mem_gb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/nemo_curator/classifiers/domain.py:111\u001b[39m, in \u001b[36m_DomainClassifier.__init__\u001b[39m\u001b[34m(self, multilingual, filter_by, batch_size, text_field, pred_column, prob_column, max_chars, device_type, autocast, max_mem_gb)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28mself\u001b[39m.labels.sort(key=\u001b[38;5;28;01mlambda\u001b[39;00m x: config.label2id[x])\n\u001b[32m    109\u001b[39m \u001b[38;5;28mself\u001b[39m.out_dim = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.labels)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m model = \u001b[43mDomainModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautocast\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautocast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_mem_gb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_mem_gb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    114\u001b[39m     model=model,\n\u001b[32m    115\u001b[39m     labels=\u001b[38;5;28mself\u001b[39m.labels,\n\u001b[32m   (...)\u001b[39m\u001b[32m    122\u001b[39m     autocast=autocast,\n\u001b[32m    123\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/nemo_curator/classifiers/domain.py:56\u001b[39m, in \u001b[36mDomainModel.__init__\u001b[39m\u001b[34m(self, config, autocast, max_mem_gb)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_mem_gb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     54\u001b[39m     max_mem_gb = _get_suggest_memory_for_classifier()\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_mem_gb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_mem_gb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/crossfit/backend/torch/hf/model.py:56\u001b[39m, in \u001b[36mHFModel.__init__\u001b[39m\u001b[34m(self, path_or_name, max_mem_gb, model_output_type, training, start_batch_size, end_batch_size, batch_size_increment, start_seq_len, seq_len_increment)\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28mself\u001b[39m.mem = joblib.load(mem_model_path)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m training \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     57\u001b[39m     end_seq_len = \u001b[38;5;28mself\u001b[39m.max_seq_length()\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/nemo_curator/classifiers/domain.py:61\u001b[39m, in \u001b[36mDomainModel.load_model\u001b[39m\u001b[34m(self, device)\u001b[39m\n\u001b[32m     59\u001b[39m model = HFDeberta.from_pretrained(\u001b[38;5;28mself\u001b[39m.config.identifier)\n\u001b[32m     60\u001b[39m model.set_autocast(\u001b[38;5;28mself\u001b[39m.autocast)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m model = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/torch/nn/modules/module.py:1371\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1369\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/torch/nn/modules/module.py:957\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    958\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/torch/nn/modules/module.py:1357\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1351\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1352\u001b[39m             device,\n\u001b[32m   1353\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1354\u001b[39m             non_blocking,\n\u001b[32m   1355\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1356\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# create the classifier\n",
    "domain_classifier = MultilingualDomainClassifier(batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d9b726-51e1-4da0-89ad-075b47353aca",
   "metadata": {},
   "source": [
    "Now, let's run the filter on our multilingual multi topics toy samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebac187d-02c7-4f04-8c1e-6edb5b7a85d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 μs, sys: 0 ns, total: 7 μs\n",
      "Wall time: 10.7 μs\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'domain_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mresult_domain = domain_classifier(dataset=toy_dataset)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2572\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2571\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2572\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2576\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/IPython/core/magics/execution.py:1447\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1445\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupt_occured:\n\u001b[32m   1446\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exit_on_interrupt \u001b[38;5;129;01mand\u001b[39;00m captured_exception:\n\u001b[32m-> \u001b[39m\u001b[32m1447\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m captured_exception\n\u001b[32m   1448\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/IPython/core/magics/execution.py:1411\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1409\u001b[39m st = clock2()\n\u001b[32m   1410\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1411\u001b[39m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1412\u001b[39m     out = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1413\u001b[39m     \u001b[38;5;66;03m# multi-line %%time case\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:1\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'domain_classifier' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 17:17:03,995 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 4 memory: 1506 MB fds: 31>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/tornado/ioloop.py\", line 945, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/system_monitor.py\", line 210, in update\n",
      "    gpu_metrics = nvml.real_time()\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 370, in real_time\n",
      "    \"utilization\": _get_utilization(h),\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 339, in _get_utilization\n",
      "    return pynvml.nvmlDeviceGetUtilizationRates(h).gpu\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 3603, in nvmlDeviceGetUtilizationRates\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 1059, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_Unknown: Unknown Error\n",
      "2026-01-02 17:17:14,997 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 4 memory: 1506 MB fds: 31>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/tornado/ioloop.py\", line 945, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/system_monitor.py\", line 210, in update\n",
      "    gpu_metrics = nvml.real_time()\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 370, in real_time\n",
      "    \"utilization\": _get_utilization(h),\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 339, in _get_utilization\n",
      "    return pynvml.nvmlDeviceGetUtilizationRates(h).gpu\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 3603, in nvmlDeviceGetUtilizationRates\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 1059, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_Unknown: Unknown Error\n",
      "2026-01-02 17:17:25,993 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 4 memory: 1506 MB fds: 31>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/tornado/ioloop.py\", line 945, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/system_monitor.py\", line 210, in update\n",
      "    gpu_metrics = nvml.real_time()\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 370, in real_time\n",
      "    \"utilization\": _get_utilization(h),\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 339, in _get_utilization\n",
      "    return pynvml.nvmlDeviceGetUtilizationRates(h).gpu\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 3603, in nvmlDeviceGetUtilizationRates\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 1059, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_Unknown: Unknown Error\n",
      "2026-01-02 17:17:36,996 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 4 memory: 1506 MB fds: 31>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/tornado/ioloop.py\", line 945, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/system_monitor.py\", line 210, in update\n",
      "    gpu_metrics = nvml.real_time()\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 370, in real_time\n",
      "    \"utilization\": _get_utilization(h),\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 339, in _get_utilization\n",
      "    return pynvml.nvmlDeviceGetUtilizationRates(h).gpu\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 3603, in nvmlDeviceGetUtilizationRates\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 1059, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_Unknown: Unknown Error\n",
      "2026-01-02 17:17:47,494 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 6 memory: 1506 MB fds: 31>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/tornado/ioloop.py\", line 945, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/system_monitor.py\", line 210, in update\n",
      "    gpu_metrics = nvml.real_time()\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 370, in real_time\n",
      "    \"utilization\": _get_utilization(h),\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 339, in _get_utilization\n",
      "    return pynvml.nvmlDeviceGetUtilizationRates(h).gpu\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 3603, in nvmlDeviceGetUtilizationRates\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 1059, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_Unknown: Unknown Error\n",
      "2026-01-02 17:18:30,494 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 2 memory: 1506 MB fds: 31>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/tornado/ioloop.py\", line 945, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/system_monitor.py\", line 210, in update\n",
      "    gpu_metrics = nvml.real_time()\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 370, in real_time\n",
      "    \"utilization\": _get_utilization(h),\n",
      "                   ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/distributed/diagnostics/nvml.py\", line 339, in _get_utilization\n",
      "    return pynvml.nvmlDeviceGetUtilizationRates(h).gpu\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 3603, in nvmlDeviceGetUtilizationRates\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"/home/aibeceles/lession_01_wsl/nemo_env_clean/lib/python3.12/site-packages/pynvml.py\", line 1059, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_Unknown: Unknown Error\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_domain = domain_classifier(dataset=toy_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a65249-94f4-4783-826a-50438521cecb",
   "metadata": {},
   "source": [
    "Check the outputs. Notice the new field `domain_pred`. Example of expected outputs: \n",
    "```\n",
    "Il adore les chats.\t                                Pets_and_Animals\n",
    "Investing in index funds is a popular strategy...\tFinance\n",
    "Ir de compras en el centro comercial es una ex...\tShopping\n",
    "Dzięki wykorzystaniu analizy danych programy t...\tSports\n",
    ".تقدم التطورات الحديثة في العلاج الجيني أملاً ...\t        Health\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42cf0fe-ca35-458a-81fa-53d548bb3a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results\n",
    "result_domain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0540530-6bd5-457e-a289-e31ee4969248",
   "metadata": {},
   "source": [
    "Now, let's use the `MultilingualDomainClassifier` to process our previously filtered multilingual corpus (French and Spanish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cd91c6-6e1b-4fcf-903c-2ba1ce575c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the filtered data\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "\n",
    "multilingual_data_path = \"./curated/01_clean_and_unify\"\n",
    "multilingual_dataset = DocumentDataset.read_json(multilingual_data_path, backend=\"cudf\")\n",
    "\n",
    "# Domain classification\n",
    "multilingual_result_domain = domain_classifier(dataset=multilingual_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2918da7-d4c9-4147-b35b-caf71cb132cf",
   "metadata": {},
   "source": [
    "Let's check the output. Expected to see an aditional field `domain_pred`:\n",
    "```\n",
    "text                                            \t\tdomain_pred\n",
    "Dragon Ball: Le 20e film de la sage sortira le...\t\tArts_and_Entertainment\n",
    "Cours D'histoire Des États Européens: Depuis L...\t\tBooks_and_Literature\n",
    "Se realizó una jornada de promoción del buentr...\t\tPeople_and_Society\n",
    "...\n",
    "```\n",
    "\n",
    "Execute the following cell to review the topic predictions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d4bc41-0aa7-477c-9b7b-a61f8629c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the domain classification\n",
    "multilingual_result_domain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b22c32d-4775-4ce5-bfc9-d425a131e40a",
   "metadata": {},
   "source": [
    "Let's now save the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10addeac-df75-4262-8ef0-1d7a787a2aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "result_domain.to_json(domain_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54154522-e708-4011-8eba-df81108c284d",
   "metadata": {},
   "source": [
    "We can check the saved outputs by executing the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a68e47c-e938-4ad7-b7cc-01972ea22ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 1 {domain_output_path}/0.part | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f8810f-f7e8-4510-95ef-b29f97e904b0",
   "metadata": {},
   "source": [
    "## 2.3 Deduplication\n",
    "\n",
    "Document-level deduplication aims to reduce the occurrence of duplicate and near-duplicate documents in a dataset. This is crucial for datasets cleaning, reducing redundancy, and ensuring that models are trained on diverse and unique data.\n",
    "\n",
    "In this section, we will explore both the Exact and Fuzzy deduplication. Both functionalities are supported in NeMo Curator and accelerated using the [RAPIDS](https://rapids.ai/) library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d188eeb3-4fbe-4eb2-8438-783e1a6ed1fe",
   "metadata": {},
   "source": [
    "\n",
    "Remember, we created our multilingual (Spanish and French) dataset by deduplicating each sample once.\n",
    "Before running deduplication, we need to ensure that each document in the dataset has a unique ID. We can use the `add_id` module within NeMo Curator to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d59e23e-9db7-4f54-a764-d8f2801ca596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output folders\n",
    "from nemo_curator import AddId\n",
    "\n",
    "data_dir = \"curated/06_add_id\"\n",
    "added_id_output_path = os.path.join(data_dir, \"add_id/cleaned\")\n",
    "!mkdir -p {data_dir}\n",
    "\n",
    "dataset_fr = DocumentDataset.read_json(\n",
    "    os.path.join(language_separated_output_path, \"FR/\"), add_filename=True\n",
    ")\n",
    "dataset_es = DocumentDataset.read_json(\n",
    "    os.path.join(language_separated_output_path, \"ES/\"), add_filename=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e6077d-4373-43a3-b68a-cc4302a99c73",
   "metadata": {},
   "source": [
    "### 2.3.1 Add Unique ID\n",
    "\n",
    "Let's start by adding a unique ID for out dataset separated per language (Spanish and French)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7d53f7-ca0b-4d87-a0d3-bdb060106fef",
   "metadata": {},
   "source": [
    "Let's run the `AddId` on the French corpus by running the next cell. The Format of output ID will be `<prefix>_<id>` where `prefix` is provided and `id` is a generated unique number. \n",
    "\n",
    "Let's apply the `AddId` function to the French corpus by running the next cell. The output ID format will be `<prefix>_<id>`, where `prefix` is specified by the user, and `id` is a uniquely generated number.\n",
    "\n",
    "\n",
    "Example of expected output:\n",
    "```\n",
    "text\t                                         \t\tid\n",
    "Dragon Ball: Le 20e film de la sage sortira le...\t\tFR_data-0000000000\n",
    "Cours D'histoire Des États Européens: Depuis L...\t\tFR_data-0000000001\n",
    "...\n",
    "```\n",
    "\n",
    "Execute the following cell to apply `AddId` to the French corpus, user prefix here is set to `FR_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e488b70b-a4be-4fad-82b2-739e4e31b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define user's prefix\n",
    "FR_add_ID_id_prefix = \"FR_data\"\n",
    "\n",
    "add_id = AddId(id_field=\"id\", id_prefix=FR_add_ID_id_prefix, start_index=0)\n",
    "id_dataset_fr = add_id(dataset_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f17f49-950d-417a-9dc0-86c097431a2c",
   "metadata": {},
   "source": [
    "Let's check the outputs. Notice the new field `id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74fbfc4-8b21-4ca9-9677-40df6e488645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check outputs\n",
    "id_dataset_fr.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a455f9-30da-4aed-89a6-cbc4249aa455",
   "metadata": {},
   "source": [
    "We can save the outputs in their designated folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e9dff1-a8b7-4e38-bf4e-98804fd42c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_dataset_fr.to_json(os.path.join(added_id_output_path, \"FR/\"), write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d9f6d-ae21-4db4-b43d-5bab02408d61",
   "metadata": {},
   "source": [
    "#### Exercice:  Add Unique ID for Spanish data.\n",
    "Make sure to replace the `# Your code here`. If you get stuck, refer to the solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c0051b-3ccb-4c42-9bc6-d98c7a59215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_add_ID_id_prefix = # Your code here\n",
    "\n",
    "add_id = AddId(id_field=\"id\", id_prefix=ES_add_ID_id_prefix, start_index=0)\n",
    "id_dataset_es = # Your code here\n",
    "\n",
    "# save to relevant folder\n",
    "id_dataset_es.to_json(os.path.join(added_id_output_path, \"ES/\"), write_to_filename=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c830c81b-6a15-4bcd-9036-3030cc044838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "ES_add_ID_id_prefix = \"ES_data\"\n",
    "\n",
    "add_id = AddId(id_field=\"id\", id_prefix=ES_add_ID_id_prefix, start_index=0)\n",
    "id_dataset_es = add_id(dataset_es)\n",
    "\n",
    "# save to relevant folder\n",
    "id_dataset_es.to_json(os.path.join(added_id_output_path, \"ES/\"), write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc0cdc8-5408-4cef-a462-3e92a8161f4e",
   "metadata": {},
   "source": [
    "### 2.3.2 Exact Deduplication\n",
    "\n",
    "Exact Deduplication consists in identifying and removing duplicate documents that are exactly identical within a dataset. This process helps eliminate redundant data, prevents models from overfitting on repeated examples, and ensures that training and test sets do not contain the same samples, which could otherwise lead to misleading evaluation metrics.\n",
    "\n",
    "In [NeMo Curator](https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/gpudeduplication.html), exact deduplication works by hashing each document and keeping only one document per hash, and it can be run on both GPU ([CuDF](https://docs.rapids.ai/api/cudf)) and CPU ([Pandas](https://pandas.pydata.org/)) based backends.\n",
    "\n",
    "\n",
    "Let's create the folders for the exact deduplication. We will save the output results in `/data`, temporary files in `/cache`, and logs in `/log`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a066f0a2-cd4b-4440-b49f-9444e246ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_es = \"curated/07_Deduplicate/exact/ES\"\n",
    "\n",
    "exact_dedup_log_dir_es = os.path.join(data_dir_es, \"log\")\n",
    "exact_dedup_cache_dir_es = os.path.join(data_dir_es, \"cache\")\n",
    "exact_dedup_output_dir_es = os.path.join(data_dir_es, \"data\")\n",
    "\n",
    "# Create all required directories\n",
    "os.makedirs(exact_dedup_log_dir_es, exist_ok=True)\n",
    "os.makedirs(exact_dedup_cache_dir_es, exist_ok=True)\n",
    "os.makedirs(exact_dedup_output_dir_es, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e6fff9-aa31-4bee-a770-8848f6868d82",
   "metadata": {},
   "source": [
    "Before running exact deduplication in NeMo Curator, the dataset needs to present a unique ID for each document (sample). We already added these unique IDs in the previous step in the field `\"id\"`.\n",
    "\n",
    "We will be running the exact deduplication on the GPU using cudf backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87637c3b-5700-44b4-994f-6854f28db506",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_field = \"id\"\n",
    "input_dataset_es = DocumentDataset.read_json(\n",
    "    os.path.join(added_id_output_path, \"ES/\"), backend=\"cudf\", add_filename=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035c15f-e01f-464b-abff-a11f418fb6a0",
   "metadata": {},
   "source": [
    "Execute the next cell to run the exact deduplication on the Spanish dataset. This should take about 10 seconds to process.\n",
    "\n",
    "We can use `perform_removal=True` to apply the duplicate removal directly on the dataset. But, for the sake of this exercise, we will first show the deduplication identifification before actually applying the removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13fc88f-cdac-414d-887e-19fc815bcdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from nemo_curator.modules import ExactDuplicates\n",
    "\n",
    "# run exact deducplicate\n",
    "exact_dup_es = ExactDuplicates(\n",
    "    logger=exact_dedup_log_dir_es,\n",
    "    id_field=\"id\",\n",
    "    text_field=\"text\",\n",
    "    hash_method=\"md5\",\n",
    "    cache_dir=exact_dedup_cache_dir_es,\n",
    ")\n",
    "duplicates_es = exact_dup_es(dataset=input_dataset_es)\n",
    "exact_docs_to_remove_es = duplicates_es.df.map_partitions(\n",
    "    lambda x: x[x._hashes.duplicated(keep=\"first\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b237f77-8155-48e4-99bd-1285c7584711",
   "metadata": {},
   "source": [
    "Check how many detected documents have duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ed545-0511-45bc-8a9a-0576ae413222",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of documents in the original data:{len(input_dataset_es)}\")\n",
    "print(f\"Number of documents to be removed:{len(exact_docs_to_remove_es)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5291954-f400-416a-8024-21c55309f7ee",
   "metadata": {},
   "source": [
    "Check some duplicate documents: \n",
    "\n",
    "Example of output: \n",
    "```\n",
    "     id                  _hashes\n",
    "18   ES_data-0000000146 2f610eed57653fbe68328fbaf3274c2a\n",
    "20   ES_data-0000000148  e473009ec2e1a246de93fea08488ca4c\n",
    "21   ES_data-0000000149  066347c8a96bc73056a9f172e4d9710\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f26eec-c730-438a-8e0a-986482095294",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_docs_to_remove_es.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e6e26f-4dd5-47b7-92a7-d7ff66f5cbcc",
   "metadata": {},
   "source": [
    "Now, apply the deduplication removal and save the results to the output data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955713b8-c1b7-41e5-9a77-e6c357624164",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_es = input_dataset_es.df[\n",
    "    ~input_dataset_es.df[id_field].isin(exact_docs_to_remove_es[id_field].compute())\n",
    "]\n",
    "DocumentDataset(result_es).to_json(exact_dedup_output_dir_es, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b9a68c-f952-4c98-9b02-d521bb60ea93",
   "metadata": {},
   "source": [
    "Check saved output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb8df5-aee1-4f2b-b1d3-90c40e1a9e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 1 {exact_dedup_output_dir_es}/file.jsonl |jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c786ff-f2ea-459b-afe1-9b57044722b6",
   "metadata": {},
   "source": [
    "#### Exercice: Run Exact Desuplication for the French data.\n",
    "\n",
    "Run the same exact deduplication for the French data. \n",
    "\n",
    "Let's first create the relevant folders and set the dataset and id field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eefbd1d-e72f-492f-953b-3d384184d399",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_fr = \"curated/07_Deduplicate/exact/FR\"\n",
    "\n",
    "exact_dedup_log_dir_fr = os.path.join(data_dir_fr, \"log\")\n",
    "exact_dedup_cache_dir_fr = os.path.join(data_dir_fr, \"cache\")\n",
    "exact_dedup_output_dir_fr = os.path.join(data_dir_fr, \"data\")\n",
    "!mkdir -p {exact_dedup_log_dir_fr}\n",
    "!mkdir -p {exact_dedup_cache_dir_fr}\n",
    "!mkdir -p {exact_dedup_output_dir_fr}\n",
    "\n",
    "id_field = \"id\"\n",
    "input_dataset_fr = DocumentDataset.read_json(\n",
    "    os.path.join(added_id_output_path, \"FR/\"), backend=\"cudf\", add_filename=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80423c61-dcea-4667-9a28-65bbf8a8d113",
   "metadata": {},
   "source": [
    "Run the deduplication. Make sure to replace the `# Your code here`. If you get stuck, refer to the solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a011c0af-0d20-42c5-a7c8-a6a2c503b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run exact deduplicate\n",
    "exact_dup_fr = # Your code here\n",
    "duplicates_fr = # Your code here\n",
    "exact_docs_to_remove_fr = # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d0b1f-efae-4188-9004-36d173c63128",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "# run exact deducplicate\n",
    "exact_dup_fr = ExactDuplicates(\n",
    "    logger=exact_dedup_log_dir_fr,\n",
    "    id_field=\"id\",\n",
    "    text_field=\"text\",\n",
    "    hash_method=\"md5\",\n",
    "    cache_dir=exact_dedup_cache_dir_fr,\n",
    ")\n",
    "\n",
    "duplicates_fr = exact_dup_fr(dataset=input_dataset_fr)\n",
    "exact_docs_to_remove_fr = duplicates_fr.df.map_partitions(\n",
    "    lambda x: x[x._hashes.duplicated(keep=\"first\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd34d96c-23fb-47b5-8988-4f25b50bafc6",
   "metadata": {},
   "source": [
    "Check how many detected documents have duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1cf7c8-4808-4f5d-9d1b-5e1a9f3191d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of documents in the original data:{len(input_dataset_fr)}\")\n",
    "print(f\"Number of documents to be removed:{len(exact_docs_to_remove_fr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2c9bb6-d4c8-4251-8559-2db39cf5d2ac",
   "metadata": {},
   "source": [
    "Now, apply the deduplication removal and save the results to the output data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87db1055-26e3-47a4-a24d-f401af247652",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_fr = input_dataset_fr.df[\n",
    "    ~input_dataset_fr.df[id_field].isin(exact_docs_to_remove_fr[id_field].compute())\n",
    "]\n",
    "DocumentDataset(result_fr).to_json(exact_dedup_output_dir_fr, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78744bc-547d-4d81-bd7d-573a8e31bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cluster.close()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d198c5-0c7a-4b44-889d-7c2de5d80ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)  # automatically restarts kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa300f62-3410-4ff8-9bc0-286b46ea2b70",
   "metadata": {},
   "source": [
    "### 2.3.3 Fuzzy Deduplication\n",
    "\n",
    "Removing near-duplicates is referred to as fuzzy deduplication at the document level, which is based on Jaccard similarity scores.\n",
    "\n",
    "This approach can be broken down into the following stages:\n",
    "- **Stage 1 - Minhash + LSH:** The first step involves generating MinHash signatures for the documents. NeMo Curator currently supports character-based n-grams for MinHashing. Then, the Locality Sensitive Hashing (LSH) is performed to identify candidate duplicates.\n",
    "- **Stage 2 - LSH Buckets to Graph edgelist:** LSH buckets are directly converted to edges for the connected components computation.\n",
    "- **Stage 3 - Connect Components:** Since LSH is an approximate method, documents that are near duplicates may end up in different buckets, with some overlapping documents between them. A GPU-accelerated connected components algorithm is used to identify all connected components in the graph formed by the edges between documents within the same bucket. The output of this step is a list of document IDs and the groups they belong to.\n",
    "\n",
    "All documents within the same group are considered near duplicates, and results can then be used to remove them from the corpus.\n",
    "For more information, refer to the Deduplication documentation of [NeMo Curator](https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/gpudeduplication.html).\n",
    "\n",
    "\n",
    "There are no near-duplicates in out example datasets. However, to demonstrate the process, let's run fuzzy deduplication on the French dataset and go through the steps involved.\n",
    "\n",
    "Let's create fisrt the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd3cc93-65fc-4467-b4a4-f5cd26682c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "fuzzy_dedup_log_dir_fr = \"curated/07_Deduplicate/fuzzy_wrapper/FR\"\n",
    "os.makedirs(fuzzy_dedup_log_dir_fr, exist_ok=True)\n",
    "\n",
    "data_dir = \"curated/06_add_id\"\n",
    "added_id_output_path = os.path.join(data_dir, \"add_id/cleaned\")\n",
    "os.makedirs(added_id_output_path, exist_ok=True)  # Creates \"curated/06_add_id/add_id/cleaned\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b998c6-62c1-4b24-9f60-9462410ee43e",
   "metadata": {},
   "source": [
    "Let's start the Dask client. Make sure that you have stopped the previous one before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c1970-36cf-4e11-a80a-8a0ebe331696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from nemo_curator.utils.import_utils import gpu_only_import, gpu_only_import_from\n",
    "\n",
    "cudf = gpu_only_import(\"cudf\")\n",
    "dask_cudf = gpu_only_import(\"dask_cudf\")\n",
    "LocalCUDACluster = gpu_only_import_from(\"dask_cuda\", \"LocalCUDACluster\")\n",
    "\n",
    "cluster = LocalCUDACluster(n_workers=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a25539-2c7a-4338-a0ac-60912b739d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDF_SPILL\"] = \"on\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4796c6-1002-4692-bcc3-6e58f5b30aa9",
   "metadata": {},
   "source": [
    "We will use the `FuzzyDuplicates` method from NeMo Curator to run the fuzzy deduplication process on the French dataset. This will allow us to identify and handle any near-duplicates based on similarity scores.\n",
    "\n",
    "You should see the three stages logged during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274cd642-b865-405e-a51f-ffa0c6bb505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_dedup_log_dir_fr = \"curated/07_Deduplicate/fuzzy_wrapper/FR\"\n",
    "\n",
    "data_dir = \"curated/06_add_id\"\n",
    "added_id_output_path = os.path.join(data_dir, \"add_id/cleaned\")\n",
    "input_fr = os.path.join(added_id_output_path, \"FR/file.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc1758-894d-44d6-a3fe-a6ae057820c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import FuzzyDuplicates, FuzzyDuplicatesConfig\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "\n",
    "config = FuzzyDuplicatesConfig(\n",
    "    cache_dir=fuzzy_dedup_log_dir_fr,  # must be cleared between runs\n",
    "    id_field=\"id\",\n",
    "    text_field=\"text\",\n",
    "    seed=42,\n",
    "    char_ngrams=24,\n",
    "    num_buckets=20,\n",
    "    hashes_per_bucket=13,\n",
    "    use_64_bit_hash=False,\n",
    "    buckets_per_shuffle=2,\n",
    "    false_positive_check=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the deduplication object\n",
    "FuzzyDups = FuzzyDuplicates(config=config, logger=\"./\")\n",
    "\n",
    "# load the dataset\n",
    "dataset_fr = DocumentDataset.read_json(\n",
    "    input_files=input_fr,\n",
    "    backend=\"cudf\",  # FuzzyDuplicates only supports datasets with the cuDF backend.\n",
    ")\n",
    "\n",
    "# run Fuzzy Duplicate\n",
    "duplicate_docs = FuzzyDups(dataset_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56671fc-d54d-4bb7-80aa-c03ed578c2da",
   "metadata": {},
   "source": [
    "The result from the connected components stage is a list of document IDs and the group they belong to. All documents in the same group are considered near duplicates. \n",
    "\n",
    "```\n",
    "id\t                group\n",
    "FR_data-0000000062\t46\n",
    "FR_data-0000000013\t47\n",
    "FR_data-0000000104\t160\n",
    "FR_data-0000000185\t161\n",
    "FR_data-0000000155\t65\n",
    "...\n",
    "```\n",
    "Let's check the outputs. Notice the `group` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc464a1-54a9-48ab-b2af-b523f34b5a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_docs.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baad0c2-827f-4664-bc89-70a5f3432340",
   "metadata": {},
   "source": [
    "These groups can be then used to remove the near duplicates from the corpus.\n",
    "\n",
    "Let's run that by executing the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2160e29-08b2-4665-9827-b8f5e42275dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_to_remove = duplicate_docs.df.map_partitions(\n",
    "    lambda x: x[x.group.duplicated(keep=\"first\")]\n",
    ")\n",
    "result = dataset_fr.df[~dataset_fr.df[\"id\"].isin(docs_to_remove[\"id\"].compute())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bbfc1b-a174-4fcd-9475-768c260f7739",
   "metadata": {},
   "source": [
    "Check how many detected documents have duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835bb183-790f-48d5-aeb7-e6e3fd66236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of documents in the original data : {len(dataset_fr)}\")\n",
    "print(f\"Number of documents to be removed : {len(result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff85b50-bcb5-4419-9cd9-9d98834bb997",
   "metadata": {},
   "source": [
    "#### [Optional] Explore further Deduplication on downstream tasks\n",
    "\n",
    "Large Language Models are typically evaluated based on their performance on downstream tasks using unseen test data. However, when working with extensive datasets, there is a risk of test data leaking into the model's training set. \n",
    "\n",
    "To mitigate this, NeMo Curator provides a Decontamination strategy, in order to ensure that any document sections appearing in downstream tasks are removed from the training set. \n",
    "\n",
    "You can explore this in more detail in the [task decontamination](https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/taskdecontamination.html) of NeMo Curator documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076bdf0-1e24-4301-8397-2bd44c26c187",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "\n",
    "In this notebook, you have used NeMo Curator to apply several data cleaning steps, including language detection and filtering, topic classification and document deduplication. These steps help ensure that the dataset is clean, diverse, and free from redundant data, improving the quality of the data used for training and evaluation.\n",
    "\n",
    "Before moving on to the next notebook, make sure to stop the Dask cluster. Please run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d11a1-c3dc-4fd3-9379-64cd7cc32c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)  # automatically restarts kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17257a2d-958c-44b0-80e3-aba254a12bba",
   "metadata": {},
   "source": [
    "Move to the next notebook to explore synthetic data generation with NeMo Curator. This will allow us to learn how to create artificial data for various tasks, enhancing the diversity and richness of our dataset.\n",
    "\n",
    "Let's move to the [synthetic_data_generation](03_synthetic_data_generation.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99528953-b137-4660-8a86-db1a0715e350",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
